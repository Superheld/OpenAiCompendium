# Transformer Revolution (2017+)

## ðŸŽ¯ Lernziele
- Verstehe das "Attention is All You Need" Paper
- Lerne Self-Attention und Multi-Head Attention
- Verstehe Transformer Architekturen

## ðŸ“– Geschichte & Kontext
"Attention is All You Need" (2017) und das Ende der RNN-Ã„ra - Transformers revolutionieren NLP.

## ðŸ“‚ Kapitel in diesem Abschnitt
- `01-attention-mechanism.md` - Self-Attention, Multi-Head Attention
- `02-positional-encoding.md` - Absolute, Relative, Rotary Encodings
- `03-transformer-architecture.md` - Encoder-Decoder, BERT, GPT
- `04-training-transformers.md` - MLM, CLM, Seq2Seq Training
- `05-scaling-laws.md` - Warum grÃ¶ÃŸere Models besser sind

## ðŸ”— WeiterfÃ¼hrende Themen
Nach diesem Abschnitt empfohlen: [05-embeddings/](../05-embeddings/) oder [06-llms/](../06-llms/)