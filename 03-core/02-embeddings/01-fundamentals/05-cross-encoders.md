# Cross-Encoders: Deep Dive

## Inhaltsverzeichnis

1. [Was sind Cross-Encoders?](#was-sind-cross-encoders)
2. [Bi-Encoder vs. Cross-Encoder](#bi-encoder-vs-cross-encoder)
3. [Wie funktioniert ein Cross-Encoder?](#wie-funktioniert-ein-cross-encoder)
4. [Training und Fine-Tuning](#training-und-fine-tuning)
5. [Re-Ranking Pipeline](#re-ranking-pipeline)
6. [Vor- und Nachteile](#vor--und-nachteile)
7. [Use-Cases und Beispiele](#use-cases-und-beispiele)

---

## Was sind Cross-Encoders?

### Die prÃ¤ziseste Methode fÃ¼r Text-Matching

**Cross-Encoder** ist eigentlich **kein Embedding-Verfahren** im klassischen Sinne. Stattdessen berechnet er direkt einen **Relevanz-Score** zwischen zwei Texten.

### Der fundamentale Unterschied

**Bei Embeddings (Bi-Encoder):**
```
Text A â†’ Vektor A [0.23, -0.45, ...]
Text B â†’ Vektor B [0.12, 0.34, ...]

Vergleich: Cosine(A, B) = 0.75
```

**Bei Cross-Encoder:**
```
Text A + Text B â†’ BERT â†’ Relevanz-Score = 0.82

Kein Vektor! Direkt ein Score.
```

### Warum "Cross"?

Der Begriff kommt von der **Cross-Attention** - beide Texte "sehen" sich gegenseitig wÃ¤hrend des Encodings.

```
Standard (Bi-Encoder):
  Text A: "KÃ¼hlschrank"
  â†“ Encoding (isoliert)
  Vector A

  Text B: "Refrigerator"
  â†“ Encoding (isoliert)
  Vector B

  A weiÃŸ nichts Ã¼ber B beim Encoding!

Cross-Encoder:
  "KÃ¼hlschrank [SEP] Refrigerator"
  â†“ Encoding (zusammen!)

  A und B interagieren im BERT!
  â†’ "KÃ¼hlschrank" sieht "Refrigerator"
  â†’ Attention lernt: Das sind Synonyme!
```

### Kernidee

**Maximum an Information durch Joint Encoding:**

Wenn Query und Dokument **zusammen** durch ein Transformer-Model gehen, kann das Model:
- **Synonyme erkennen** ("KÃ¼hlschrank" â†” "Refrigerator")
- **WidersprÃ¼che finden** ("billig" vs. "teuer")
- **Kontext verstehen** ("Bank" am Fluss vs. Geldinstitut)
- **Semantische Beziehungen** erfassen (Ursache-Wirkung, Teil-Ganzes)

**Der Preis:** Sehr langsam, weil jedes Query-Doc-Paar neu berechnet werden muss.

---

## Bi-Encoder vs. Cross-Encoder

### Architektur-Vergleich

**Bi-Encoder (Standard Dense Embeddings):**

```
Query: "LaborkÃ¼hlschrank 280L"
    â†“
  [BERT]
    â†“
  [Vector] [0.23, -0.45, 0.12, ...]

Document: "Medizinischer KÃ¼hlschrank 280 Liter"
    â†“
  [BERT] â† Separates Encoding!
    â†“
  [Vector] [0.25, -0.43, 0.14, ...]

Cosine Similarity = 0.87
```

**Eigenschaften:**
- Dokumente kÃ¶nnen **vorher** encoded werden (einmal!)
- Query wird encoded (einmal!)
- Vergleich ist Vektor-Operation (sehr schnell)
- Skaliert zu Millionen Dokumenten

---

**Cross-Encoder:**

```
Query + Document zusammen:
"[CLS] LaborkÃ¼hlschrank 280L [SEP] Medizinischer KÃ¼hlschrank 280 Liter [SEP]"
    â†“
  [BERT mit Cross-Attention]
    â†“
  [CLS]-Token Embedding
    â†“
  [Linear Layer]
    â†“
  Score: 0.92
```

**Eigenschaften:**
- Dokumente kÃ¶nnen **nicht** vorher encoded werden
- FÃ¼r jede neue Query mÃ¼ssen **alle** Docs neu berechnet werden
- Vergleich ist komplettes BERT-Forward-Pass (langsam)
- Skaliert schwer (nur fÃ¼r Re-Ranking von wenigen Kandidaten)

### Warum ist Cross-Encoder genauer?

**1. Token-zu-Token Attention**

Bei Bi-Encoder:
```
"LaborkÃ¼hlschrank" wird encoded ohne "280 Liter" zu sehen
â†’ Embedding ist generisch fÃ¼r alle KÃ¼hlschrÃ¤nke
```

Bei Cross-Encoder:
```
"LaborkÃ¼hlschrank" sieht "280 Liter" wÃ¤hrend Encoding
â†’ Attention kann fokussieren: "Ah, es geht um Volumen!"
â†’ Spezialisiertes VerstÃ¤ndnis
```

**2. Asymmetrische Beziehungen**

Bi-Encoder: Symmetrisch
```
Sim(A, B) = Sim(B, A)
Cosine ist immer symmetrisch
```

Cross-Encoder: Kann asymmetrisch sein
```
Query: "Was ist ein KÃ¼hlschrank?"
Doc: "Ein KÃ¼hlschrank ist ein GerÃ¤t..."

vs.

Query: "Ein KÃ¼hlschrank ist ein GerÃ¤t..."
Doc: "Was ist ein KÃ¼hlschrank?"

Cross-Encoder kann lernen: Erste Kombination ist relevant, zweite nicht!
```

**3. Negative Signale**

Bi-Encoder:
```
Query: "KÃ¼hlschrank OHNE Gefrierfach"
Doc: "KÃ¼hlschrank mit Gefrierfach"

Embedding von "OHNE" und "mit" sind beide Ã¤hnlich (beide PrÃ¤positionen)
â†’ Hohe Similarity trotz Widerspruch!
```

Cross-Encoder:
```
Sieht "OHNE" und "mit" zusammen
â†’ Attention erkennt Widerspruch
â†’ Niedriger Score!
```

### Performance-Unterschied

**Empirisch (BEIR Benchmark):**

```
Task: Information Retrieval

Bi-Encoder (Dense):
  nDCG@10 = 0.52

Cross-Encoder (Re-Ranking):
  nDCG@10 = 0.61  â† +17% Verbesserung!

Aber:
  Bi-Encoder: 1M Docs in 100ms
  Cross-Encoder: 100 Docs in 2000ms
```

**Trade-off:** Genauigkeit vs. Geschwindigkeit

---

## Wie funktioniert ein Cross-Encoder?

### Architektur im Detail

**Input-Konstruktion:**

```
Query: "LaborkÃ¼hlschrank DIN 13277"
Document: "Der Kirsch LABO-288 ist nach DIN 13277 zertifiziert"

Combined Input:
[CLS] LaborkÃ¼hlschrank DIN 13277 [SEP] Der Kirsch LABO-288 ist nach DIN 13277 zertifiziert [SEP]
  â†‘                               â†‘                                                          â†‘
Special  Query Tokens            Separator        Document Tokens                     Separator
```

**Wichtig:**
- `[CLS]` - Classification Token (wird fÃ¼r finalen Score genutzt)
- Erste `[SEP]` - trennt Query von Document
- Zweite `[SEP]` - markiert Ende

### BERT Processing

**Self-Attention Ã¼ber beide Texte:**

```
Attention Matrix (vereinfacht):

           [CLS] Labor... DIN  [SEP] Der  Kirsch LABO  ist  nach  DIN  ...
[CLS]      [0.1  0.2     0.3   0.1   0.05  0.1   0.05  0.1  0.05  0.3  ...]
Labor...   [0.1  0.4     0.2   0.05  0.1   0.2   0.35  0.1  0.1   0.15 ...] â† Attention!
DIN        [0.2  0.1     0.3   0.05  0.05  0.05  0.1   0.1  0.15  0.45 ...] â† Matched "DIN"!
[SEP]      [0.15 0.1     0.15  0.2   0.1   0.15  0.1   0.1  0.1   0.15 ...]
...
```

**Cross-Attention findet:**
- "Labor..." achtet auf "LABO" (0.35) â† Prefix-Match!
- "DIN" achtet auf "DIN" (0.45) â† Exakter Match!
- `[CLS]` aggregiert alle Informationen (0.1-0.3 Ã¼ber alle)

**Nach 12 Transformer-Layern:**
```
[CLS]-Token Embedding: [768 Dimensionen]
  Kodiert die Relevanz-Information!
```

### Classification Head

**Final Score Berechnung:**

```
[CLS] Embedding: [768-dim]
    â†“
Linear Layer: 768 â†’ 1
    â†“
Sigmoid (optional)
    â†“
Score: 0.0 - 1.0
```

**Verschiedene Objectives mÃ¶glich:**

**1. Binary Classification:**
```
Output: [0, 1]
  0 = nicht relevant
  1 = relevant
```

**2. Regression:**
```
Output: [0.0 - 1.0]
  Continuous score
  0.0 = vÃ¶llig irrelevant
  1.0 = perfektes Match
```

**3. Multi-Class:**
```
Output: [not_relevant, somewhat_relevant, very_relevant]
  3 Klassen statt binÃ¤r
```

### Token-Type Embeddings

**BERT unterscheidet Query von Document intern:**

```
Tokens:     [CLS] Labor... DIN [SEP] Der Kirsch [SEP]
Token IDs:  [101  2342    5123 102   4532 8234  102  ]
Segment:    [  0    0      0    0     1    1     1  ]
            â†‘ Query Segment         â†‘ Document Segment

Embedding = Token Embedding + Position Embedding + Segment Embedding
```

**Segment Embedding hilft:**
- Model weiÃŸ "Das ist die Query"
- Model weiÃŸ "Das ist das Dokument"
- Kann unterschiedlich behandeln (z.B. Query-Terms wichtiger)

---

## Training und Fine-Tuning

### Dataset-Anforderungen

**Cross-Encoder braucht:**

```
(Query, Document, Label)
```

**Beispiele:**

```
("KÃ¼hlschrank 280L", "LABO-288 LaborkÃ¼hlschrank 280 Liter", 1)  â† Relevant
("KÃ¼hlschrank 280L", "Pommes Frites Rezept", 0)                â† Nicht relevant
("DIN 13277", "MedikamentenkÃ¼hlschrank nach DIN 13277", 1)     â† Relevant
("DIN 13277", "LaborkÃ¼hlschrank ohne Zertifizierung", 0)        â† Nicht relevant
```

**Label-Arten:**

**Binary:**
```
0 = nicht relevant
1 = relevant
```

**Graded (besser!):**
```
0 = nicht relevant
1 = etwas relevant
2 = relevant
3 = sehr relevant
```

**Continuous:**
```
0.0 - 1.0 (Similarity Score)
```

### Training-Prozess

**1. Pre-Training (optional):**

Starte mit vortrainiertem BERT:
- `bert-base-uncased` (Englisch)
- `bert-base-german-cased` (Deutsch)
- `xlm-roberta-base` (Multilingual)

**2. Fine-Tuning:**

**Loss-Function (Binary Cross-Entropy):**

```
FÃ¼r jedes (Query, Doc, Label):

  Score = CrossEncoder(Query, Doc)

  Loss = -[Label Ã— log(Score) + (1-Label) Ã— log(1-Score)]

  Backprop â†’ Update Weights
```

**FÃ¼r Regression (MSE):**

```
Loss = (Predicted_Score - True_Score)Â²
```

**3. Hard Negative Mining:**

Nicht alle Negative sind gleich lehrreich!

```
Easy Negative (nutzlos):
  Query: "LaborkÃ¼hlschrank"
  Doc: "Pommes Frites" â† Offensichtlich irrelevant

Hard Negative (sehr wertvoll!):
  Query: "LaborkÃ¼hlschrank DIN 13277"
  Doc: "LaborkÃ¼hlschrank DIN 13271" â† Fast relevant, aber falsche Norm!
```

**Mining-Strategie:**
1. Nutze Bi-Encoder fÃ¼r initiales Retrieval
2. Top-100 Kandidaten
3. Davon sind einige relevant, viele "hard negatives"
4. Nutze diese fÃ¼r Cross-Encoder Training

### Knowledge Distillation

**Problem:** Cross-Encoder sehr groÃŸ/langsam

**LÃ¶sung:** Lehrer-SchÃ¼ler-Training

```
Teacher (Cross-Encoder - groÃŸ, genau):
  BERT-large mit 24 Layern

  Query + Doc â†’ Teacher Score = 0.87

Student (Bi-Encoder - klein, schnell):
  BERT-base mit 12 Layern

  Query â†’ Vec_Q
  Doc â†’ Vec_D

  Similarity = Cosine(Vec_Q, Vec_D)

  Loss = (Similarity - 0.87)Â²  â† Imitiere Teacher!
```

**Resultat:** Bi-Encoder lernt von Cross-Encoder, wird fast so gut, aber bleibt schnell!

---

## Re-Ranking Pipeline

### Two-Stage Retrieval

**Das Standard-Pattern fÃ¼r Production:**

```
Stage 1: Candidate Generation (schnell, breit)
  â†“
  Bi-Encoder oder BM25
  1 Million Docs â†’ Top-1000 Kandidaten
  Zeit: ~100ms

Stage 2: Re-Ranking (langsam, prÃ¤zise)
  â†“
  Cross-Encoder
  1000 Kandidaten â†’ Top-10 Final Results
  Zeit: ~2000ms

Total: ~2.1 Sekunden
```

**Warum funktioniert das?**

- **Recall (Stage 1):** Bi-Encoder/BM25 haben hohen Recall
  - Von 10 relevanten Docs finden sie 9 in Top-1000

- **Precision (Stage 2):** Cross-Encoder hat hohe Precision
  - Von 1000 Kandidaten findet er die besten 10

### Multi-Stage Refinement

**FÃ¼r sehr groÃŸe Datenmengen:**

```
Stage 1: BM25
  100M Docs â†’ Top-10k
  Zeit: 50ms

Stage 2: Bi-Encoder
  10k â†’ Top-100
  Zeit: 200ms

Stage 3: Cross-Encoder
  100 â†’ Top-10
  Zeit: 500ms

Total: ~750ms
```

**Trade-off:** Mehr Stages = mehr KomplexitÃ¤t, aber bessere Latenz

### Hybrid Re-Ranking

**Kombiniere mehrere Signale:**

```
FÃ¼r jedes Dokument:

  BM25_Score = 0.75
  Dense_Score = 0.82
  Cross_Encoder_Score = 0.89

  Final_Score = Î±Ã—BM25 + Î²Ã—Dense + Î³Ã—Cross_Encoder
              = 0.2Ã—0.75 + 0.3Ã—0.82 + 0.5Ã—0.89
              = 0.841
```

**Parameter (Î±, Î², Î³):**
- Manuell tunen
- Grid-Search auf Validation-Set
- Oder: Learned Fusion (LambdaMART, LightGBM)

---

## Vor- und Nachteile

### Vorteile von Cross-Encoders

âœ… **HÃ¶chste Genauigkeit**

Alle Benchmarks zeigen: Cross-Encoder > Bi-Encoder > BM25

```
TREC-COVID (Medical Search):
  BM25: nDCG@10 = 0.48
  Bi-Encoder: nDCG@10 = 0.56
  Cross-Encoder: nDCG@10 = 0.65  â† Beste Performance
```

âœ… **Versteht Kontext besser**

```
Query: "KÃ¼hlschrank ohne Gefrierfach"
Doc A: "KÃ¼hlschrank mit separatem Gefrierfach" â† Nicht relevant!
Doc B: "KÃ¼hlschrank, kein Gefrierfach integriert" â† Relevant!

Bi-Encoder: Beide Ã¤hnlich (enthalten "KÃ¼hlschrank", "Gefrierfach")
Cross-Encoder: Erkennt "ohne" vs. "mit" â†’ Unterschiedliche Scores
```

âœ… **Semantische Relationen**

```
Query: "Ursachen von KÃ¼hlschrank-Defekten"
Doc: "Defekte KÃ¼hlschrÃ¤nke kÃ¶nnen durch Stromausfall entstehen"

Bi-Encoder: Moderate Similarity
Cross-Encoder: Erkennt Ursache-Wirkung-Beziehung â†’ Hoher Score
```

âœ… **Negation und WidersprÃ¼che**

```
Query: "MedikamentenkÃ¼hlschrank"
Doc: "Dies ist KEIN MedikamentenkÃ¼hlschrank, sondern ein LaborkÃ¼hlschrank"

Bi-Encoder: Hohe Similarity (beide enthalten "MedikamentenkÃ¼hlschrank")
Cross-Encoder: Niedrige Score (erkennt "KEIN")
```

---

### Nachteile von Cross-Encoders

âŒ **Sehr langsam**

```
1000 Dokumente Ã— BERT Forward Pass (50ms pro Doc)
= 50 Sekunden!

vs.

Bi-Encoder: 1000 Ã— Cosine (0.001ms pro Doc)
= 1 Sekunde
```

âŒ **Kein Pre-Computation**

```
Bi-Encoder:
  Docs kÃ¶nnen vorher embedded werden
  â†’ Speichere Vektoren
  â†’ Bei neuer Query nur Query embedden

Cross-Encoder:
  Jedes (Query, Doc) Paar muss neu berechnet werden
  â†’ Keine Wiederverwendung mÃ¶glich
```

âŒ **Speicher-intensiv bei Inference**

```
BERT-base: 110M Parameter
â†’ ~440MB Model-Weights
â†’ Pro Forward Pass: ~2GB GPU Memory (bei Batch=32)
```

âŒ **Skaliert nicht zu Millionen Docs**

```
1M Docs Ã— 50ms = 13.9 Stunden fÃ¼r eine Query!

Praktisch nur fÃ¼r:
  - Re-Ranking (wenige Kandidaten)
  - Kleine Datenmengen (<10k Docs)
```

âŒ **Keine Embeddings**

```
Bi-Encoder:
  Kann Embeddings fÃ¼r Clustering, Visualization nutzen

Cross-Encoder:
  Gibt nur Scores, keine Vektoren
  â†’ Nicht fÃ¼r Downstream-Tasks nutzbar
```

---

### Wann Cross-Encoder nutzen?

âœ… **Perfekt fÃ¼r:**

**1. Re-Ranking (hÃ¤ufigster Use-Case)**
```
Stage 1: Bi-Encoder (1M â†’ 1000)
Stage 2: Cross-Encoder (1000 â†’ 10)
```

**2. Kleine Datenmengen**
```
<10k Dokumente â†’ Cross-Encoder direkt einsetzbar
Beispiel: FAQ (100 Fragen), Legal Docs (1000 VertrÃ¤ge)
```

**3. High-Precision Tasks**
```
Medical Search, Legal Search, Question Answering
â†’ Genauigkeit wichtiger als Speed
```

**4. Evaluation & Benchmarking**
```
Nutze Cross-Encoder als "Ground Truth"
â†’ Trainiere schnellere Models (Bi-Encoder) mit Distillation
```

âŒ **Nicht geeignet fÃ¼r:**

**1. First-Stage Retrieval**
```
Millionen Docs â†’ zu langsam
```

**2. Real-Time Anwendungen**
```
<100ms Latenz erforderlich â†’ Bi-Encoder nutzen
```

**3. Embedding-basierte Tasks**
```
Clustering, Semantic Search Index, Similarity-based Recommendation
â†’ Braucht Vektoren, Cross-Encoder gibt keine
```

---

## Use-Cases und Beispiele

### 1. Question Answering

**Szenario:** Finde beste Antwort auf Frage

```
Frage: "Wie funktioniert ein KÃ¼hlschrank?"

Kandidaten (von Bi-Encoder):
  1. "Ein KÃ¼hlschrank ist ein GerÃ¤t zur KÃ¼hlung..."
  2. "KÃ¼hlschrÃ¤nke gibt es in verschiedenen GrÃ¶ÃŸen..."
  3. "Der KÃ¼hlkreislauf funktioniert durch Kompression..."
  4. "LaborkÃ¼hlschrÃ¤nke unterscheiden sich von..."

Cross-Encoder Re-Ranking:
  1. Doc 3: 0.92 â† ErklÃ¤rt "wie funktioniert"!
  2. Doc 1: 0.78 â† Definiert nur
  3. Doc 4: 0.45
  4. Doc 2: 0.23
```

**Cross-Encoder erkennt:** Query fragt nach "wie", nicht "was"

### 2. Duplicate Detection

**Szenario:** Finde doppelte Produktbeschreibungen

```
Doc A: "Kirsch LABO-288 LaborkÃ¼hlschrank 280 Liter"
Doc B: "LABO-288 von Kirsch, LaborkÃ¼hlschrank mit 280L Volumen"

Bi-Encoder: Similarity = 0.75 (nicht eindeutig)
Cross-Encoder: Score = 0.95 â† Sehr wahrscheinlich Duplikat!

Doc C: "Liebherr LaborkÃ¼hlschrank 280 Liter"
Cross-Encoder: Score = 0.65 â† Ã„hnlich, aber anderer Hersteller
```

### 3. Sentiment-Aware Search

**Szenario:** Finde Reviews mit passender Stimmung

```
Query: "positive Erfahrungen mit LaborkÃ¼hlschrÃ¤nken"

Doc A: "Der KÃ¼hlschrank funktioniert leider nicht gut..."
  Bi-Encoder: 0.7 (enthÃ¤lt "KÃ¼hlschrank", "funktioniert")
  Cross-Encoder: 0.2 â† Erkennt negative Stimmung!

Doc B: "Sehr zufrieden mit unserem neuen LaborkÃ¼hlschrank..."
  Bi-Encoder: 0.72
  Cross-Encoder: 0.91 â† Erkennt positive Stimmung!
```

### 4. Multi-Hop Reasoning

**Szenario:** Komplexe Fragen die mehrere Fakten kombinieren

```
Query: "Welche DIN-zertifizierten KÃ¼hlschrÃ¤nke haben Ã¼ber 250 Liter?"

Doc A: "Der LABO-288 hat 280 Liter Volumen"
  â†’ EnthÃ¤lt "280 Liter" âœ“ aber nicht "DIN"

Doc B: "DIN 13277 zertifizierte KÃ¼hlschrÃ¤nke..."
  â†’ EnthÃ¤lt "DIN" âœ“ aber nicht "Liter"

Doc C: "LABO-288 ist DIN 13277 zertifiziert mit 280L"
  â†’ EnthÃ¤lt BEIDE Infos âœ“âœ“

Bi-Encoder: A=0.7, B=0.65, C=0.75 (alle Ã¤hnlich)
Cross-Encoder: A=0.5, B=0.4, C=0.95 â† Erkennt dass C beide Bedingungen erfÃ¼llt!
```

### 5. Passage Re-Ranking

**Szenario:** Finde relevantesten Textabschnitt in langem Dokument

```
Query: "Temperaturbereich von MedikamentenkÃ¼hlschrÃ¤nken"

Dokument (3000 WÃ¶rter):
  Absatz 1: "MedikamentenkÃ¼hlschrÃ¤nke sind wichtig..." (allgemein)
  Absatz 5: "Der Temperaturbereich liegt zwischen +2Â°C und +8Â°C..." â† Relevant!
  Absatz 12: "Installation und Wartung..." (irrelevant)

Bi-Encoder:
  Absatz 1: 0.65 (enthÃ¤lt "MedikamentenkÃ¼hlschrank")
  Absatz 5: 0.72
  Absatz 12: 0.45

Cross-Encoder:
  Absatz 1: 0.45
  Absatz 5: 0.94 â† Exakte Antwort!
  Absatz 12: 0.15
```

### Tools und Libraries

**Sentence-Transformers:**
```python
from sentence_transformers import CrossEncoder

model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = model.predict([
    ("Query", "Document 1"),
    ("Query", "Document 2")
])
# [0.85, 0.23]
```

**Hugging Face Transformers:**
```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    'cross-encoder/ms-marco-MiniLM-L-6-v2'
)
# Manueller Forward Pass fÃ¼r mehr Kontrolle
```

**VerfÃ¼gbare Pre-Trained Models:**
- `cross-encoder/ms-marco-MiniLM-L-6-v2` - Schnell, Englisch
- `cross-encoder/ms-marco-electra-base` - Besser, Englisch
- `cross-encoder/mmarco-mMiniLMv2-L12-H384-v1` - Multilingual
- `cross-encoder/qnli-electra-base` - Question-Answering

---

## Zusammenfassung

### Key Takeaways

1. **Cross-Encoder = Joint Encoding** von Query + Document
2. **HÃ¶chste Genauigkeit** durch volle Attention zwischen Texten
3. **Sehr langsam** - jedes Paar muss neu berechnet werden
4. **Hauptanwendung: Re-Ranking** (Stage 2 nach Bi-Encoder)
5. **Versteht Kontext** besser als Bi-Encoder (Negation, WidersprÃ¼che, Relationen)
6. **Keine Embeddings** - gibt nur Scores
7. **Two-Stage Retrieval** ist Best Practice fÃ¼r Production

### Architektur-Vergleich

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚ Bi-Encoder   â”‚ Cross-Encoder  â”‚ Multi-Vectorâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Genauigkeit     â”‚ â­â­         â”‚ â­â­â­â­â­    â”‚ â­â­â­â­    â”‚
â”‚ Geschwindigkeit â”‚ âš¡âš¡âš¡       â”‚ ğŸ’¤             â”‚ âš¡âš¡        â”‚
â”‚ Skalierbarkeit  â”‚ 1M+ Docs     â”‚ <10k Docs      â”‚ 100k Docs   â”‚
â”‚ Pre-Compute     â”‚ âœ…           â”‚ âŒ             â”‚ âœ…          â”‚
â”‚ Embeddings      â”‚ âœ…           â”‚ âŒ             â”‚ âœ…          â”‚
â”‚ Use-Case        â”‚ Stage 1      â”‚ Re-Ranking     â”‚ Stage 1/2   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Production Pipeline

**Empfehlung:**
```
1. BM25 (100M â†’ 10k) - 50ms
   â†“
2. Bi-Encoder (10k â†’ 100) - 200ms
   â†“
3. Cross-Encoder (100 â†’ 10) - 500ms
   â†“
Total: ~750ms fÃ¼r hÃ¶chste QualitÃ¤t
```

---

## NÃ¤chste Schritte

- [06-HYBRID-APPROACHES.md](06-HYBRID-APPROACHES.md) - Kombiniere alle Methoden optimal
- [07-QUANTIZATION.md](07-QUANTIZATION.md) - Mache Cross-Encoder schneller
- [02-DENSE-EMBEDDINGS.md](02-DENSE-EMBEDDINGS.md) - ZurÃ¼ck zu Bi-Encodern

---

## WeiterfÃ¼hrende Ressourcke

**Papers:**
- Nogueira et al. 2019: "Passage Re-ranking with BERT"
- Humeau et al. 2020: "Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring"

**Datasets:**
- MS MARCO - Microsoft Machine Reading Comprehension
- Natural Questions - Google Q&A Dataset
- TREC - Text Retrieval Conference Benchmarks

**Tools:**
- Sentence-Transformers Library - Einfachste Cross-Encoder Nutzung
- Hugging Face Model Hub - Viele Pre-trained Cross-Encoders
