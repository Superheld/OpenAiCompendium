# Dense Embeddings: Deep Dive

## üéØ Ziel
Beherrsche Dense Embeddings von Transformer-Architektur bis zu Sentence-Transformers - der Industriestandard f√ºr semantische Suche und RAG-Systeme.

## üìñ Geschichte & Kontext

Dense Embeddings revolutionierte NLP mit der Transformer-Architektur. Von BERTs bidirektionaler Innovation bis zu modernen Sentence-Transformers - sie sind das Herzst√ºck aller modernen AI-Anwendungen.

**Meilensteine:**
- **2017**: Transformer-Architektur ("Attention is All You Need")
- **2018**: BERT - bidirektionale kontextualisierte Embeddings
- **2019**: Sentence-BERT - optimiert f√ºr Sentence-Level Similarity
- **2024**: E5, BGE - speziell f√ºr Retrieval optimierte Dense Models

## üßÆ Konzept & Theorie

## Was sind Dense Embeddings?

### Definition

**Dense Embeddings** sind kontinuierliche Vektorrepr√§sentationen, bei denen **alle Dimensionen** mit Werten gef√ºllt sind (keine Nullen).

```python
# Dense Embedding (384 Dimensionen)
[0.234, -0.456, 0.123, 0.789, -0.234, 0.567, ..., -0.123]
# Alle 384 Werte sind != 0 (meist zwischen -1 und 1)
```

**Im Gegensatz zu Sparse:**
```python
# Sparse Embedding (30.000 Dimensionen)
[0, 0, 0, 3.2, 0, 0, 0, 1.5, 0, 0, 0, ..., 0]
# Meiste Werte = 0, nur wenige != 0
```

### Eigenschaften

- **Gr√∂√üe:** Typisch 384, 768, 1024 Dimensionen
- **Wertebereich:** Meist -1 bis +1 (normalisiert)
- **Speicher:** ~3KB pro Embedding (768 dims √ó 4 bytes float32)
- **Semantik:** Encodiert Bedeutung, nicht Keywords

### Wie entstehen Dense Embeddings?

**Von Text zu Vektor - die Pipeline:**

```
1. Tokenisierung
"Labork√ºhlschrank" ‚Üí ["Labor", "##k√ºhl", "##schrank"]

2. Token-zu-ID
["Labor", "##k√ºhl", "##schrank"] ‚Üí [2847, 19384, 8473]

3. Embedding Lookup
[2847, 19384, 8473] ‚Üí [[0.12, -0.34, ...], [0.56, 0.23, ...], [-0.78, 0.45, ...]]

4. Transformer Layers
Contextualisierung durch Self-Attention

5. Pooling
3 Token-Embeddings ‚Üí 1 Sentence-Embedding

6. Normalisierung
Final: [0.234, -0.456, ..., -0.123]
```

**Schauen wir uns jeden Schritt genau an:**

---

## Transformer-Architektur

### Das Herzst√ºck moderner Embeddings

**Paper:** "Attention is All You Need" (Vaswani et al., 2017)

### Self-Attention Mechanism

**Problem:** Wie versteht ein Model Kontext?

**Beispiel:**
```
"Die Bank am Fluss ist sch√∂n."     # Bank = Sitzbank
"Die Bank hat heute geschlossen."  # Bank = Geldinstitut
```

Das Wort "Bank" braucht unterschiedliche Embeddings je nach Kontext!

**L√∂sung: Self-Attention**

Jedes Wort "schaut" auf alle anderen W√∂rter und entscheidet, welche wichtig sind.

```
Satz: "Labork√ºhlschrank f√ºr Medikamente"

Token: "Medikamente"
Attention-Gewichte auf andere W√∂rter:
- "Labor"        : 0.1  (etwas relevant)
- "k√ºhlschrank"  : 0.7  (sehr relevant!)
- "f√ºr"          : 0.05 (wenig relevant)
- "Medikamente"  : 0.15 (selbst)
```

### Mathematik (vereinfacht)

**Self-Attention in 3 Schritten:**

**1. Query, Key, Value Matrizen erstellen**
```python
Q = input √ó W_query   # "Was suche ich?"
K = input √ó W_key     # "Was biete ich?"
V = input √ó W_value   # "Was ist mein Inhalt?"
```

**2. Attention Scores berechnen**
```python
scores = Q @ K.T / sqrt(d_k)  # Wie √§hnlich sind Query und Keys?
attention_weights = softmax(scores)  # Normalisiere zu Wahrscheinlichkeiten
```

**3. Gewichtete Summe der Values**
```python
output = attention_weights @ V  # Mische Values basierend auf Relevanz
```

**Visualisierung:**

```
Input: "Der K√ºhlschrank ist kalt"

        Der    K√ºhl-   schrank   ist    kalt
Der    [0.2    0.1     0.1      0.3    0.3]   ‚Üê Attention-Gewichte
K√ºhl-  [0.1    0.3     0.4      0.1    0.1]
schrank[0.1    0.4     0.3      0.1    0.1]   ‚Üê "schrank" achtet auf "K√ºhl-"!
ist    [0.2    0.1     0.1      0.4    0.2]
kalt   [0.2    0.15    0.15     0.2    0.3]
```

**"schrank" hat hohe Attention auf "K√ºhl-"** ‚Üí Kontext wird verstanden!

### Multi-Head Attention

**Problem:** Ein Attention-Mechanismus ist limitiert

**L√∂sung:** Mehrere parallel (z.B. 12 Heads)

- Head 1: Achtet auf Syntax (Subjekt-Verb-Beziehungen)
- Head 2: Achtet auf Semantik (√§hnliche Bedeutung)
- Head 3: Achtet auf Named Entities
- ...
- Head 12: Achtet auf Negationen

**Jeder Head lernt andere Muster!**

```python
# Pseudo-Code
outputs = []
for head in range(12):
    Q, K, V = create_qkv(input, head)
    attention = self_attention(Q, K, V)
    outputs.append(attention)

final_output = concatenate(outputs) @ W_output
```

### Feed-Forward Network

Nach Self-Attention kommt noch ein FFN:

```python
# Self-Attention Output
x_attention = multi_head_attention(x)

# Feed-Forward
x_ff = linear(relu(linear(x_attention)))

# Residual Connection + Layer Norm
output = layer_norm(x + x_ff)
```

**Warum?** FFN transformiert die Attention-Outputs nochmal non-linear.

### Transformer Block (komplett)

```
Input Embedding
    ‚Üì
[Multi-Head Self-Attention]
    ‚Üì
[Add & Normalize]  ‚Üê Residual Connection
    ‚Üì
[Feed-Forward Network]
    ‚Üì
[Add & Normalize]  ‚Üê Residual Connection
    ‚Üì
Output
```

**Typisch:** 12-24 solcher Bl√∂cke gestackt!

---

## BERT

### Bidirectional Encoder Representations from Transformers

**Paper:** Devlin et al., 2018

### Was macht BERT besonders?

**Bidirektional:** Schaut in beide Richtungen gleichzeitig

```
Andere Models (GPT):          BERT:
"Der K√ºhlschrank ___"         "Der ___ ist kalt"
     ‚Üê                              ‚Üî
Nur links schauen             Links UND rechts!
```

**Vorteil:** Besseres Kontext-Verst√§ndnis

### BERT-Architektur

```
[CLS] Der K√ºhlschrank ist kalt [SEP]
  ‚Üì     ‚Üì       ‚Üì       ‚Üì   ‚Üì    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Token Embeddings            ‚îÇ
‚îÇ   + Positional Encodings         ‚îÇ ‚Üê Wo steht das Token?
‚îÇ   + Segment Embeddings           ‚îÇ ‚Üê Satz A oder B?
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Transformer √ó 12 ‚îÇ  ‚Üê BERT-Base
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚Üì
   768-dim Embeddings f√ºr jedes Token
```

**Spezielle Tokens:**
- `[CLS]` - Classification Token (am Anfang)
- `[SEP]` - Separator (zwischen S√§tzen)

### BERT Training (Pre-Training)

**Zwei Tasks gleichzeitig:**

**1. Masked Language Modeling (MLM)**

```
Original: "Der Labork√ºhlschrank ist sehr kalt"
Masked:   "Der [MASK] ist sehr kalt"

Task: Vorhersage des maskierten Wortes
Model Output: "Labork√ºhlschrank" ‚úì
```

- 15% der Tokens werden maskiert
- Model lernt Kontext-Verst√§ndnis

**2. Next Sentence Prediction (NSP)**

```
Sentence A: "Der K√ºhlschrank ist defekt."
Sentence B: "Wir m√ºssen ihn reparieren."
Label: IsNext ‚úì

Sentence A: "Der K√ºhlschrank ist defekt."
Sentence B: "Bananen sind gelb."
Label: NotNext ‚úì
```

**Training-Daten:** Wikipedia, BookCorpus (3.3 Milliarden Tokens!)

### BERT-Varianten

**BERT-Base:**
- 12 Transformer-Layers
- 768 Dimensionen
- 110M Parameter
- Schneller

**BERT-Large:**
- 24 Transformer-Layers
- 1024 Dimensionen
- 340M Parameter
- Genauer, aber langsamer

**Andere Sprachen:**
- **mBERT** - Multilingual (104 Sprachen)
- **GBERT** - Deutsch-spezifisch
- **CamemBERT** - Franz√∂sisch
- etc.

### Problem mit BERT f√ºr Sentence Embeddings

**BERT wurde f√ºr Token-Klassifikation trainiert, nicht Sentence-Similarity!**

```python
from transformers import BertModel, BertTokenizer

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Problem: Welchen Output nehmen?
inputs = tokenizer("Der K√ºhlschrank", return_tensors='pt')
outputs = model(**inputs)

# outputs.last_hidden_state: [1, seq_len, 768]
# ‚Üí Ein Vektor PRO TOKEN, nicht pro Satz!
```

**L√∂sung:** Sentence-BERT (SBERT)

---

## Sentence-Transformers

### Von BERT zu SBERT

**Paper:** Reimers & Gurevych, 2019

**Problem:** BERT gibt Token-Level Embeddings, wir brauchen Sentence-Level!

**Naive L√∂sung: Mean Pooling**
```python
# Nimm alle Token-Embeddings und berechne Durchschnitt
sentence_embedding = mean(token_embeddings)
```

**Aber:** BERT wurde nicht darauf trainiert! ‚Üí Schlechte Sentence-Similarity

**SBERT-L√∂sung:** BERT mit Siamese Networks finetunen

### Siamese Network Architecture

**Training f√ºr Sentence Similarity:**

```
Sentence A: "Labork√ºhlschrank 280L"
Sentence B: "Medikamentenk√ºhlschrank 280 Liter"
Label: √Ñhnlich (0.9)

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   BERT      ‚îÇ         ‚îÇ   BERT      ‚îÇ  ‚Üê Gleiche Gewichte!
    ‚îÇ  (shared)   ‚îÇ         ‚îÇ  (shared)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì                       ‚Üì
    [0.23, -0.45,...]       [0.25, -0.43,...]
          ‚Üì                       ‚Üì
       Cosine Similarity = 0.89
          ‚Üì
    Loss: |0.89 - 0.9| = 0.01  ‚Üê Minimize!
```

**Zwei identische BERT-Modelle** (shared weights) ‚Üí Embeddings ‚Üí Loss berechnen

### Training-Objectives

**1. Regression (MSE Loss)**
```python
# Dataset: Sentence-Paare mit Similarity-Scores
("K√ºhlschrank", "K√ºhlaggregat") ‚Üí 0.7
("K√ºhlschrank", "Banane")       ‚Üí 0.1

loss = (predicted_similarity - true_similarity)¬≤
```

**2. Contrastive Loss**
```python
# Positive Paare (√§hnlich) sollen nah sein
# Negative Paare (unterschiedlich) sollen weit auseinander

if similar:
    loss = distance¬≤
else:
    loss = max(0, margin - distance)¬≤
```

**3. Triplet Loss**
```python
# Anchor, Positive, Negative
anchor = "Labork√ºhlschrank"
positive = "Medikamentenk√ºhlschrank"  # √Ñhnlich
negative = "Pommes Frites"             # Unterschiedlich

loss = max(0,
    distance(anchor, positive) - distance(anchor, negative) + margin
)

# Ziel: anchor n√§her an positive als an negative (um mindestens margin)
```

**Visualisierung Triplet Loss:**
```
Vorher:
    Anchor ‚Ä¢

    Positive ‚Ä¢        Negative ‚Ä¢

Nachher:
    Anchor ‚Ä¢
    Positive ‚Ä¢                       ‚Ä¢ Negative

‚Üí Positive n√§her, Negative weiter weg
```

### Sentence-Transformers Library

**Installation:**
```bash
pip install sentence-transformers
```

**Verwendung:**
```python
from sentence_transformers import SentenceTransformer

# Model laden (viele vortrainierte verf√ºgbar)
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Embeddings generieren
sentences = [
    "Labork√ºhlschrank mit 280 Liter Volumen",
    "Medikamentenk√ºhlschrank 280L",
    "Pommes Frites sind lecker"
]

embeddings = model.encode(sentences)
# Output: (3, 384) numpy array

# Similarity berechnen
from sentence_transformers import util

similarities = util.cos_sim(embeddings[0], embeddings[1:])
print(similarities)
# [[0.85, 0.12]]  ‚Üê K√ºhlschr√§nke √§hnlich, Pommes nicht
```

**Vorteile:**
- ‚úÖ Direkt einsatzbereit
- ‚úÖ Hunderte vortrainierte Models
- ‚úÖ Optimiert f√ºr Sentence-Similarity
- ‚úÖ Einfaches Fine-Tuning

---

## Pooling-Strategien

### Von Token-Embeddings zu Sentence-Embeddings

**Problem:** BERT gibt `[seq_len, 768]`, wir brauchen `[768]`

**L√∂sung:** Pooling - kombiniere Token-Embeddings zu einem

### 1. Mean Pooling (am h√§ufigsten!)

**Idee:** Durchschnitt aller Token-Embeddings

```python
def mean_pooling(token_embeddings, attention_mask):
    """
    token_embeddings: [batch_size, seq_len, hidden_dim]
    attention_mask: [batch_size, seq_len]  # 1 f√ºr echte Tokens, 0 f√ºr Padding
    """
    # Erweitere attention_mask f√ºr Broadcasting
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())

    # Summiere nur echte Tokens (ignoriere Padding)
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)

    # Durchschnitt
    return sum_embeddings / sum_mask
```

**Beispiel:**
```
Tokens: [CLS] Der K√ºhlschrank [SEP] [PAD] [PAD]
Embeddings:
  [0.1, 0.2, 0.3]  ‚Üê [CLS]
  [0.4, 0.5, 0.6]  ‚Üê Der
  [0.7, 0.8, 0.9]  ‚Üê K√ºhlschrank
  [0.2, 0.3, 0.4]  ‚Üê [SEP]
  [0.0, 0.0, 0.0]  ‚Üê [PAD] (ignoriert)
  [0.0, 0.0, 0.0]  ‚Üê [PAD] (ignoriert)

Mean Pooling:
  (0.1+0.4+0.7+0.2)/4, (0.2+0.5+0.8+0.3)/4, (0.3+0.6+0.9+0.4)/4
= [0.35, 0.45, 0.55]
```

**Vorteile:**
- ‚úÖ Alle Tokens tragen bei
- ‚úÖ Standard f√ºr Sentence-Transformers
- ‚úÖ Funktioniert robust

**Nachteile:**
- ‚ö†Ô∏è Wichtige W√∂rter nicht st√§rker gewichtet

---

### 2. CLS Pooling

**Idee:** Nutze nur das `[CLS]`-Token

```python
def cls_pooling(token_embeddings):
    # Erstes Token ist immer [CLS]
    return token_embeddings[:, 0, :]
```

**BERT wurde so trainiert:** `[CLS]` soll ganze Sequence repr√§sentieren

**Vorteile:**
- ‚úÖ Einfach
- ‚úÖ Schnell

**Nachteile:**
- ‚ö†Ô∏è Nur gut wenn Model darauf trainiert (nicht immer der Fall!)
- ‚ö†Ô∏è Verschwendet Info aus anderen Tokens

**Wann nutzen?**
- Bei BERT-basierten Models die auf NSP trainiert wurden
- F√ºr Klassifikation (nicht Similarity!)

---

### 3. Max Pooling

**Idee:** Nimm Maximum √ºber alle Tokens (pro Dimension)

```python
def max_pooling(token_embeddings, attention_mask):
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())

    # Setze Padding auf sehr kleinen Wert
    token_embeddings[input_mask_expanded == 0] = -1e9

    # Max √ºber Sequence-Dimension
    return torch.max(token_embeddings, dim=1)[0]
```

**Beispiel:**
```
Token Embeddings:
  Dim 0: [0.1, 0.4, 0.7, 0.2]  ‚Üí Max = 0.7
  Dim 1: [0.2, 0.5, 0.8, 0.3]  ‚Üí Max = 0.8
  Dim 2: [0.3, 0.6, 0.9, 0.4]  ‚Üí Max = 0.9

Max Pooling: [0.7, 0.8, 0.9]
```

**Vorteile:**
- ‚úÖ Beh√§lt st√§rkste Signale

**Nachteile:**
- ‚ö†Ô∏è Kann zu "spiky" Embeddings f√ºhren
- ‚ö†Ô∏è Seltener verwendet als Mean

---

### 4. Weighted Mean (Attention-based)

**Idee:** Gewichte Tokens basierend auf Wichtigkeit

```python
def weighted_mean_pooling(token_embeddings, attention_weights):
    # attention_weights aus letzter Layer
    weighted = token_embeddings * attention_weights.unsqueeze(-1)
    return weighted.sum(dim=1)
```

**Vorteil:** Wichtige W√∂rter z√§hlen mehr

**Nachteil:** Komplexer, nicht immer besser als Mean

---

### Vergleich der Pooling-Strategien

```python
from sentence_transformers import SentenceTransformer
import torch

model = SentenceTransformer('bert-base-uncased')
text = "Der Labork√ºhlschrank ist sehr kalt"

# Token-Embeddings holen
inputs = model.tokenize([text])
with torch.no_grad():
    features = model.forward(inputs)
    token_embeddings = features['token_embeddings']  # [1, seq_len, 768]
    attention_mask = features['attention_mask']       # [1, seq_len]

# Mean Pooling
mean_emb = mean_pooling(token_embeddings, attention_mask)

# CLS Pooling
cls_emb = token_embeddings[:, 0, :]

# Max Pooling
max_emb = max_pooling(token_embeddings, attention_mask)

print("Mean:", mean_emb.shape)  # [1, 768]
print("CLS:", cls_emb.shape)    # [1, 768]
print("Max:", max_emb.shape)    # [1, 768]

# Sind sie √§hnlich?
from sentence_transformers import util
print("Mean vs CLS:", util.cos_sim(mean_emb, cls_emb).item())
print("Mean vs Max:", util.cos_sim(mean_emb, max_emb).item())
```

**Empfehlung:** **Mean Pooling** (Standard in Sentence-Transformers)

---

## Training-Methoden

### Wie lernt ein Embedding-Model?

**Ziel:** Semantisch √§hnliche S√§tze sollen nahe Vektoren haben

### 1. Contrastive Learning

**Prinzip:** Positive Paare n√§her bringen, Negative weiter auseinander

**Dataset:**
```python
positive_pairs = [
    ("K√ºhlschrank", "K√ºhlaggregat"),
    ("Medikament", "Arznei"),
]

negative_pairs = [
    ("K√ºhlschrank", "Banane"),
    ("Medikament", "Auto"),
]
```

**Loss:**
```python
for (anchor, positive) in positive_pairs:
    emb_a = model.encode(anchor)
    emb_p = model.encode(positive)

    loss_positive = distance(emb_a, emb_p)  # Minimize

for (anchor, negative) in negative_pairs:
    emb_a = model.encode(anchor)
    emb_n = model.encode(negative)

    loss_negative = max(0, margin - distance(emb_a, emb_n))  # Push apart
```

---

### 2. Triplet Loss (h√§rter)

**Prinzip:** Anchor n√§her an Positive als an Negative (um mindestens Margin)

```python
triplets = [
    ("K√ºhlschrank", "K√ºhlaggregat", "Banane"),
    #   anchor        positive        negative
]

for (anchor, positive, negative) in triplets:
    emb_a = model.encode(anchor)
    emb_p = model.encode(positive)
    emb_n = model.encode(negative)

    loss = max(0,
        distance(emb_a, emb_p) - distance(emb_a, emb_n) + margin
    )
```

**Visualisierung:**
```
Gutes Triplet:
  Anchor ‚Ä¢---‚Ä¢ Positive          ‚Ä¢ Negative
  (Margin erf√ºllt, kein Loss)

Schlechtes Triplet:
  Anchor ‚Ä¢-------‚Ä¢ Positive
                ‚Ä¢ Negative
  (Negative zu nah! Loss > 0)
```

**Hard Negative Mining:**

Nicht alle Negative sind gleich lehrreich:

```python
# Easy Negative (nutzlos)
("K√ºhlschrank", "Pommes")  # Offensichtlich unterschiedlich

# Hard Negative (sehr lehrreich!)
("Labork√ºhlschrank", "Gefrierschrank")  # √Ñhnlich, aber nicht gleich!
```

**Mining-Strategie:**
```python
# Finde h√§rteste Negatives im Batch
for anchor in batch:
    # Alle anderen als Kandidaten
    candidates = [x for x in batch if x != anchor]

    # Sortiere nach Similarity
    similarities = [cos_sim(anchor, c) for c in candidates]

    # H√§rtestes = √§hnlichstes (aber falsche Klasse)
    hardest_negative = candidates[argmax(similarities)]
```

---

### 3. Multiple Negatives Ranking Loss

**Prinzip:** In einem Batch, jedes Positive-Paar, alle anderen sind Negatives

```python
batch = [
    ("Query 1", "Positive Doc 1"),
    ("Query 2", "Positive Doc 2"),
    ("Query 3", "Positive Doc 3"),
]

# F√ºr Query 1:
#   - Positive: Doc 1
#   - Negatives: Doc 2, Doc 3 (alle anderen Docs im Batch!)
```

**Loss:**
```python
# F√ºr jeden Query
scores = cos_sim(query, all_docs_in_batch)  # [batch_size]
labels = index_of_positive_doc              # z.B. 0 (erstes Doc)

loss = cross_entropy(scores, labels)  # Maximiere Score f√ºr richtiges Doc
```

**Vorteil:** Sehr effizient! Batch als implizites Hard Negative Mining

**Verwendet von:** Sentence-Transformers, E5, BGE, ...

---

### 4. Knowledge Distillation

**Prinzip:** Kleines Model lernt von gro√üem Model

```python
# Teacher (gro√ü, genau, langsam)
teacher = SentenceTransformer('all-mpnet-base-v2')  # 768 dims, 420M params

# Student (klein, schneller, weniger genau)
student = SentenceTransformer('all-MiniLM-L6-v2')   # 384 dims, 22M params

# Training
for sentence in dataset:
    teacher_emb = teacher.encode(sentence)  # "Gold Standard"
    student_emb = student.encode(sentence)

    loss = mse(student_emb, teacher_emb)  # Student imitiert Teacher
```

**Vorteil:** Kleines Model fast so gut wie gro√ües, aber viel schneller!

**Beispiel:** MiniLM (22M params) erreicht 95% Performance von RoBERTa-Large (355M params)

---

## Model-Familien

### √úbersicht der wichtigsten Dense Embedding Models

### 1. BERT-Familie

**BERT-Base / BERT-Large**
- Basis f√ºr viele andere
- 768 / 1024 dims
- Englisch oder Multilingual

**Varianten:**
- **RoBERTa** - Robustly optimized BERT (besseres Training)
- **DeBERTa** - Decoding-enhanced BERT (bessere Attention)
- **ALBERT** - Parameter-Sharing (kleiner)

### 2. Sentence-Transformers Familie

**all-MiniLM-L6-v2** ‚≠ê Empfehlung f√ºr Anfang
- 384 Dimensionen
- 22M Parameter
- Sehr schnell
- Englisch
- [Hugging Face](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)

**all-mpnet-base-v2**
- 768 Dimensionen
- 110M Parameter
- Beste Qualit√§t (SBERT)
- Englisch

**paraphrase-multilingual-MiniLM-L12-v2** ‚≠ê F√ºr Deutsch
- 384 Dimensionen
- Multilingual (50+ Sprachen)
- Gut f√ºr deutsche Texte

**paraphrase-multilingual-mpnet-base-v2**
- 768 Dimensionen
- Multilingual
- H√∂here Qualit√§t als MiniLM

### 3. E5-Familie (Microsoft)

**intfloat/multilingual-e5-large** ‚≠ê‚≠ê Top-Wahl f√ºr Deutsch
- 1024 Dimensionen
- Multilingual
- State-of-the-art Performance
- Trainiert auf 1 Billion Token-Paare!

**Besonderheit:** Nutzt Prefix f√ºr Query vs. Document

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('intfloat/multilingual-e5-large')

# WICHTIG: Prefix verwenden!
query_emb = model.encode("query: Labork√ºhlschrank 280L")
doc_emb = model.encode("passage: Der Kirsch LABO-288 ist ein Labork√ºhlschrank...")

similarity = util.cos_sim(query_emb, doc_emb)
```

**Warum Prefix?**
- Model wurde so trainiert
- Unterscheidet Query (kurz) von Document (lang)
- Bessere Performance!

**Varianten:**
- **e5-small** - 384 dims, schnell
- **e5-base** - 768 dims
- **e5-large** - 1024 dims, beste Qualit√§t

### 4. BGE-Familie (BAAI - Beijing Academy of AI)

**BAAI/bge-large-en-v1.5**
- 1024 Dimensionen
- Englisch
- Sehr gut f√ºr RAG

**BAAI/bge-m3** ‚≠ê Multilingual + Multi-Granularity
- Multilingual (100+ Sprachen)
- Dense + Sparse + Multi-Vector in einem!
- Sehr vielseitig

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-m3')

# Dense Embedding
dense_emb = model.encode("Text")

# Kann auch Sparse und Multi-Vector (advanced)
```

### 5. Deutsche Models

**T-Systems-onsite/german-roberta-sentence-transformer-v2** ‚≠ê Beste rein deutsche Option
- 768 Dimensionen
- Nur Deutsch
- Sehr gut bei Fachsprache

**deutsche-telekom/gbert-large-paraphrase-cosine**
- 1024 Dimensionen
- Nur Deutsch

### 6. Spezialisierte Models

**allenai/specter2** - Scientific Papers
**microsoft/codebert-base** - Code
**emilyalsentzer/Bio_ClinicalBERT** - Medical (Englisch)

---

## Vor- und Nachteile

### Vorteile von Dense Embeddings

‚úÖ **Semantisches Verst√§ndnis**
```python
# Findet Synonyme, Paraphrasen
query = "K√ºhlaggregat"
# Findet: "K√ºhlschrank", "K√ºhlger√§t", "K√§ltemaschine"
```

‚úÖ **Cross-Lingual**
```python
# Multilingual Models
query_de = model.encode("K√ºhlschrank")
query_en = model.encode("refrigerator")
# √Ñhnliche Embeddings! Cross-Language Search m√∂glich
```

‚úÖ **Robustheit gegen Typos**
```python
query = "K√ºhlschronk"  # Typo
# Findet trotzdem "K√ºhlschrank" (√§hnlicher Vektor)
```

‚úÖ **Kontextuelles Verst√§ndnis**
```python
"Bank am Fluss" vs "Bank hat geschlossen"
# Unterschiedliche Embeddings trotz gleichem Wort!
```

---

### Nachteile von Dense Embeddings

‚ùå **Exakte Matches k√∂nnen fehlen**
```python
query = "LABO-288-PRO"  # Modellnummer
# K√∂nnte schlechter matchen als Sparse (BM25)
```

‚ùå **Blackbox**
```python
# Warum rankt Doc A h√∂her als Doc B?
# Schwer zu erkl√§ren (768 Dimensionen!)
```

‚ùå **Rechenintensiv**
```python
# Embedding-Generierung braucht GPU
# F√ºr 1M Dokumente: ~10 Minuten auf GPU
```

‚ùå **Speicher**
```python
# 1M Dokumente √ó 768 dims √ó 4 bytes = ~3GB
# Sparse (BM25) braucht weniger
```

‚ùå **Domain-Shift**
```python
# Trainiert auf Wikipedia
# K√∂nnte schlechter sein f√ºr medizinische Fachsprache
# ‚Üí L√∂sung: Fine-Tuning (separates Kapitel)
```

---

### Wann Dense Embeddings?

‚úÖ **Perfekt f√ºr:**
- Semantische Suche
- Frage-Antwort-Systeme
- Paraphrasen-Erkennung
- Cross-Lingual Retrieval
- √Ñhnliche Dokumente finden

‚ùå **Nicht ideal f√ºr:**
- Exakte Keyword-Suche (‚Üí Sparse)
- Modellnummern, IDs, SKUs (‚Üí Sparse oder Hybrid)
- Sehr lange Dokumente >512 Tokens (‚Üí Chunking oder Sparse)
- Wenn Erkl√§rbarkeit wichtig (‚Üí BM25 zeigt Keyword-Matches)

**Best Practice:** Hybrid (Dense + Sparse) f√ºr Production!

---

## Code-Beispiele

### 1. Basic Usage

```python
from sentence_transformers import SentenceTransformer, util

# Model laden
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Texte
texts = [
    "Labork√ºhlschrank mit 280 Liter Volumen",
    "Medikamentenk√ºhlschrank 280L DIN-konform",
    "Gefrierschrank -20¬∞C",
    "Pommes Frites"
]

# Embeddings generieren
embeddings = model.encode(texts, convert_to_tensor=True)
print(embeddings.shape)  # (4, 384)

# Similarity Matrix
similarities = util.cos_sim(embeddings, embeddings)
print(similarities)
# [[1.00, 0.85, 0.62, 0.12],
#  [0.85, 1.00, 0.58, 0.10],
#  [0.62, 0.58, 1.00, 0.15],
#  [0.12, 0.10, 0.15, 1.00]]
```

---

### 2. Semantic Search

```python
from sentence_transformers import SentenceTransformer, util
import torch

model = SentenceTransformer('intfloat/multilingual-e5-large')

# Dokumente (Produktbeschreibungen)
corpus = [
    "passage: Der Kirsch LABO-288 ist ein Labork√ºhlschrank mit 280L Volumen",
    "passage: Liebherr HMF-4001 Medikamentenk√ºhlschrank nach DIN 13277",
    "passage: Pommes Frites zubereiten in 10 Minuten",
    "passage: Gefrierschrank f√ºr Labor mit -40¬∞C",
]

# Corpus embedden
corpus_embeddings = model.encode(corpus, convert_to_tensor=True)

# Query
query = "query: K√ºhlschrank f√ºr Medikamente mit DIN-Zertifizierung"
query_embedding = model.encode(query, convert_to_tensor=True)

# Suche
hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)[0]

print(f"\nQuery: {query}\n")
for hit in hits:
    print(f"Score: {hit['score']:.4f} - {corpus[hit['corpus_id']]}")

# Output:
# Score: 0.7834 - passage: Liebherr HMF-4001 Medikamentenk√ºhlschrank nach DIN 13277
# Score: 0.6521 - passage: Der Kirsch LABO-288 ist ein Labork√ºhlschrank mit 280L Volumen
# Score: 0.2341 - passage: Gefrierschrank f√ºr Labor mit -40¬∞C
```

---

### 3. Batch Processing (gro√üe Datenmengen)

```python
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

# Gro√üe Anzahl Texte
texts = ["Text " + str(i) for i in range(10000)]

# Batch-Encoding f√ºr Effizienz
embeddings = model.encode(
    texts,
    batch_size=32,        # 32 Texte parallel
    show_progress_bar=True,
    convert_to_numpy=True
)

print(embeddings.shape)  # (10000, 384)

# Speichern
np.save('embeddings.npy', embeddings)

# Laden
embeddings_loaded = np.load('embeddings.npy')
```

---

### 4. Mit Normalisierung

```python
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

texts = ["K√ºhlschrank", "Refrigerator"]
embeddings = model.encode(texts)

# Normalisieren (L2-Norm)
embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

# Pr√ºfen
print(np.linalg.norm(embeddings_normalized[0]))  # 1.0
print(np.linalg.norm(embeddings_normalized[1]))  # 1.0

# Jetzt: Cosine = Dot Product
cosine = np.dot(embeddings_normalized[0], embeddings_normalized[1])
print(f"Similarity: {cosine:.4f}")
```

---

### 5. Custom Pooling

```python
from sentence_transformers import SentenceTransformer
import torch

model = SentenceTransformer('bert-base-uncased')

def custom_pooling(token_embeddings, attention_mask):
    # Gewichte erste Tokens h√∂her (oft wichtiger!)
    seq_len = token_embeddings.size(1)
    weights = torch.linspace(1.0, 0.5, seq_len).to(token_embeddings.device)

    # Gewichteter Durchschnitt
    weighted = token_embeddings * weights.unsqueeze(0).unsqueeze(-1)
    sum_embeddings = torch.sum(weighted * attention_mask.unsqueeze(-1), dim=1)
    sum_mask = torch.sum(attention_mask, dim=1, keepdim=True)

    return sum_embeddings / sum_mask

# Nutze Custom Pooling
# (siehe Sentence-Transformers Docs f√ºr Integration)
```

---

## üöÄ Was du danach kannst

**Architektur-Verst√§ndnis:**
- Du verstehst die Transformer-Architektur und Self-Attention Mechanismus
- Du erkennst den Unterschied zwischen BERT (Token-Level) und Sentence-BERT
- Du verstehst verschiedene Pooling-Strategien und ihre Anwendungsf√§lle

**Praktische Implementierung:**
- Du w√§hlst das richtige Dense Embedding Model f√ºr deinen Use Case
- Du implementierst effiziente Batch-Processing f√ºr gro√üe Datenmengen
- Du optimierst Model-Performance f√ºr deutsche und englische Texte

**Production-Skills:**
- Du verstehst Training-Methoden (Contrastive, Triplet, Multiple Negatives)
- Du integrierst Dense Embeddings in RAG-Systeme
- Du kombinierst Dense mit Sparse Embeddings f√ºr optimale Retrieval-Performance

### Model-Empfehlungen nach Use Case:
- **Deutsch + schnell:** `multilingual-e5-small`
- **Deutsch + genau:** `multilingual-e5-large`
- **Englisch + schnell:** `all-MiniLM-L6-v2`
- **Englisch + genau:** `all-mpnet-base-v2`

## üîó Weiterf√ºhrende Themen

**Verwandte Kapitel:**
- [03-sentence-embeddings.md](03-sentence-embeddings.md) - Sparse Embeddings als Alternative
- [04-dense-vs-sparse.md](04-dense-vs-sparse.md) - Dense + Sparse Hybrid-Strategien
- [08-model-selection.md](08-model-selection.md) - Systematische Model-Evaluation

**Papers & Ressourcen:**
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. 2017 (Transformer)
- [BERT Paper](https://arxiv.org/abs/1810.04805) - Devlin et al. 2018
- [Sentence-BERT](https://arxiv.org/abs/1908.10084) - Reimers & Gurevych 2019

**Code & Tools:**
- [Sentence-Transformers Library](https://www.sbert.net/) - Production-ready Implementation
- [Hugging Face Transformers](https://huggingface.co/transformers/) - Model Hub
