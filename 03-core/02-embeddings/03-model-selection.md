# Model Selection: Der richtige Embedding-Space f√ºr deinen Use Case

## ‚ùì Das Problem (Problem-First)

**Ohne richtiges Model-Verst√§ndnis geht folgendes schief:**
- **Embedding Spaces sind inkompatibel**: Du mischt Embeddings von Model A mit Model B ‚Üí v√∂llig sinnlose Similarity-Werte
- **Evaluation ist Model-abh√§ngig**: Recall@10 mit Sentence-BERT ‚â† Recall@10 mit E5 ‚Üí du kannst Models nicht vergleichen
- **Falsches Model = schlechte Performance**: Englisch-only Model f√ºr deutsche Texte ‚Üí Qualit√§tsverlust von 40%+

**Die zentrale Frage:**
Warum erzeugen verschiedene Models verschiedene "Welten" (Embedding Spaces) und wie w√§hle ich das richtige Model f√ºr meinen Use Case?

**Beispiel-Szenario:**
```python
# Model A
model_a = SentenceTransformer('all-MiniLM-L6-v2')
emb_a = model_a.encode("Labork√ºhlschrank")  # [0.23, -0.45, ..., 0.12]

# Model B
model_b = SentenceTransformer('multilingual-e5-large')
emb_b = model_b.encode("Labork√ºhlschrank")  # [0.89, 0.12, ..., -0.67]

# ‚ùå FALSCH: Similarity zwischen A und B
cosine_sim(emb_a, emb_b)  # 0.34 ‚Üê Sinnlos!
# Wie Celsius und Fahrenheit vergleichen!

# ‚úì RICHTIG: Beide vom gleichen Model
emb_1 = model_a.encode("K√ºhlschrank")
emb_2 = model_a.encode("Gefrierschrank")
cosine_sim(emb_1, emb_2)  # 0.78 ‚Üê Sinnvoll!
```

## üéØ Lernziele

Nach diesem Kapitel kannst du:
- [ ] Du verstehst dass jedes Model einen eigenen Embedding-Space erzeugt (nicht vergleichbar!)
- [ ] Du w√§hlst das richtige Model basierend auf Sprache, Domain, Qualit√§ts-Anforderungen
- [ ] Du erkennst wann Model-Wechsel alle Embeddings neu berechnen erfordert

## üß† Intuition zuerst (Scaffolded Progression)

### Alltagsanalogie: Verschiedene Landkarten

**Beispiel aus dem echten Leben:**

```
Google Maps (Model A):
  Berlin ‚Üí [52.52¬∞N, 13.40¬∞E]
  M√ºnchen ‚Üí [48.14¬∞N, 11.58¬∞E]
  ‚Üí Distanz: 504 km

Mittelalterliche Karte (Model B):
  Berlin ‚Üí [7cm von links, 12cm von oben]
  M√ºnchen ‚Üí [5cm von links, 18cm von oben]
  ‚Üí Distanz: 6.3 cm

‚ùå FALSCH: Google-Koordinate mit mittelalterlicher Koordinate vergleichen!
‚úì RICHTIG: Nur Punkte auf DERSELBEN Karte vergleichen
```

**Bei Embeddings genauso:**
- Jedes Model = eigene "Karte" (Embedding Space)
- Punkte auf verschiedenen Karten vergleichen = sinnlos
- Nur Punkte auf GLEICHER Karte vergleichen!

### Visualisierung: Model Spaces

```
Model A Space:              Model B Space:
    K√ºhl-                      Apfel
    schrank                      ‚Ä¢
       ‚Ä¢
                                K√ºhl-
    Gefrier-                   schrank
    schrank                       ‚Ä¢
       ‚Ä¢           VS.
                                Gefrier-
    Apfel                      schrank
       ‚Ä¢                          ‚Ä¢

Gleiche Objekte, aber VERSCHIEDENE Positionen!
‚Üí Distanzen NICHT vergleichbar
```

### Die Br√ºcke zur Mathematik

**Intuition:** Jedes Model lernt eigene Transformation Text ‚Üí Vektor

**Mathematisch:**
```
Model A: f_A: Text ‚Üí ‚Ñù^384
Model B: f_B: Text ‚Üí ‚Ñù^1024

f_A("K√ºhlschrank") ‚â† f_B("K√ºhlschrank")
‚Üí Verschiedene Vektorr√§ume!
‚Üí Verschiedene Dimensionen!
‚Üí Verschiedene "Bedeutung" pro Dimension!
```

**Warum?**
- Training-Daten unterschiedlich
- Architektur unterschiedlich
- Objektive (Loss) unterschiedlich
‚Üí Gelernte Repr√§sentation unterschiedlich!

## üßÆ Das Konzept verstehen

### 1. Embedding Spaces sind Model-spezifisch

#### Was ist ein Embedding Space?

**Definition:**
Der hochdimensionale Raum den ein Model "gelernt" hat, um Bedeutung zu repr√§sentieren.

```python
# Model A lernt einen Space:
model_a = SentenceTransformer('all-MiniLM-L6-v2')
# ‚Üí Space: ‚Ñù^384
# ‚Üí Training: NLI + Paraphrase Datasets
# ‚Üí Objective: Contrastive Loss

# Model B lernt ANDEREN Space:
model_b = SentenceTransformer('multilingual-e5-large')
# ‚Üí Space: ‚Ñù^1024
# ‚Üí Training: 1B multilingual pairs
# ‚Üí Objective: Multiple Negatives Ranking Loss

# VERSCHIEDENE Spaces!
```

#### Warum sind Spaces inkompatibel?

**Grund 1: Verschiedene Dimensionen**
```python
model_a.encode("text").shape  # (384,)
model_b.encode("text").shape  # (1024,)

# Kann man nicht mal dot product machen!
```

**Grund 2: Selbst bei gleicher Dimension - verschiedene "Bedeutung"**
```python
# Beide 768-dim, aber:
model_1 = SentenceTransformer('bert-base')
model_2 = SentenceTransformer('roberta-base')

emb_1 = model_1.encode("K√ºhlschrank")  # [0.1, 0.2, ..., 0.3]
emb_2 = model_2.encode("K√ºhlschrank")  # [0.9, -0.5, ..., 0.1]

# Dimension 0 bedeutet bei Model 1 vielleicht "Formalit√§t"
# Dimension 0 bedeutet bei Model 2 vielleicht "Sentiment"
# ‚Üí V√∂llig verschiedene semantische R√§ume!
```

#### Implikation f√ºr Vector Databases

**Kritisch wichtig:**
```python
# Vector DB mit Model A bef√ºllt
vectordb = ChromaDB()
for doc in corpus:
    emb = model_a.encode(doc)
    vectordb.add(doc, emb)

# ‚ùå FALSCH: Query mit Model B
query_emb = model_b.encode(query)
results = vectordb.search(query_emb)  # Garbage results!

# ‚úì RICHTIG: Query mit gleichem Model
query_emb = model_a.encode(query)
results = vectordb.search(query_emb)  # Sinnvolle Results!
```

**Regel:**
```
Ein Embedding Space pro Vector Database.
Model-Wechsel = Alle Embeddings neu berechnen.
```

### 2. Evaluation ist Model-abh√§ngig

#### Das Problem

**Verschiedene Models ‚Üí verschiedene Retrieval-Qualit√§t:**

```python
# Test-Set: 1000 Query-Doc Pairs
queries = ["K√ºhlschrank 280L", ...]
relevant_docs = [["Doc_123", "Doc_456"], ...]

# Model A
model_a = SentenceTransformer('all-MiniLM-L6-v2')
recall_a = compute_recall_at_10(model_a, queries, relevant_docs)
# Recall@10 = 0.75

# Model B
model_b = SentenceTransformer('multilingual-e5-large')
recall_b = compute_recall_at_10(model_b, queries, relevant_docs)
# Recall@10 = 0.82

# Model B ist besser f√ºr diesen Use Case!
```

**Warum unterschiedlich?**
- Model B lernte bessere Repr√§sentation f√ºr diese Domain
- Model B ist multilingual ‚Üí generalisiert besser
- Model B gr√∂√üer (1024 vs 384 dims) ‚Üí mehr Kapazit√§t

#### MTEB Benchmark verstehen

**Massive Text Embedding Benchmark:**

| Model | Avg Score | Retrieval | Classification | Clustering |
|-------|-----------|-----------|----------------|------------|
| **E5-large-v2** | 66.1 | 52.8 | 75.0 | 44.5 |
| **all-mpnet-base-v2** | 63.3 | 50.2 | 73.8 | 43.7 |
| **all-MiniLM-L6-v2** | 58.8 | 41.9 | 66.8 | 42.4 |

**Interpretation:**
- **Avg Score**: Durchschnitt √ºber 56 Datasets
- **Retrieval**: Relevant f√ºr RAG/Search
- **Classification**: Relevant f√ºr Categorization
- **Clustering**: Relevant f√ºr Topic Modeling

**Wichtig:**
```
MTEB-Score ist NICHT absolut!
‚Üí Dein Use Case kann anders performen
‚Üí Immer auf eigenem Dataset evaluieren!
```

### 3. Model-Auswahl Kriterien

#### Kriterium 1: Sprache

```python
# Englisch-only Use Case:
model = SentenceTransformer('all-mpnet-base-v2')  # ‚úì

# Deutsch/Multilingual:
model = SentenceTransformer('all-mpnet-base-v2')  # ‚ùå Schlecht!
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')  # ‚úì

# Test:
emb_en = model.encode("refrigerator")
emb_de = model.encode("K√ºhlschrank")
similarity = cosine_sim(emb_en, emb_de)

# Englisch-only Model: 0.3 (schlecht!)
# Multilingual Model: 0.85 (gut!)
```

#### Kriterium 2: Domain

```python
# General Domain (Wikipedia, News):
model = SentenceTransformer('all-mpnet-base-v2')  # ‚úì

# Specific Domain:
# Legal:
model = SentenceTransformer('nlpaueb/legal-bert-base-uncased')

# Medical:
model = SentenceTransformer('emilyalsentzer/Bio_ClinicalBERT')

# Scientific Papers:
model = SentenceTransformer('allenai/specter2')

# Performance-Unterschied: 10-30% bei Domain-spezifischen Tasks!
```

#### Kriterium 3: Qualit√§t vs. Speed

| Model | Dims | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| **MiniLM-L6** | 384 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Prototyping, Hobby |
| **MPNet-base** | 768 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Production (balanced) |
| **E5-large** | 1024 | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | High-quality required |

```python
# Benchmark auf deinem Corpus:
import time

models = [
    'all-MiniLM-L6-v2',
    'all-mpnet-base-v2',
    'intfloat/multilingual-e5-large'
]

for model_name in models:
    model = SentenceTransformer(model_name)

    start = time.time()
    embeddings = model.encode(corpus[:1000])
    duration = time.time() - start

    recall = evaluate_retrieval(model, test_queries)

    print(f"{model_name}:")
    print(f"  Time: {duration:.2f}s")
    print(f"  Recall@10: {recall:.3f}")
```

## ‚ö†Ô∏è H√§ufige Missverst√§ndnisse

### ‚ùå Missverst√§ndnis 1: "Ich kann Models mischen"

**Warum falsch:**
```python
# 50% der Docs mit Model A embedded
vectordb.add_batch(docs[:500], model_a.encode(docs[:500]))

# 50% mit Model B embedded
vectordb.add_batch(docs[500:], model_b.encode(docs[500:]))

# Query mit Model A
query_emb = model_a.encode(query)
results = vectordb.search(query_emb, k=10)

# Problem: Docs[500:] haben Random-Scores!
# ‚Üí Retrieval komplett kaputt
```

**‚úì Richtig:**
EIN Model pro Vector Database. Bei Wechsel: Alles neu embedden.

**Merksatz:** "Ein Space, ein Model - niemals mischen!"

### ‚ùå Missverst√§ndnis 2: "Mehr Dimensionen = bessere Qualit√§t"

**Warum falsch:**
```python
# Schlecht trainiertes 1024-dim Model
model_bad = SomeRandomModel(dim=1024)
# Recall@10: 0.45

# Gut trainiertes 384-dim Model
model_good = SentenceTransformer('all-MiniLM-L6-v2')
# Recall@10: 0.68

# Training-Daten + Architektur > Dimensionalit√§t!
```

**‚úì Richtig:**
Benchmark auf eigenem Dataset, nicht nur auf Dimensionen schauen.

**Merksatz:** "Qualit√§t kommt vom Training, nicht von der Gr√∂√üe!"

### ‚ùå Missverst√§ndnis 3: "MTEB Top-1 ist immer am besten"

**Warum falsch:**
```python
# MTEB Top-1 auf English News-Corpus
model_mteb = SentenceTransformer('best-mteb-model')
# Dein Use Case: Deutsche Medizin-Texte

# Performance:
recall_mteb = 0.52  # Schlecht!

# Domain-spezifisches Model:
model_medical_de = SentenceTransformer('german-medical-bert')
recall_medical = 0.79  # Viel besser!

# MTEB misst General Performance!
# Dein spezifischer Use Case kann anders sein!
```

**‚úì Richtig:**
MTEB als Orientierung, aber immer selbst evaluieren.

**Merksatz:** "MTEB zeigt Trends, dein Dataset zeigt Wahrheit!"

## üî¨ Hands-On: Model-Auswahl praktisch

```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

# Test-Corpus (Deutsch, Medizin-Domain)
corpus = [
    "Labork√ºhlschrank mit 280 Liter Volumen DIN EN 13277",
    "Medikamentenk√ºhlschrank f√ºr Apotheken",
    "Blutkonservenk√ºhlschrank mit Alarmfunktion",
    # ... weitere Docs
]

queries = [
    "K√ºhlschrank f√ºr Medikamente",
    "Labor K√ºhlung DIN",
    # ... weitere Queries
]

# Ground Truth: Welche Docs relevant f√ºr Query?
ground_truth = {
    0: [0, 1],  # Query 0 ‚Üí Doc 0, 1 relevant
    1: [0, 2],  # Query 1 ‚Üí Doc 0, 2 relevant
}

# Models zum Testen
models_to_test = [
    'all-MiniLM-L6-v2',  # Englisch-only
    'paraphrase-multilingual-MiniLM-L12-v2',  # Multilingual
    'intfloat/multilingual-e5-large',  # Multilingual SOTA
]

def evaluate_model(model_name, corpus, queries, ground_truth):
    model = SentenceTransformer(model_name)

    # Embed corpus
    corpus_embs = model.encode(corpus, convert_to_tensor=True)

    # Evaluate
    recalls = []
    for q_idx, query in enumerate(queries):
        query_emb = model.encode(query, convert_to_tensor=True)

        # Search
        hits = util.semantic_search(query_emb, corpus_embs, top_k=10)[0]
        retrieved = [hit['corpus_id'] for hit in hits]

        # Recall@10
        relevant = ground_truth[q_idx]
        recall = len(set(retrieved) & set(relevant)) / len(relevant)
        recalls.append(recall)

    avg_recall = np.mean(recalls)
    return avg_recall

# Benchmark
print("Model Performance auf deutschem Medizin-Corpus:\n")
for model_name in models_to_test:
    recall = evaluate_model(model_name, corpus, queries, ground_truth)
    print(f"{model_name:50} Recall@10: {recall:.3f}")

# Erwartetes Ergebnis:
# all-MiniLM-L6-v2                             Recall@10: 0.450  (Englisch-only!)
# paraphrase-multilingual-MiniLM-L12-v2        Recall@10: 0.750  (Multilingual)
# intfloat/multilingual-e5-large               Recall@10: 0.850  (Best!)
```

## ‚è±Ô∏è 5-Minuten-Experte

### 1. Frage: Kann ich Model wechseln ohne neu zu embedden?

<details><summary>üí° Antwort</summary>

**Nein, niemals!**

Embedding Spaces sind Model-spezifisch. Wechsel erfordert:
1. Alle Docs neu embedden mit neuem Model
2. Vector DB neu bef√ºllen
3. Downtime oder Blue-Green Deployment

**Ausnahme:** Modell-Distillation wo Student explizit Teacher-Space imitiert (selten).

**Merksatz:** "Model-Wechsel = Rebuild Everything"
</details>

### 2. Frage: E5-large vs. MPNet - welches f√ºr Production?

<details><summary>üí° Antwort</summary>

**Kommt auf Requirements an:**

**E5-large wenn:**
- Multilingual wichtig
- Qualit√§t > Latenz
- Budget f√ºr 1024-dim Vektoren

**MPNet wenn:**
- Nur Englisch
- Latenz wichtig
- Sparsamkeit (768-dim = 25% weniger Speicher)

**Benchmark beide auf eigenem Dataset!**
</details>

### 3. Frage: Domain-specific finetunen oder pretrained nehmen?

<details><summary>üí° Antwort</summary>

**Decision Tree:**

```
Hast du >10k Domain-Paare (Query-Doc)?
‚îú‚îÄ Ja ‚Üí Finetune (10-30% Improvement)
‚îî‚îÄ Nein ‚Üí Pretrained (zu wenig Daten)

Budget f√ºr Training?
‚îú‚îÄ Ja ‚Üí Finetune ab 1k Paaren mit Few-Shot
‚îî‚îÄ Nein ‚Üí Best pretrained multilingual Model
```

**Merksatz:** "Finetune wenn Daten + Budget, sonst best pretrained"
</details>

## üìä Model-Empfehlungen

### Nach Use Case

| Use Case | Empfehlung | Warum? |
|----------|------------|--------|
| **Prototyping (Deutsch)** | `paraphrase-multilingual-MiniLM-L12-v2` | Schnell, multilingual, gut genug |
| **Production RAG (Multiingual)** | `intfloat/multilingual-e5-large` | SOTA Qualit√§t, 100+ Sprachen |
| **Production RAG (Englisch)** | `all-mpnet-base-v2` | Balanced, battle-tested |
| **Legal (Englisch)** | `nlpaueb/legal-bert-base-uncased` | Domain-optimiert |
| **Medical (Englisch)** | `emilyalsentzer/Bio_ClinicalBERT` | PubMed trainiert |
| **Scientific Papers** | `allenai/specter2` | Citation-aware |

### Nach Constraint

| Constraint | Empfehlung | Trade-off |
|------------|------------|-----------|
| **Latenz <50ms** | `all-MiniLM-L6-v2` | -10% Qualit√§t |
| **Speicher <2GB** | `all-MiniLM-L6-v2` | Kleinere Embeddings |
| **Mobile/Edge** | `all-MiniLM-L6-v2` + Quantization | -15% Qualit√§t |
| **Maximale Qualit√§t** | `intfloat/multilingual-e5-large` | 3x langsamer |

## üõ†Ô∏è Tools f√ºr Model Selection

```python
# MTEB Benchmark
from mteb import MTEB

model = SentenceTransformer('your-model')
evaluation = MTEB(tasks=["Retrieval"])
results = evaluation.run(model)

# Eigenes Benchmark
def benchmark_on_your_data(model_names, corpus, queries, ground_truth):
    results = {}
    for name in model_names:
        model = SentenceTransformer(name)
        recall = evaluate_retrieval(model, corpus, queries, ground_truth)
        latency = measure_encoding_time(model, corpus[:100])
        results[name] = {"recall": recall, "latency": latency}
    return results
```

## üöÄ Was du jetzt kannst

**Verst√§ndnis:**
- ‚úì Du verstehst dass Embedding Spaces Model-spezifisch und inkompatibel sind
- ‚úì Du erkennst dass Evaluation Model-abh√§ngig ist
- ‚úì Du verstehst MTEB als Orientierung, nicht als absolute Wahrheit

**Praktische F√§higkeiten:**
- ‚úì Du benchmarkst verschiedene Models auf eigenem Dataset
- ‚úì Du w√§hlst Model basierend auf Sprache, Domain, Constraints
- ‚úì Du planst Model-Migration (alle Embeddings neu)

**Kritisches Denken:**
- ‚úì Du mischt niemals Embeddings verschiedener Models
- ‚úì Du evaluierst vor Production-Deployment
- ‚úì Du verstehst Trade-offs (Qualit√§t vs. Speed vs. Speicher)

## üîó Weiterf√ºhrende Themen

**N√§chster logischer Schritt:**
‚Üí [04-vector-databases.md](04-vector-databases.md) - Vector DBs, Quantization, Deployment Patterns
‚Üí [../../04-advanced/02-retrieval-optimization.md](../../04-advanced/02-retrieval-optimization.md) - Chunking Strategies, Two-Stage Retrieval, Hybrid Search

**Praktische Anwendung:**
‚Üí [../../06-applications/rag-systems.md](../../06-applications/rag-systems.md) - Vollst√§ndiges RAG-System mit Model Selection

**Verwandte Konzepte:**
- [../evaluation/metrics.md](../evaluation/metrics.md) - Recall@k, MRR, nDCG f√ºr Retrieval-Qualit√§t
- [../training/contrastive-learning.md](../training/contrastive-learning.md) - Wie Embedding Models trainiert werden
