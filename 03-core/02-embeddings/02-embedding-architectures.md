# Embedding Architectures: Wie lernen Maschinen Bedeutung?

## â“ Das Problem (Problem-First)

**Ohne Embedding-Architekturen geht folgendes schief:**
- **Nur Keyword-Matching**: "KÃ¼hlschrank" findet nicht "KÃ¼hlaggregat" oder "refrigerator" - semantisches VerstÃ¤ndnis fehlt
- **Bag-of-Words verliert Kontext**: "Bank am Fluss" vs. "Bank hat geschlossen" - gleiches Wort, vÃ¶llig andere Bedeutung
- **Skalierung unmÃ¶glich**: Dense findet Synonyme aber verliert exakte Matches, BM25 findet Keywords aber keine Semantik - du musst zwischen beidem wÃ¤hlen

**Die zentrale Frage:**
Welche Embedding-Architektur nutze ich wann? Dense, Sparse, Multi-Vector oder Cross-Encoder - und warum gibt es Ã¼berhaupt so viele verschiedene AnsÃ¤tze?

**Beispiel-Szenario:**
```python
Query: "KÃ¼hlgerÃ¤t fÃ¼r Arzneimittel mit TemperaturÃ¼berwachung"

# BM25 (Sparse): Findet nur exakte WÃ¶rter
Doc 1: "KÃ¼hlgerÃ¤t Arzneimittel TemperaturÃ¼berwachung" â†’ Score: 5.2 âœ“
Doc 2: "MedikamentenkÃ¼hlschrank mit Alarm-System"     â†’ Score: 0.0 âœ—

# Dense: Versteht Semantik, verliert aber PrÃ¤zision
Doc 1: "KÃ¼hlgerÃ¤t Arzneimittel TemperaturÃ¼berwachung" â†’ Score: 0.82 âœ“
Doc 2: "MedikamentenkÃ¼hlschrank mit Alarm-System"     â†’ Score: 0.79 âœ“
Doc 3: "Labor-Equipment KÃ¼hlung"                       â†’ Score: 0.75 â“ (zu fuzzy!)

# Was wir wirklich brauchen: Beides!
```

## ğŸ¯ Lernziele

Nach diesem Kapitel kannst du:
- [ ] Du verstehst die 4 Haupt-Architekturen (Dense, Sparse, Multi-Vector, Cross-Encoder) und ihre Trade-offs
- [ ] Du kannst begrÃ¼ndet entscheiden welche Architektur fÃ¼r deinen Use Case passt
- [ ] Du implementierst alle 4 Architekturen praktisch und verstehst wann Hybrid-AnsÃ¤tze nÃ¶tig sind

## ğŸ§  Intuition zuerst (Scaffolded Progression)

### Alltagsanalogie: Verschiedene Arten zu suchen

**Beispiel aus dem echten Leben: Buch in einer Bibliothek finden**

**1. Index-Suche (= Sparse / BM25):**
```
Du suchst: "Quantenphysik"
â†’ Schaust im Index: "Quantenphysik" â†’ Seite 142, 287, 453
â†’ Sehr schnell, aber nur EXAKTE Begriffe
â†’ Findet NICHT: "Quantenmechanik", "Teilchenphysik"
```

**2. Inhaltliche Suche (= Dense Embeddings):**
```
Du fragst Bibliothekar: "Etwas Ã¼ber Quantenphysik"
â†’ Bibliothekar versteht: "Ah, du meinst moderne Physik, Teilchen..."
â†’ Zeigt dir auch: Quantenmechanik, Heisenberg, SchrÃ¶dinger
â†’ Langsamer, aber VERSTEHT was du meinst
```

**3. Wort-fÃ¼r-Wort Vergleich (= Multi-Vector / ColBERT):**
```
Du vergleichst jedes Wort deiner Notiz mit jedem Wort im Buch
â†’ "Quanten" matched "Quantenmechanik" âœ“
â†’ "Physik" matched "Physik" âœ“
â†’ Sehr prÃ¤zise, aber aufwÃ¤ndig
```

**4. Bibliothekar liest beide (= Cross-Encoder):**
```
Bibliothekar liest deine Frage UND das Buch zusammen
â†’ Perfektes VerstÃ¤ndnis ob es passt
â†’ Aber: Muss JEDES Buch lesen (sehr langsam!)
```

### Visualisierung: Die 4 Architekturen

```
                    Geschwindigkeit
                         â†‘
    Sparse (BM25)        â”‚      Dense (Sentence-BERT)
         âš¡âš¡âš¡            â”‚            âš¡âš¡
    Exakte Keywords      â”‚       Semantisch
    Keine Semantik       â”‚       Fuzzy Matches
                         â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Genauigkeit
                         â”‚
    Multi-Vector         â”‚      Cross-Encoder
    (ColBERT)            â”‚        (Re-Ranker)
         âš¡              â”‚             âš¡
    Token-Level          â”‚       Maximale
    PrÃ¤zision            â”‚       Genauigkeit
                         â†“
                   Rechenaufwand
```

**Trade-off verstehen:**
- **Links-Oben (Sparse)**: Schnell, aber nur Keywords
- **Rechts-Oben (Dense)**: Balance von Speed + Semantik
- **Links-Unten (Multi-Vector)**: PrÃ¤zise, aber langsamer
- **Rechts-Unten (Cross-Encoder)**: Perfekt, aber nur fÃ¼r Re-Ranking

### Die BrÃ¼cke zur Mathematik

**Intuition:** Verschiedene Wege Text in Zahlen zu verwandeln

**Mathematisch:**
- **Sparse**: $v \in \mathbb{R}^{|V|}$ wo 99% = 0 (nur WÃ¶rter die vorkommen)
- **Dense**: $v \in \mathbb{R}^d$ wo alle Werte $\neq 0$ (d = 384-1024)
- **Multi-Vector**: $M \in \mathbb{R}^{n \times d}$ (n Token-Vektoren statt einem)
- **Cross-Encoder**: $f(query, doc) \rightarrow [0, 1]$ (keine Vektoren!)

## ğŸ§® Das Konzept verstehen

### 1. Dense Embeddings (Sentence-BERT)

#### Wie funktioniert es?

**Architektur:**
```
Text: "LaborkÃ¼hlschrank 280L"
  â†“
[Tokenization]
  ["Labor", "##kÃ¼hl", "##schrank", "280", "L"]
  â†“
[BERT Transformer Ã— 12 Layer]
  Self-Attention lernt Kontext
  â†“
Token-Embeddings: [5, 768]
  â†“
[Mean Pooling]
  Durchschnitt Ã¼ber alle Token
  â†“
Sentence-Embedding: [768]
  [0.234, -0.456, 0.123, ..., -0.234]
```

**Intuition hinter der Formel:**

Mean Pooling:
$$\text{emb}_{\text{sentence}} = \frac{1}{n} \sum_{i=1}^{n} \text{emb}_{\text{token}_i}$$

**Warum Mean Pooling?**
- Alle Token tragen bei
- DokumentlÃ¤nge spielt keine Rolle (normalisiert)
- Robust und funktioniert praktisch immer

**Schritt-fÃ¼r-Schritt:**

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 1. Text â†’ Tokens
text = "LaborkÃ¼hlschrank"
# Intern: ["Labor", "##kÃ¼hl", "##schrank"]

# 2. Transformer Processing
# Self-Attention: Jedes Token schaut auf alle anderen
# "##schrank" lernt von "Labor" und "##kÃ¼hl"

# 3. Pooling
embedding = model.encode(text)  # [384]

# 4. Similarity
query_emb = model.encode("MedikamentenkÃ¼hlschrank")
from sentence_transformers import util
similarity = util.cos_sim(embedding, query_emb)
# â†’ 0.87 (sehr Ã¤hnlich!)
```

#### Training: Contrastive Learning

**Prinzip:** Ã„hnliche Texte â†’ Ã¤hnliche Vektoren

```
Positive Pair:
  "LaborkÃ¼hlschrank 280L"
  "Medizinischer KÃ¼hlschrank 280 Liter"
  â†’ Embeddings sollen NAH sein

Negative Pair:
  "LaborkÃ¼hlschrank 280L"
  "Pommes Frites"
  â†’ Embeddings sollen WEIT sein
```

**Loss-Funktion (Multiple Negatives Ranking):**

$$\mathcal{L} = -\log \frac{e^{\text{sim}(q, p^+)}}{\sum_{p \in \{p^+, p^-_1, ..., p^-_k\}} e^{\text{sim}(q, p)}}$$

**In Worten:**
- Maximiere Similarity zum positiven Beispiel
- Minimiere Similarity zu allen negativen Beispielen im Batch

#### Varianten & Trade-offs

| Model | Dimensionen | Speed | QualitÃ¤t | Use-Case |
|-------|-------------|-------|----------|----------|
| **all-MiniLM-L6-v2** | 384 | âš¡âš¡âš¡ | â­â­â­ | Schnelles Prototyping |
| **all-mpnet-base-v2** | 768 | âš¡âš¡ | â­â­â­â­ | Production (Englisch) |
| **multilingual-e5-large** | 1024 | âš¡ | â­â­â­â­â­ | Multilinguale Production |

---

### 2. Sparse Embeddings (BM25 & SPLADE)

#### Klassisch: BM25

**Intuition:** Nur WÃ¶rter die vorkommen haben Gewichte

```python
Text: "LaborkÃ¼hlschrank 280L"

# Sparse Vector (30.000 Dimensionen)
{
    5421: 3.2,   # "LaborkÃ¼hlschrank"
    8934: 1.5,   # "280L"
    # Alle anderen 29.998 Dimensionen: 0
}
```

**BM25 Formel:**

$$\text{BM25}(q, d) = \sum_{t \in q} \text{IDF}(t) \cdot \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}$$

**Intuition hinter der Formel:**
- **IDF**: Seltene WÃ¶rter = wichtiger
- **Saturation ($k_1$)**: Nach 2-3 Vorkommen zÃ¤hlt mehr nicht viel
- **Length Norm ($b$)**: Kurze Docs werden bevorzugt

**Warum dieser Ansatz?**
- TF-IDF wÃ¤chst linear (10Ã— Wort = 10Ã— Score) â†’ unrealistisch
- BM25 saturiert (10Ã— Wort â‰ˆ 2Ã— Score) â†’ realistischer

#### Modern: SPLADE (Learned Sparse)

**Das Beste aus beiden Welten:**

```python
Input: "KÃ¼hlgerÃ¤t"

# BM25 Output (nur Input):
{
    "KÃ¼hlgerÃ¤t": 3.5
}

# SPLADE Output (+ Expansions!):
{
    "KÃ¼hlgerÃ¤t": 3.5,      # Original
    "KÃ¼hlschrank": 2.1,    # Gelernt!
    "KÃ¼hlanlage": 1.8,     # Synonym
    "refrigerator": 0.9,   # Cross-lingual!
}
```

**Architektur:**

```
Text: "KÃ¼hlgerÃ¤t"
  â†“
[BERT Transformer]
  â†“
[MLM Head]  â† Vorhersage fÃ¼r JEDES Wort im Vokabular
  [30.000] logits
  â†“
[ReLU + Log(1+x)]  â† Sparsity
  â†“
Sparse Vector: nur ~50 non-zero
```

**Training:** Contrastive + Sparsity Regularization

$$\mathcal{L} = \mathcal{L}_{\text{ranking}} + \lambda \cdot \|v\|_1$$

- $\mathcal{L}_{\text{ranking}}$: Positive Docs ranken hÃ¶her
- $\lambda \cdot \|v\|_1$: Strafe fÃ¼r zu viele non-zero Werte

#### Varianten & Trade-offs

| Methode | Semantik | Speed | Speicher | Use-Case |
|---------|----------|-------|----------|----------|
| **BM25** | âŒ | âš¡âš¡âš¡ | âœ…âœ…âœ… | Keyword-Search, Baseline |
| **SPLADE** | âœ… | âš¡âš¡ | âœ…âœ… | Production Hybrid, Best of Both |

---

### 3. Multi-Vector Embeddings (ColBERT)

#### Das Problem mit Single-Vector Dense

**Informationsverlust durch Pooling:**

```python
Text: "LaborkÃ¼hlschrank mit TemperaturÃ¼berwachung"

# Dense (Single Vector):
embedding = mean([emb_Labor, emb_kÃ¼hlschrank, emb_Temperatur, ...])
# â†’ Ein 768-dim Vektor
# â†’ Details gehen verloren!

# Multi-Vector (ColBERT):
embeddings = [emb_Labor, emb_kÃ¼hlschrank, emb_Temperatur, ...]
# â†’ 5Ã— 128-dim Vektoren
# â†’ Jedes Wort behÃ¤lt eigene Nuance!
```

#### ColBERT Architecture

**Late Interaction:**

```
Query: "KÃ¼hlschrank Alarm"
  â†“ BERT
[emb_KÃ¼hlschrank, emb_Alarm]  â† Query Embeddings

Document: "MedikamentenkÃ¼hlschrank mit Alarm-System"
  â†“ BERT
[emb_Medikamenten, emb_kÃ¼hlschrank, emb_Alarm, emb_System]  â† Doc Embeddings

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Late Interaction: MaxSim

FÃ¼r jedes Query-Token â†’ finde bestes Doc-Token:

  "KÃ¼hlschrank" matched am besten mit "kÃ¼hlschrank" â†’ 0.95
  "Alarm"       matched am besten mit "Alarm"       â†’ 0.98

Score = 0.95 + 0.98 = 1.93
```

**MaxSim Formel:**

$$\text{Score}(q, d) = \sum_{i=1}^{|q|} \max_{j=1}^{|d|} \text{sim}(q_i, d_j)$$

**Intuition:**
- Jedes Query-Wort sucht bestes Match im Document
- Summe aller besten Matches = Gesamt-Score
- **PrÃ¤ziser** als Single-Vector (keine Info geht verloren)

**Warum dieser Ansatz?**
- **Cross-Encoder:** $O(n)$ - muss jedes Doc neu encoden
- **Single-Vector:** $O(1)$ - aber Informationsverlust
- **ColBERT:** $O(k)$ - k = avg tokens, aber behÃ¤lt PrÃ¤zision!

#### Code-Beispiel

```python
from colbert import Indexer, Searcher
from colbert.infra import Run, RunConfig

# Setup
with Run().context(RunConfig(nranks=1)):
    # Index erstellen
    indexer = Indexer(checkpoint='colbert-ir/colbertv2.0')
    indexer.index(
        name='my_index',
        collection=['Doc 1 text', 'Doc 2 text', ...],
        overwrite=True
    )

    # Suche
    searcher = Searcher(index='my_index', checkpoint='colbert-ir/colbertv2.0')
    results = searcher.search("LaborkÃ¼hlschrank mit Alarm", k=10)

    for doc_id, rank, score in results:
        print(f"#{rank} Doc {doc_id}: {score:.2f}")
```

#### Varianten & Trade-offs

| Aspekt | Single-Vector | Multi-Vector (ColBERT) |
|--------|---------------|------------------------|
| **Speicher** | 768 floats/doc | 768 Ã— avg_tokens floats/doc (~10x mehr) |
| **Geschwindigkeit** | âš¡âš¡âš¡ | âš¡âš¡ |
| **Genauigkeit** | â­â­â­ | â­â­â­â­â­ |
| **Use-Case** | Standard RAG | High-Precision Retrieval |

---

### 4. Cross-Encoders (Re-Ranking)

#### Warum Cross-Encoders?

**Bi-Encoder (Dense) Problem:**

```python
# Bi-Encoder: Query und Doc UNABHÃ„NGIG encoded
query_emb = model.encode("KÃ¼hlschrank Alarm")
doc_emb = model.encode("Medikamenten-KÃ¼hlschrank mit Temperatur-Alarm")

# Similarity:
score = cosine_sim(query_emb, doc_emb)  # 0.78

# Problem: Model sieht Query und Doc NIE zusammen!
# â†’ Kann Nuancen nicht verstehen
```

**Cross-Encoder: Query + Doc ZUSAMMEN:**

```python
# Cross-Encoder: Concatenate Query + Doc
input_text = "[CLS] KÃ¼hlschrank Alarm [SEP] Medikamenten-KÃ¼hlschrank Alarm [SEP]"
  â†“ BERT
  â†“ Classification Head
score = 0.94  # HÃ¶her! Model versteht Kontext besser
```

#### Architektur

```
Query: "KÃ¼hlschrank Alarm"
Document: "Medikamenten-KÃ¼hlschrank mit Alarm-System"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [CLS] Query [SEP] Document [SEP]                 â”‚
â”‚  â†“                                                 â”‚
â”‚  [BERT Transformer Ã— 12]                          â”‚
â”‚    Self-Attention Ã¼ber Query UND Doc!             â”‚
â”‚  â†“                                                 â”‚
â”‚  [CLS] Token Embedding                            â”‚
â”‚  â†“                                                 â”‚
â”‚  [Linear Layer + Sigmoid]                         â”‚
â”‚  â†“                                                 â”‚
â”‚  Relevance Score: 0.94                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Warum besser?**
- **Bi-Encoder:** $\text{emb}(q) \cdot \text{emb}(d)$ - unabhÃ¤ngig
- **Cross-Encoder:** $f(q, d)$ - Query und Doc interagieren via Attention!

**Training:**

```python
# Positive Pairs:
("[CLS] Query [SEP] Relevant Doc [SEP]", label=1)

# Negative Pairs:
("[CLS] Query [SEP] Irrelevant Doc [SEP]", label=0)

# Loss: Binary Cross-Entropy
loss = BCE(predicted_score, true_label)
```

#### Re-Ranking Pipeline

**Zwei-Stufen Retrieval:**

```
Stage 1: Bi-Encoder (schnell, 1000 Kandidaten)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query â†’ Bi-Encoder â†’ Vector DB Search
        â†’ Top-1000 Dokumente (100ms)

Stage 2: Cross-Encoder (prÃ¤zise, Top-10)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query + Top-1000 Docs â†’ Cross-Encoder Re-Ranking
        â†’ Top-10 prÃ¤zise geranked (500ms)
```

**Warum nicht nur Cross-Encoder?**
- 1M Docs Ã— Cross-Encoder = **zu langsam** (10+ Sekunden)
- Bi-Encoder â†’ 1000 Kandidaten (schnell)
- Cross-Encoder â†’ 1000 â†’ 10 (prÃ¤zise)

#### Code-Beispiel

```python
from sentence_transformers import SentenceTransformer, CrossEncoder, util

# Stage 1: Bi-Encoder Retrieval
bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')
corpus = ["Doc 1", "Doc 2", ..., "Doc 1000000"]
corpus_embs = bi_encoder.encode(corpus)

query = "LaborkÃ¼hlschrank mit Alarm"
query_emb = bi_encoder.encode(query)

# Top-100 via Cosine Similarity
hits = util.semantic_search(query_emb, corpus_embs, top_k=100)[0]
candidates = [corpus[hit['corpus_id']] for hit in hits]

# Stage 2: Cross-Encoder Re-Ranking
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
pairs = [[query, doc] for doc in candidates]
scores = cross_encoder.predict(pairs)

# Re-Rank nach Cross-Encoder Scores
reranked_hits = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)

print("Top-10 nach Re-Ranking:")
for doc, score in reranked_hits[:10]:
    print(f"{score:.4f}: {doc}")
```

#### Varianten & Trade-offs

| Aspekt | Bi-Encoder | Cross-Encoder |
|--------|------------|---------------|
| **Speed** | âš¡âš¡âš¡ (cached embeddings) | âš¡ (muss jeden Pair encoden) |
| **Genauigkeit** | â­â­â­ | â­â­â­â­â­ |
| **Skalierung** | Millionen Docs âœ“ | Nur fÃ¼r Top-K Re-Ranking |
| **Use-Case** | Retrieval | Re-Ranking |

---

## âš ï¸ HÃ¤ufige MissverstÃ¤ndnisse (Misconception Debugging)

### âŒ MissverstÃ¤ndnis 1: "Dense ist immer besser als Sparse"

**Warum das falsch ist:**

```python
Query: "LABO-288-PRO"  # Modellnummer

# Dense (Semantic):
# Findet: "LaborkÃ¼hlschrank", "Labor-Equipment", ...
# â†’ Zu fuzzy! User will EXAKT "LABO-288-PRO"

# BM25 (Sparse):
# Findet: Dokument mit "LABO-288-PRO"
# â†’ Perfekt!
```

**âœ“ Richtig ist:**
- **Exakte Matches** (IDs, SKUs, Modellnummern) â†’ Sparse
- **Semantische Suche** (Synonyme, Paraphrasen) â†’ Dense
- **Production** â†’ Hybrid (beides!)

**Merksatz:**
"Sparse fÃ¼r Keywords, Dense fÃ¼r Semantik, Hybrid fÃ¼r Production!"

### âŒ MissverstÃ¤ndnis 2: "Cross-Encoder fÃ¼r alles nutzen"

**Warum das falsch ist:**

```python
# 1 Million Dokumente
# Cross-Encoder muss JEDEN Query-Doc-Pair encoden
# = 1.000.000 Forward-Passes
# = 10+ Sekunden pro Query âŒ

# Bi-Encoder:
# Embeddings cached
# Vector Search: 100ms âœ“
```

**âœ“ Richtig ist:**
Cross-Encoder **nur fÃ¼r Re-Ranking** der Top-K Kandidaten!

**Merksatz:**
"Bi-Encoder holt Kandidaten, Cross-Encoder findet den Besten!"

### âŒ MissverstÃ¤ndnis 3: "ColBERT ist immer besser als Single-Vector"

**Warum das falsch ist:**

```python
# ColBERT Speicher:
1M Docs Ã— 50 avg tokens Ã— 128 dims Ã— 4 bytes
= 25 GB

# Single-Vector:
1M Docs Ã— 384 dims Ã— 4 bytes
= 1.5 GB

# 16x mehr Speicher!
```

**âœ“ Richtig ist:**
- **Kleiner Corpus (<100k)** â†’ ColBERT (PrÃ¤zision wichtiger)
- **GroÃŸer Corpus (>1M)** â†’ Single-Vector (Speicher wichtiger)
- **High-Value Queries** (z.B. Legal, Medical) â†’ ColBERT

**Merksatz:**
"ColBERT fÃ¼r QualitÃ¤t, Single-Vector fÃ¼r Skalierung!"

## ğŸ”¬ Hands-On: Alle 4 Architekturen vergleichen

```python
from sentence_transformers import SentenceTransformer, CrossEncoder, util
from rank_bm25 import BM25Okapi
import numpy as np

# Corpus
corpus = [
    "LaborkÃ¼hlschrank mit 280 Liter Volumen und TemperaturÃ¼berwachung",
    "MedikamentenkÃ¼hlschrank nach DIN 13277 mit Alarm-System",
    "Gefrierschrank fÃ¼r Labor mit -40Â°C Temperatur",
    "Pommes Frites Zubereitung in der Fritteuse"
]

query = "KÃ¼hlschrank fÃ¼r Medikamente mit Alarm"

print(f"Query: {query}\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. BM25 (Sparse)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
tokenized_corpus = [doc.lower().split() for doc in corpus]
bm25 = BM25Okapi(tokenized_corpus)
bm25_scores = bm25.get_scores(query.lower().split())

print("1. BM25 (Sparse):")
for idx, score in enumerate(bm25_scores):
    print(f"  {score:.4f}: {corpus[idx][:60]}...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Dense (Bi-Encoder)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')
query_emb = bi_encoder.encode(query, convert_to_tensor=True)
corpus_embs = bi_encoder.encode(corpus, convert_to_tensor=True)
dense_scores = util.cos_sim(query_emb, corpus_embs)[0].cpu().numpy()

print("\n2. Dense (Bi-Encoder):")
for idx, score in enumerate(dense_scores):
    print(f"  {score:.4f}: {corpus[idx][:60]}...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Hybrid (BM25 + Dense)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Normalize scores to [0, 1]
bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)
dense_norm = dense_scores

# Weighted combination
alpha = 0.5  # 50% BM25, 50% Dense
hybrid_scores = alpha * bm25_norm + (1 - alpha) * dense_norm

print("\n3. Hybrid (BM25 + Dense):")
for idx, score in enumerate(hybrid_scores):
    print(f"  {score:.4f}: {corpus[idx][:60]}...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. Cross-Encoder (Re-Ranking Top-3 from Dense)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# Top-3 Kandidaten from Dense
top_3_indices = np.argsort(dense_scores)[-3:][::-1]
top_3_docs = [corpus[idx] for idx in top_3_indices]

# Cross-Encoder Re-Ranking
pairs = [[query, doc] for doc in top_3_docs]
cross_scores = cross_encoder.predict(pairs)

print("\n4. Cross-Encoder (Re-Ranking Top-3):")
for idx, score in zip(top_3_indices, cross_scores):
    print(f"  {score:.4f}: {corpus[idx][:60]}...")
```

**Was du beobachten solltest:**
- **BM25** rankt Doc mit exaktem "Medikamente" + "Alarm" hoch
- **Dense** findet auch "MedikamentenkÃ¼hlschrank" (Synonym!)
- **Hybrid** kombiniert beste von beiden
- **Cross-Encoder** gibt prÃ¤ziseste Scores fÃ¼r Top-Kandidaten

**Experimentiere selbst:**
- Was passiert wenn Query = "LABO-288" (Modellnummer)? Welche Architektur gewinnt?
- Wie Ã¤ndert sich `alpha` im Hybrid? Teste 0.2, 0.5, 0.8
- Was wenn du Cross-Encoder auf ALLE Docs anwendest? (Zeit messen!)

## â±ï¸ 5-Minuten-Experte

### 1. VerstÃ¤ndnisfrage: Warum ist Mean Pooling der Standard?

<details><summary>ğŸ’¡ Zeige Antwort</summary>

**Antwort:**
Mean Pooling ist lÃ¤ngenunabhÃ¤ngig und nutzt alle Token-Informationen.

**ErklÃ¤rung:**
- **CLS Pooling:** Nur ein Token - verschwendet Info
- **Max Pooling:** Nur stÃ¤rkste Signale - kann "spiky" sein
- **Mean Pooling:** Durchschnitt aller Token - robust und fair fÃ¼r kurze/lange Texte

**Merksatz:**
"Mean ist der demokratische Durchschnitt - jedes Token zÃ¤hlt gleich!"

</details>

### 2. Anwendungsfrage: Dein RAG System ist zu langsam - was tun?

<details><summary>ğŸ’¡ Zeige Antwort</summary>

**Antwort:**
Zwei-Stufen Retrieval: Dense Bi-Encoder (Top-100) â†’ Cross-Encoder Re-Ranking (Top-10)

**BegrÃ¼ndung:**
```python
# Vorher: Cross-Encoder auf alles
1M Docs Ã— Cross-Encoder = 10+ Sekunden âŒ

# Nachher: Two-Stage
Stage 1: Bi-Encoder â†’ Top-100 (100ms)
Stage 2: Cross-Encoder â†’ Top-10 (200ms)
Total: 300ms âœ“
```

**Alternative:**
- ColBERT statt Cross-Encoder (etwas schneller, cached embeddings)
- Quantization (int8 statt float32)
- Kleineres Model (MiniLM statt MPNet)

</details>

### 3. Trade-off-Frage: Wann ColBERT statt Single-Vector Dense?

<details><summary>ğŸ’¡ Zeige Antwort</summary>

**Antwort:**
Kommt auf Corpus-GrÃ¶ÃŸe und QualitÃ¤ts-Anforderungen an.

**Kontext matters:**

| Szenario | Wahl | Warum? |
|----------|------|--------|
| <100k Docs, High-Value (Legal, Medical) | ColBERT | PrÃ¤zision wichtiger, Speicher egal |
| >1M Docs, Consumer App | Single-Vector | Speicher & Latenz wichtiger |
| Scientific Papers, Code Search | ColBERT | Token-Level Matching kritisch |
| E-Commerce, News | Single-Vector | Skalierung wichtiger |

**Red Flags fÃ¼r ColBERT:**
- Budget-Cloud (Speicher teuer)
- Mobile/Edge Deployment
- >10M Dokumente (Speicher explodiert)

**Merksatz:**
"ColBERT wenn QualitÃ¤t kritisch, Single-Vector wenn Skalierung kritisch!"

</details>

## ğŸ“Š Vergleiche & Varianten

### Wann nutze ich was?

| Use Case | Empfehlung | Warum? | Trade-off |
|----------|------------|--------|-----------|
| **E-Commerce Suche** | BM25 + Dense Hybrid | Keywords + Semantik | Zwei Indizes pflegen |
| **RAG System (Prototyping)** | Dense (Sentence-BERT) | Schnell zu starten | Exakte Matches fehlen |
| **RAG System (Production)** | Hybrid + Cross-Encoder Re-Ranking | Beste QualitÃ¤t | KomplexitÃ¤t |
| **Legal/Medical Retrieval** | ColBERT | Token-PrÃ¤zision wichtig | Hoher Speicher |
| **Millionen Docs, Latenz <50ms** | BM25 + Dense (keine Re-Ranking) | Speed kritisch | QualitÃ¤tsverlust |

### Decision Tree

```
Brauchst du exakte Keyword-Matches (IDs, Modellnummern)?
â”œâ”€ Ja, nur Keywords
â”‚   â””â”€ BM25 (Sparse)
â”‚
â””â”€ Nein, Semantik wichtig
    â”œâ”€ Corpus klein (<100k)?
    â”‚   â”œâ”€ Ja â†’ High-Quality nÃ¶tig?
    â”‚   â”‚   â”œâ”€ Ja â†’ ColBERT
    â”‚   â”‚   â””â”€ Nein â†’ Dense (Bi-Encoder)
    â”‚   â”‚
    â”‚   â””â”€ Nein (>1M Docs)
    â”‚       â”œâ”€ Latenz <50ms?
    â”‚       â”‚   â”œâ”€ Ja â†’ Dense only (cached)
    â”‚       â”‚   â””â”€ Nein â†’ Dense + Cross-Encoder
    â”‚       â”‚
    â”‚       â””â”€ Budget fÃ¼r Hybrid?
    â”‚           â”œâ”€ Ja â†’ BM25 + Dense + Cross-Encoder
    â”‚           â””â”€ Nein â†’ Dense only
```

## ğŸ› ï¸ Tools & Frameworks

### Dense Embeddings

```python
# Sentence-Transformers (Standard)
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(["text1", "text2"])
```

**Models:**
- **all-MiniLM-L6-v2**: 384-dim, schnell, Englisch
- **paraphrase-multilingual-MiniLM-L12-v2**: 384-dim, multilingual
- **multilingual-e5-large**: 1024-dim, State-of-the-art

### Sparse Embeddings

```python
# BM25
from rank_bm25 import BM25Okapi

corpus_tokenized = [doc.split() for doc in corpus]
bm25 = BM25Okapi(corpus_tokenized)
scores = bm25.get_scores(query.split())
```

```python
# SPLADE
from transformers import AutoModelForMaskedLM, AutoTokenizer

model = AutoModelForMaskedLM.from_pretrained('naver/splade-cocondenser-ensembledistil')
tokenizer = AutoTokenizer.from_pretrained('naver/splade-cocondenser-ensembledistil')
```

### Multi-Vector (ColBERT)

```python
# ColBERT
from colbert import Indexer, Searcher

indexer = Indexer(checkpoint='colbert-ir/colbertv2.0')
indexer.index(name='my_index', collection=corpus)

searcher = Searcher(index='my_index')
results = searcher.search(query, k=10)
```

### Cross-Encoders

```python
# Cross-Encoder
from sentence_transformers import CrossEncoder

cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
pairs = [[query, doc] for doc in candidates]
scores = cross_encoder.predict(pairs)
```

**HÃ¤ufige Stolpersteine:**

1. **Problem:** Cross-Encoder zu langsam
   ```python
   # Ursache: Auf zu viele Docs angewendet
   # LÃ¶sung: Nur Top-K Re-Ranken (K=50-100)
   ```

2. **Problem:** BM25 findet nichts bei Typos
   ```python
   # Ursache: Exakte String-Matches
   # LÃ¶sung: Fuzzy Matching oder Hybrid mit Dense
   ```

3. **Problem:** Dense rankt irrelevante Docs hoch
   ```python
   # Ursache: Zu "fuzzy" Matches
   # LÃ¶sung: Hybrid mit BM25 oder Cross-Encoder Re-Ranking
   ```

## ğŸš€ Was du jetzt kannst

**VerstÃ¤ndnis:**
- âœ“ Du verstehst die 4 Haupt-Architekturen und ihre mathematischen Grundlagen
- âœ“ Du erkennst Trade-offs zwischen Speed, QualitÃ¤t, Speicher
- âœ“ Du verstehst warum Hybrid-AnsÃ¤tze in Production Standard sind

**Praktische FÃ¤higkeiten:**
- âœ“ Du implementierst alle 4 Architekturen praktisch
- âœ“ Du baust Two-Stage Retrieval (Bi-Encoder â†’ Cross-Encoder)
- âœ“ Du kombinierst BM25 + Dense fÃ¼r Hybrid-Search

**Kritisches Denken:**
- âœ“ Du wÃ¤hlst Architektur basierend auf Requirements (Latenz, Corpus-GrÃ¶ÃŸe, QualitÃ¤t)
- âœ“ Du erkennst wann Single-Vector reicht vs. wann ColBERT/Cross-Encoder nÃ¶tig
- âœ“ Du debuggst Retrieval-Probleme (zu fuzzy? zu strict?)

**NÃ¤chste Schritte:**
- [ ] Baue Hybrid-System mit allen 4 Architekturen
- [ ] Benchmarke auf deinem eigenen Corpus
- [ ] Tune Hybrid-Gewichte (alpha-Parameter)

## ğŸ”— WeiterfÃ¼hrende Themen

**NÃ¤chster logischer Schritt:**
â†’ [03-model-selection.md](03-model-selection.md) - **Embedding Spaces & Model Selection** (Kritisch: Warum Models verschiedene "Sprachen" sprechen!)

**Von Theorie zu Production:**
â†’ [04-vector-databases.md](04-vector-databases.md) - Vector DBs, Quantization, Deployment Patterns
â†’ [../../04-advanced/02-retrieval-optimization.md](../../04-advanced/02-retrieval-optimization.md) - Chunking Strategies, Two-Stage Retrieval, Hybrid Search

**Praktische Anwendung:**
â†’ [../../06-applications/rag-systems.md](../../06-applications/rag-systems.md) - VollstÃ¤ndiges RAG-System mit allem aus diesem Kapitel

**Verwandte Konzepte:**
- [../training/contrastive-learning.md](../training/contrastive-learning.md) - Wie Dense Models mit Contrastive Learning trainiert werden
- [../evaluation/metrics.md](../evaluation/metrics.md) - Recall@k, MRR, nDCG fÃ¼r Retrieval-QualitÃ¤t
