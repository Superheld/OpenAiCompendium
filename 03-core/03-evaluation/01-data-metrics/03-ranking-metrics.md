# Ranking Metrics: Wie gut findet dein Retrieval System relevante Dokumente?

> **ğŸ’¡ Wichtig:** Diese Metriken sind **unabhÃ¤ngig** von der Retrieval-Methode!
>
> Sie funktionieren gleich, egal ob du nutzt:
> - Dense Retrieval (Embedding-basiert: Sentence-Transformers, OpenAI)
> - Sparse Retrieval (BM25, TF-IDF)
> - Hybrid Retrieval (Kombination)
>
> **Ranking-Metriken** messen nur: "Sind die Top-K Ergebnisse relevant?" - unabhÃ¤ngig davon WIE du sie gefunden hast.
>
> â†’ FÃ¼r Retrieval-Methoden siehe: [04-advanced/retrieval-methods/](../../../04-advanced/02-retrieval-optimization.md)
> â†’ FÃ¼r Embedding-Models siehe: [03-core/embeddings/](../../02-embeddings/)

## â“ Das Problem (Problem-First)

**Ohne gute Ranking-Metriken geht folgendes schief:**

1. **Du merkst nicht, dass relevante Docs auf Platz 20 landen** - User sehen nur Top-5, aber wichtigste Info ist weiter unten
   - *Beispiel:* User fragt "Wie kÃ¼ndige ich meinen Vertrag?" â†’ KÃ¼ndigungsformular ist auf Position 15, User findet es nicht

2. **Du kannst Systeme nicht vergleichen** - Ist Retrieval-System A besser als System B? Ohne Metriken: BauchgefÃ¼hl
   - *Beispiel:* Zwei verschiedene AnsÃ¤tze testen, aber keine quantitative Basis fÃ¼r Entscheidung

3. **Du optimierst am falschen Ende** - System liefert 100 Dokumente, aber nur Top-3 sind relevant â†’ Hoher Recall tÃ¤uscht Ã¼ber schlechte User Experience hinweg
   - *Beispiel:* 80% Recall klingt gut, aber Precision@5 ist nur 20% â†’ User sieht 4 von 5 irrelevanten Docs

**Die zentrale Frage:**
Von allen zurÃ¼ckgegebenen Dokumenten - wie viele der *ersten K* Dokumente sind relevant? Und wo steht das erste relevante Dokument?

**Beispiel-Szenario:**
```
Query: "Python async programming tutorial"
Deine Datenbank: 1000 Dokumente (davon 20 relevant zu async Python)

System liefert Top-10:
1. âœ… "Asyncio Tutorial"
2. âŒ "Java Threading"
3. âŒ "JavaScript Promises"
4. âœ… "Python Async/Await Guide"
5. âŒ "C++ Multithreading"
6. âœ… "Python Asyncio Best Practices"
7-10. âŒ Alle irrelevant

â†’ Wie bewertest du diese Retrieval-Performance quantitativ?
```

## ğŸ¯ Lernziele

Nach diesem Kapitel kannst du:
- [ ] Verstehe Precision@K, Recall@K, MRR, NDCG **mathematisch und intuitiv**
- [ ] Erkenne wann welche Metrik sinnvoll ist (Binary vs. Graded Relevance)
- [ ] Implementiere alle Standard-Metriken from scratch in Python
- [ ] Interpretiere Metrik-Werte richtig (was ist ein "guter" MRR@10?)
- [ ] WÃ¤hle die richtige Metrik fÃ¼r deinen Use-Case (E-Commerce vs. FAQ-Bot)
- [ ] Erkenne hÃ¤ufige Fehlinterpretationen (hoher Recall â‰  gutes System)

## ğŸ§  Intuition zuerst

### Alltagsanalogie: Die Bibliothek

**Stell dir vor, du suchst in einer Bibliothek:**

Du fragst den Bibliothekar nach BÃ¼chern Ã¼ber "Machine Learning".
Er bringt dir 10 BÃ¼cher.

**Precision@10**: Von diesen 10 BÃ¼chern, wie viele sind wirklich Ã¼ber ML?
- Er bringt 7 ML-BÃ¼cher + 3 Ã¼ber Statistik â†’ Precision@10 = 7/10 = 70%
- *Intuitiv:* "Wie viel MÃ¼ll ist dabei?"

**Recall@10**: Die Bibliothek hat 50 ML-BÃ¼cher. Von denen hat er dir 7 gebracht.
- Recall@10 = 7/50 = 14%
- *Intuitiv:* "Wie viel habe ich verpasst?"

**Mean Reciprocal Rank (MRR)**: Wo lag das erste relevante Buch?
- Erstes ML-Buch war auf Position 3 â†’ Reciprocal Rank = 1/3 = 0.33
- *Intuitiv:* "Wie lange musste ich suchen?"

**NDCG**: Manche BÃ¼cher sind "perfekt", andere "okay", andere "irrelevant"
- Perfekte Sortierung: Beste BÃ¼cher zuerst â†’ NDCG = 1.0
- Deine Sortierung: Manche gute BÃ¼cher erst spÃ¤ter â†’ NDCG = 0.75
- *Intuitiv:* "Wie gut ist die Reihenfolge?"

### Visualisierung: Retrieval Results

```
Query: "Python async"
System liefert 10 Docs, du hast Ground Truth (welche sind relevant):

Position â”‚ Relevanz â”‚ Precision@K â”‚ Recall@K (von 5 relevanten)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1     â”‚    âœ…    â”‚   1/1=100%  â”‚   1/5=20%
   2     â”‚    âŒ    â”‚   1/2=50%   â”‚   1/5=20%
   3     â”‚    âŒ    â”‚   1/3=33%   â”‚   1/5=20%
   4     â”‚    âœ…    â”‚   2/4=50%   â”‚   2/5=40%
   5     â”‚    âŒ    â”‚   2/5=40%   â”‚   2/5=40%  â† Precision@5
   6     â”‚    âœ…    â”‚   3/6=50%   â”‚   3/5=60%
   7     â”‚    âŒ    â”‚   3/7=43%   â”‚   3/5=60%
   8     â”‚    âŒ    â”‚   3/8=38%   â”‚   3/5=60%
   9     â”‚    âŒ    â”‚   3/9=33%   â”‚   3/5=60%
  10     â”‚    âŒ    â”‚   3/10=30%  â”‚   3/5=60%  â† Precision@10
```

**Beobachtungen:**
- Precision sinkt mit mehr Docs (mehr MÃ¼ll kommt dazu)
- Recall steigt mit mehr Docs (mehr relevante gefunden)
- Trade-off: Mehr Docs â†’ Mehr Recall, aber weniger Precision

### Die BrÃ¼cke zur Mathematik

Jetzt machen wir das prÃ¤zise mit Formeln - aber die Intuition bleibt gleich:
- **Precision** = "Wie viel von dem was ich bekomme ist gut?"
- **Recall** = "Wie viel von dem was gut ist bekomme ich?"
- **Ranking-QualitÃ¤t** = "Sind die guten Sachen weit oben?"

## ğŸ§® Das Konzept verstehen

### 1. Precision@K und Recall@K

**Mathematische Definition:**

$$
\text{Precision@K} = \frac{|\text{relevant\_docs} \cap \text{retrieved\_docs@K}|}{K}
$$

$$
\text{Recall@K} = \frac{|\text{relevant\_docs} \cap \text{retrieved\_docs@K}|}{|\text{relevant\_docs}|}
$$

**Intuition hinter den Formeln:**

- **Numerator (ZÃ¤hler)** ist bei beiden gleich: Anzahl relevanter Docs in Top-K
- **Denominator (Nenner)** unterscheidet:
  - Precision: Geteilt durch K (alle zurÃ¼ckgegebenen)
  - Recall: Geteilt durch Gesamtzahl relevanter Docs

**Warum diese Definitionen?**

- **Precision@K**: User-Perspektive â†’ "Von dem was ich sehe (K Docs), wie viel ist brauchbar?"
  - Wichtig wenn User nur Top-K sieht (z.B. Top-5 in Web-Search)

- **Recall@K**: System-Perspektive â†’ "Von allem was relevant ist, wie viel habe ich gefunden?"
  - Wichtig bei E-Discovery (Legal) wo du ALLE relevanten Docs finden musst

**Beispiel-Rechnung:**

```
Query: "Python async"
Ground Truth: 5 relevante Docs in der Datenbank: {Doc_A, Doc_B, Doc_C, Doc_D, Doc_E}

System retrieves Top-10: [Doc_A, Doc_X, Doc_Y, Doc_B, Doc_Z, Doc_C, ...]

Relevant in Top-5: {Doc_A, Doc_B} â†’ 2 relevante
Relevant in Top-10: {Doc_A, Doc_B, Doc_C} â†’ 3 relevante

Precision@5  = 2/5 = 0.40 (40%)
Recall@5     = 2/5 = 0.40 (40% aller relevanten gefunden)

Precision@10 = 3/10 = 0.30 (30%)
Recall@10    = 3/5 = 0.60 (60% aller relevanten gefunden)
```

**Trade-off sichtbar:**
- Mehr Docs (Kâ†‘) â†’ Precisionâ†“, Recallâ†‘
- Weniger Docs (Kâ†“) â†’ Precisionâ†‘, Recallâ†“

### 2. Mean Reciprocal Rank (MRR)

**Mathematische Definition:**

$$
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
$$

Wobei $\text{rank}_i$ die Position des **ersten** relevanten Dokuments fÃ¼r Query $i$ ist.

**Intuition hinter der Formel:**

- **Reciprocal Rank (1/rank)**: Je frÃ¼her das erste relevante Doc, desto hÃ¶her der Score
  - Position 1: 1/1 = 1.0
  - Position 2: 1/2 = 0.5
  - Position 3: 1/3 = 0.33
  - Position 10: 1/10 = 0.1

- **Mean Ã¼ber Queries**: Durchschnitt Ã¼ber alle Test-Queries

**Warum dieser Ansatz?**

MRR fokussiert auf "**Time to first relevant result**" - wichtig fÃ¼r:
- Web-Search (User klickt erstes gutes Ergebnis)
- FAQ-Bots (User braucht EINE gute Antwort)
- Navigational Queries ("Facebook Login" â†’ User will eine spezifische Seite)

**Beispiel-Rechnung:**

```
Test-Set: 3 Queries

Query 1: "Python async"
Retrieved: [âŒ, âŒ, âœ…, âŒ, âŒ, ...]
First relevant: Position 3 â†’ RRâ‚ = 1/3 = 0.33

Query 2: "Docker networking"
Retrieved: [âœ…, âŒ, âœ…, ...]
First relevant: Position 1 â†’ RRâ‚‚ = 1/1 = 1.0

Query 3: "Redis caching"
Retrieved: [âŒ, âœ…, âŒ, ...]
First relevant: Position 2 â†’ RRâ‚ƒ = 1/2 = 0.5

MRR = (0.33 + 1.0 + 0.5) / 3 = 1.83 / 3 = 0.61
```

**Interpretation:**
- MRR = 1.0: Perfekt! Immer erstes Ergebnis relevant
- MRR = 0.5: Im Schnitt ist zweites Ergebnis relevant
- MRR = 0.1: Im Schnitt Position 10 (schlecht!)

### 3. Normalized Discounted Cumulative Gain (NDCG@K)

**Problem mit Precision/Recall:** Binary Relevanz (relevant âœ… vs. irrelevant âŒ)
**RealitÃ¤t:** Graded Relevanz (perfekt=3, gut=2, okay=1, irrelevant=0)

**Schritt-fÃ¼r-Schritt Ableitung:**

**Step 1: Cumulative Gain (CG)**

$$
\text{CG@K} = \sum_{i=1}^{K} \text{rel}_i
$$

Einfach die Summe aller Relevanz-Scores.

**Problem:** Position egal! [3,2,1] hat gleichen CG wie [1,2,3]

**Step 2: Discounted Cumulative Gain (DCG)**

$$
\text{DCG@K} = \sum_{i=1}^{K} \frac{\text{rel}_i}{\log_2(i+1)}
$$

**Discount-Faktor:** $\frac{1}{\log_2(i+1)}$ bestraft spÃ¤te Positionen

| Position | Discount | Interpretation |
|----------|----------|----------------|
| 1 | 1/logâ‚‚(2) = 1.0 | Volle Wertung |
| 2 | 1/logâ‚‚(3) = 0.63 | 63% Wert |
| 3 | 1/logâ‚‚(4) = 0.5 | 50% Wert |
| 5 | 1/logâ‚‚(6) = 0.39 | 39% Wert |
| 10 | 1/logâ‚‚(11) = 0.29 | 29% Wert |

**Step 3: Normalized DCG (NDCG)**

$$
\text{NDCG@K} = \frac{\text{DCG@K}}{\text{IDCG@K}}
$$

Wobei **IDCG** (Ideal DCG) = DCG der *perfekten* Sortierung (beste Docs zuerst)

**Warum Normalisierung?**
- ErmÃ¶glicht Vergleich zwischen Queries (verschiedene Anzahl relevanter Docs)
- Range: [0, 1], wobei 1.0 = perfekte Sortierung

**Beispiel-Rechnung:**

```
Query: "Python async tutorial"

Ground Truth Relevanz-Scores:
- Doc_A: 3 (perfekt)
- Doc_B: 2 (gut)
- Doc_C: 2 (gut)
- Doc_D: 1 (okay)
- Rest: 0 (irrelevant)

System Retrieval (Top-5):
[Doc_B(2), Doc_X(0), Doc_A(3), Doc_C(2), Doc_D(1)]

DCG@5:
= 2/logâ‚‚(2) + 0/logâ‚‚(3) + 3/logâ‚‚(4) + 2/logâ‚‚(5) + 1/logâ‚‚(6)
= 2/1 + 0/1.58 + 3/2 + 2/2.32 + 1/2.58
= 2.0 + 0 + 1.5 + 0.86 + 0.39
= 4.75

Ideal Retrieval (beste Reihenfolge):
[Doc_A(3), Doc_B(2), Doc_C(2), Doc_D(1), Doc_X(0)]

IDCG@5:
= 3/logâ‚‚(2) + 2/logâ‚‚(3) + 2/logâ‚‚(4) + 1/logâ‚‚(5) + 0/logâ‚‚(6)
= 3/1 + 2/1.58 + 2/2 + 1/2.32 + 0
= 3.0 + 1.27 + 1.0 + 0.43 + 0
= 5.70

NDCG@5 = DCG/IDCG = 4.75/5.70 = 0.833
```

**Interpretation:**
- NDCG@5 = 0.833 â†’ 83% der idealen Sortierung erreicht
- Gut, aber nicht perfekt (Doc_A sollte auf Position 1 sein, nicht 3)

## âš ï¸ HÃ¤ufige MissverstÃ¤ndnisse (Misconception Debugging)

### âŒ MissverstÃ¤ndnis 1: "Hoher Recall bedeutet gutes System"

**Warum das falsch ist:**
```python
# "Dummes" System: Gib ALLE Dokumente zurÃ¼ck
retrieved = all_docs  # 10,000 Dokumente
relevant = 50  # Davon 50 relevant

Recall = 50/50 = 100%  # Perfekt!
Precision = 50/10,000 = 0.5%  # Katastrophal!
```

User bekommt 10,000 Docs und muss selbst suchen â†’ Nutzlos trotz 100% Recall

**âœ“ Richtig ist:**
- Recall alleine sagt nichts Ã¼ber User Experience
- Wichtig: **Precision@K** fÃ¼r K das der User sieht (meist K=5 oder K=10)

**Merksatz:** "Recall ohne Precision ist wie eine Bibliothek die dir alle BÃ¼cher bringt"

### âŒ MissverstÃ¤ndnis 2: "MRR ist besser als NDCG"

**Warum das falsch ist:** Kommt auf den Use-Case an!

**MRR ist besser fÃ¼r:**
- Navigational Queries ("Facebook login" â†’ User will EINE Seite)
- FAQ-Bots (User braucht EINE Antwort)
- "Quick Answer" Szenarios

**NDCG ist besser fÃ¼r:**
- Informational Queries ("Machine Learning tutorials" â†’ User will mehrere gute Ergebnisse)
- E-Commerce Search (User will Auswahl von Produkten sehen)
- Research (mehrere relevante Papers)

**âœ“ Richtig ist:**
- MRR: "First relevant result" Use-Cases
- NDCG: "Multiple relevant results with varying quality" Use-Cases
- Oft: **Beide** messen und interpretieren!

**Beispiel:**
```python
# Query: "Best Python IDE"
# User will mehrere Optionen vergleichen â†’ NDCG besser

# Query: "Python official website"
# User will genau eine URL â†’ MRR besser
```

### âŒ MissverstÃ¤ndnis 3: "Precision@5 = 80% ist gut genug"

**Warum das falsch ist:** Context matters!

**FAQ-Bot:**
- Precision@5 = 80% â†’ User sieht 4 richtige + 1 falsche Antwort
- **Risiko:** Falsche Antwort kÃ¶nnte gewÃ¤hlt werden â†’ Kunde verÃ¤rgert
- **Target:** Precision@5 > 95%

**E-Commerce Search:**
- Precision@5 = 80% â†’ 4 von 5 Produkten passen
- **Akzeptabel:** User kann falsche Produkte ignorieren
- **Target:** Precision@5 > 70% (niedrigere Anforderung)

**Medical/Legal:**
- Precision@5 = 80% â†’ NICHT akzeptabel
- **Risiko:** Falsche Information kÃ¶nnte schwerwiegende Folgen haben
- **Target:** Precision@5 > 98%

**âœ“ Richtig ist:**
- Kein universeller "guter Wert"
- Definiere Targets basierend auf:
  - Konsequenz von False Positives
  - User Tolerance fÃ¼r irrelevante Ergebnisse
  - Business Requirements

**Merksatz:** "Dein Precision-Target hÃ¤ngt davon ab, wie teuer ein Fehler ist"

## ğŸ”¬ Hands-On: Retrieval Metrics implementieren

### Setup & Beispiel-Daten

```python
from typing import List, Set, Dict
import numpy as np

# Beispiel Ground Truth: Welche Docs sind fÃ¼r welche Query relevant?
ground_truth = {
    "python async": {1, 4, 6, 12, 15},  # Doc IDs
    "docker networking": {2, 7, 8},
    "redis caching": {3, 9, 11, 14}
}

# Beispiel Retrieval Results (Doc IDs in Ranking-Reihenfolge)
retrieval_results = {
    "python async": [1, 23, 45, 4, 67, 6, 89, 12],  # Top-8
    "docker networking": [2, 7, 34, 56, 8],  # Top-5
    "redis caching": [78, 3, 9, 45, 11]  # Top-5
}
```

### 1. Precision@K Implementation

```python
def precision_at_k(retrieved: List[int], relevant: Set[int], k: int) -> float:
    """
    Berechnet Precision@K

    Args:
        retrieved: Liste von Doc IDs in Ranking-Reihenfolge
        relevant: Set von relevanten Doc IDs (Ground Truth)
        k: Anzahl Top-Docs zu evaluieren

    Returns:
        Precision@K (0.0 bis 1.0)
    """
    # Nur Top-K betrachten
    retrieved_at_k = retrieved[:k]

    # Wie viele der Top-K sind relevant?
    relevant_retrieved = set(retrieved_at_k) & relevant  # Intersection

    # Precision = relevante / alle zurÃ¼ckgegebenen
    return len(relevant_retrieved) / k if k > 0 else 0.0


# Test
query = "python async"
retrieved = retrieval_results[query]
relevant = ground_truth[query]

print(f"Precision@5: {precision_at_k(retrieved, relevant, 5):.2%}")
print(f"Precision@10: {precision_at_k(retrieved, relevant, 10):.2%}")
```

**Erwartete Ausgabe:**
```
Precision@5: 60.00%  # 3 von 5 relevant (IDs: 1, 4, 6)
Precision@10: 40.00%  # 4 von 10 relevant (wird schlechter mit mehr K)
```

### 2. Recall@K Implementation

```python
def recall_at_k(retrieved: List[int], relevant: Set[int], k: int) -> float:
    """
    Berechnet Recall@K

    Args:
        retrieved: Liste von Doc IDs in Ranking-Reihenfolge
        relevant: Set von relevanten Doc IDs (Ground Truth)
        k: Anzahl Top-Docs zu evaluieren

    Returns:
        Recall@K (0.0 bis 1.0)
    """
    if len(relevant) == 0:
        return 0.0

    # Nur Top-K betrachten
    retrieved_at_k = retrieved[:k]

    # Wie viele der relevanten Docs wurden gefunden?
    relevant_retrieved = set(retrieved_at_k) & relevant

    # Recall = gefunden / alle relevanten
    return len(relevant_retrieved) / len(relevant)


# Test
print(f"Recall@5: {recall_at_k(retrieved, relevant, 5):.2%}")
print(f"Recall@10: {recall_at_k(retrieved, relevant, 10):.2%}")
print(f"Ground Truth: {len(relevant)} relevante Docs total")
```

**Erwartete Ausgabe:**
```
Recall@5: 60.00%   # 3 von 5 relevanten gefunden
Recall@10: 80.00%  # 4 von 5 relevanten gefunden (steigt mit K)
Ground Truth: 5 relevante Docs total
```

### 3. Mean Reciprocal Rank (MRR) Implementation

```python
def reciprocal_rank(retrieved: List[int], relevant: Set[int]) -> float:
    """
    Berechnet Reciprocal Rank fÃ¼r eine Query
    (Position des ersten relevanten Dokuments)

    Args:
        retrieved: Liste von Doc IDs in Ranking-Reihenfolge
        relevant: Set von relevanten Doc IDs

    Returns:
        Reciprocal Rank (0.0 bis 1.0), oder 0.0 wenn kein relevantes Doc gefunden
    """
    for rank, doc_id in enumerate(retrieved, start=1):
        if doc_id in relevant:
            return 1.0 / rank
    return 0.0  # Kein relevantes Doc gefunden


def mean_reciprocal_rank(
    results: Dict[str, List[int]],
    ground_truth: Dict[str, Set[int]]
) -> float:
    """
    Berechnet MRR Ã¼ber mehrere Queries

    Args:
        results: Dict[query -> Liste von retrieved Doc IDs]
        ground_truth: Dict[query -> Set von relevanten Doc IDs]

    Returns:
        Mean Reciprocal Rank (0.0 bis 1.0)
    """
    rr_scores = []

    for query in results:
        if query not in ground_truth:
            continue

        rr = reciprocal_rank(results[query], ground_truth[query])
        rr_scores.append(rr)

        # Debug: Position des ersten relevanten Docs
        first_pos = int(1/rr) if rr > 0 else "N/A"
        print(f"Query '{query}': First relevant at position {first_pos} (RR={rr:.3f})")

    return np.mean(rr_scores) if rr_scores else 0.0


# Test Ã¼ber alle Queries
mrr = mean_reciprocal_rank(retrieval_results, ground_truth)
print(f"\nMean Reciprocal Rank: {mrr:.3f}")
```

**Erwartete Ausgabe:**
```
Query 'python async': First relevant at position 1 (RR=1.000)
Query 'docker networking': First relevant at position 1 (RR=1.000)
Query 'redis caching': First relevant at position 2 (RR=0.500)

Mean Reciprocal Rank: 0.833
```

**Interpretation:** Im Schnitt ist das erste relevante Doc auf Position ~1.2 (sehr gut!)

### 4. NDCG@K Implementation

```python
def dcg_at_k(relevances: List[float], k: int) -> float:
    """
    Berechnet Discounted Cumulative Gain@K

    Args:
        relevances: Liste von Relevanz-Scores in Ranking-Reihenfolge
        k: Anzahl Top-Docs zu evaluieren

    Returns:
        DCG@K
    """
    relevances_at_k = relevances[:k]

    dcg = 0.0
    for i, rel in enumerate(relevances_at_k, start=1):
        # Discount-Faktor: 1/logâ‚‚(i+1)
        discount = 1.0 / np.log2(i + 1)
        dcg += rel * discount

    return dcg


def ndcg_at_k(
    retrieved: List[int],
    relevance_scores: Dict[int, float],
    k: int
) -> float:
    """
    Berechnet Normalized Discounted Cumulative Gain@K

    Args:
        retrieved: Liste von Doc IDs in Ranking-Reihenfolge
        relevance_scores: Dict[doc_id -> Relevanz-Score]
        k: Anzahl Top-Docs zu evaluieren

    Returns:
        NDCG@K (0.0 bis 1.0)
    """
    # Relevanz-Scores fÃ¼r retrieved Docs (0 wenn nicht relevant)
    retrieved_relevances = [
        relevance_scores.get(doc_id, 0.0)
        for doc_id in retrieved
    ]

    # DCG des aktuellen Rankings
    actual_dcg = dcg_at_k(retrieved_relevances, k)

    # Ideal DCG: Sortiere Relevanz-Scores absteigend
    ideal_relevances = sorted(relevance_scores.values(), reverse=True)
    ideal_dcg = dcg_at_k(ideal_relevances, k)

    # Normalisierung (verhindert Division durch 0)
    return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0.0


# Test mit graded relevance
query = "python async"
retrieved = retrieval_results[query]

# Graded Relevanz (3=perfekt, 2=gut, 1=okay, 0=irrelevant)
relevance_scores = {
    1: 3,   # Perfektes Match
    4: 2,   # Gutes Match
    6: 2,   # Gutes Match
    12: 1,  # Okay Match
    15: 1   # Okay Match
}

ndcg_5 = ndcg_at_k(retrieved, relevance_scores, 5)
ndcg_10 = ndcg_at_k(retrieved, relevance_scores, 10)

print(f"NDCG@5: {ndcg_5:.3f}")
print(f"NDCG@10: {ndcg_10:.3f}")

# Zeige Ranking vs. Ideal
print("\nAktual Ranking (Top-5):")
for i, doc_id in enumerate(retrieved[:5], 1):
    rel = relevance_scores.get(doc_id, 0)
    print(f"  {i}. Doc {doc_id}: Relevance={rel}")

print("\nIdeal Ranking:")
ideal_ranking = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)
for i, (doc_id, rel) in enumerate(ideal_ranking[:5], 1):
    print(f"  {i}. Doc {doc_id}: Relevance={rel}")
```

**Erwartete Ausgabe:**
```
NDCG@5: 0.874
NDCG@10: 0.812

Aktual Ranking (Top-5):
  1. Doc 1: Relevance=3  âœ“ Gut!
  2. Doc 23: Relevance=0  âœ— Schlecht
  3. Doc 45: Relevance=0  âœ— Schlecht
  4. Doc 4: Relevance=2  âœ“ Okay
  5. Doc 67: Relevance=0  âœ— Schlecht

Ideal Ranking:
  1. Doc 1: Relevance=3
  2. Doc 4: Relevance=2
  3. Doc 6: Relevance=2
  4. Doc 12: Relevance=1
  5. Doc 15: Relevance=1
```

**Interpretation:**
- NDCG@5 = 0.874 â†’ 87% der idealen Sortierung
- Gut, weil Doc 1 (beste) auf Position 1
- KÃ¶nnte besser sein: Doc 4, 6 sollten auf Position 2-3 sein (nicht 23, 45)

## â±ï¸ 5-Minuten-Experte

### 1. VerstÃ¤ndnisfrage: Precision vs. Recall Trade-off

**Frage:** Du hast Precision@10=30% und Recall@10=90%. Was bedeutet das? Ist das gut oder schlecht?

<details><summary>ğŸ’¡ Zeige Antwort</summary>

**Antwort:**
- **Precision@10=30%**: Von 10 zurÃ¼ckgegebenen Docs sind nur 3 relevant (viel MÃ¼ll)
- **Recall@10=90%**: Von allen relevanten Docs hast du 90% gefunden (hohe Abdeckung)

**Interpretation:** Dein System ist "zu groÃŸzÃ¼gig"
- Findet fast alles Relevante (gut!)
- Aber gibt auch viel Irrelevantes zurÃ¼ck (schlecht fÃ¼r User Experience)

**Ist das gut oder schlecht?**
- **Schlecht fÃ¼r:** User-facing Search (User sieht 7 irrelevante von 10 Docs)
- **Okay fÃ¼r:** First-Stage Retrieval in Two-Stage System (Re-Ranker filtert spÃ¤ter)
- **Gut fÃ¼r:** Legal Discovery (lieber zu viel als zu wenig finden)

**Verbesserung:**
- Threshold erhÃ¶hen (weniger Docs zurÃ¼ckgeben)
- Trade-off: Precisionâ†‘, Recallâ†“
</details>

### 2. Anwendungsfrage: Welche Metrik fÃ¼r welchen Use-Case?

**Frage:** Du baust drei Systeme: (A) FAQ-Bot, (B) E-Commerce Search, (C) Academic Paper Search. Welche Metrik ist jeweils am wichtigsten?

<details><summary>ğŸ’¡ Zeige Antwort</summary>

**A) FAQ-Bot:**
- **Wichtigste Metrik:** MRR (Mean Reciprocal Rank)
- **Warum:** User braucht EINE gute Antwort schnell
- **Target:** MRR > 0.8 (erste relevante Antwort meist in Top-2)
- **Zweitwichtig:** Precision@1 (ist die Top-Antwort korrekt?)

**B) E-Commerce Search:**
- **Wichtigste Metrik:** NDCG@10
- **Warum:** User will mehrere Produkte vergleichen, beste sollten oben sein
- **Target:** NDCG@10 > 0.7
- **Zweitwichtig:** Recall@10 (genug Auswahl?)

**C) Academic Paper Search:**
- **Wichtigste Metrik:** Recall@50 + Precision@10
- **Warum:** Researcher will ALLE relevanten Papers finden (hoher Recall), aber auch gute Top-Results (Precision@10)
- **Target:** Recall@50 > 0.9, Precision@10 > 0.6
- **Trade-off:** Niedrigere Precision akzeptabel, wenn Recall hoch

**Merksatz:**
- One answer needed â†’ MRR
- Multiple quality-graded results â†’ NDCG
- Comprehensive coverage â†’ Recall + Precision
</details>

### 3. Trade-off-Frage: NDCG vs. Precision@K

**Frage:** Dein System hat NDCG@10=0.85, aber Precision@10=50%. Ein zweites System hat NDCG@10=0.75 und Precision@10=70%. Welches ist besser?

<details><summary>ğŸ’¡ Zeige Antwort</summary>

**Kommt darauf an:**

**System 1 (NDCG=0.85, P@10=50%):**
- Hoher NDCG â†’ Gute Sortierung, beste Docs weit oben
- Niedriger Precision â†’ Viele irrelevante Docs in Top-10
- **Interpretation:** Perfekte Docs auf Position 1-2, dann viel MÃ¼ll
- **Beispiel:** [3, 3, 0, 0, 0, 0, 0, 0, 2, 2] (Relevanz-Scores)

**System 2 (NDCG=0.75, P@10=70%):**
- Niedriger NDCG â†’ Schlechtere Sortierung
- HÃ¶herer Precision â†’ Weniger irrelevante Docs
- **Interpretation:** Viele relevante Docs, aber nicht optimal sortiert
- **Beispiel:** [1, 2, 1, 0, 2, 1, 0, 1, 2, 0] (Relevanz-Scores)

**WÃ¤hle System 1 wenn:**
- User sieht nur Top-3 (dann ist hoher NDCG wichtiger)
- "Best Match" kritisch (z.B. Product Recommendation)

**WÃ¤hle System 2 wenn:**
- User scrollt durch alle Top-10
- Wichtiger dass kein MÃ¼ll dabei ist (z.B. Medical Info)

**Best Practice:** Messe BEIDES und entscheide basierend auf User Behavior
- Tracke: Welche Position klickt User? â†’ Wenn meist Top-1-3: NDCG wichtiger
- Tracke: Scrollt User durch alle? â†’ Wenn ja: Precision wichtiger
</details>

## ğŸ“Š Vergleiche & Benchmarks

### Wann nutze ich was?

| Metrik | Use Case | Vorteil | Nachteil | Typisches Target |
|--------|----------|---------|----------|------------------|
| **Precision@K** | User-facing Search, K=5 oder 10 | Einfach zu verstehen | Ignoriert Ranking-QualitÃ¤t | >70% |
| **Recall@K** | E-Discovery, Medical | Findet alle relevanten Docs | User sieht evtl. viel MÃ¼ll | >90% |
| **MRR** | FAQ-Bot, Navigational Search | Fokus auf erste Antwort | Ignoriert restliche Results | >0.8 |
| **NDCG@K** | E-Commerce, Multi-Result Search | BerÃ¼cksichtigt Ranking + Grades | Komplexer zu berechnen | >0.7 |

### Decision Tree: Metrik-Auswahl

```
Brauchst du graded relevance (0-3) oder binary (ja/nein)?
â”œâ”€ Binary â†’ Precision@K, Recall@K, MRR
â”‚   â””â”€ User sieht nur Top-1 Ergebnis?
â”‚       â”œâ”€ JA â†’ MRR (FAQ-Bot)
â”‚       â””â”€ NEIN â†’ Precision@K + Recall@K
â”‚           â””â”€ Was ist wichtiger?
â”‚               â”œâ”€ User Experience â†’ Precision@K
â”‚               â””â”€ Nichts verpassen â†’ Recall@K
â”‚
â””â”€ Graded â†’ NDCG@K
    â””â”€ K = Anzahl Ergebnisse die User sieht
```

### Benchmark: Typische Werte verschiedener Systeme

| System-Typ | Precision@5 | Recall@10 | MRR | NDCG@10 |
|------------|-------------|-----------|-----|---------|
| **State-of-the-Art (BEIR Benchmark)** | 65-75% | 55-65% | 0.55-0.70 | 0.52-0.62 |
| **Gutes Production RAG-System** | 60-70% | 70-80% | 0.70-0.85 | 0.65-0.75 |
| **Okay Production System** | 45-60% | 60-70% | 0.60-0.70 | 0.55-0.65 |
| **Needs Improvement** | <45% | <60% | <0.60 | <0.55 |

**Quelle:** BEIR Benchmark (2021), eigene Production-Erfahrung

### ğŸ” Metrik-UnabhÃ¤ngigkeit verstehen

**Wichtig:** Diese Metriken messen nur das **Ergebnis** des Retrievals, nicht die **Methode**!

```
Retrieval-Pipeline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Retrieval-Methode (WIE du suchst)                        â”‚
â”‚    â”œâ”€ Dense Retrieval (Embeddings + Cosine Similarity)     â”‚
â”‚    â”œâ”€ Sparse Retrieval (BM25, TF-IDF)                      â”‚
â”‚    â””â”€ Hybrid (Kombination)                                 â”‚
â”‚                                                              â”‚
â”‚    â†’ Siehe: 04-advanced/retrieval-methods/                 â”‚
â”‚    â†’ Siehe: 03-core/embeddings/ (fÃ¼r Dense Retrieval)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Retrieval-Ergebnis                                       â”‚
â”‚    Top-K Dokumente: [Doc_1, Doc_5, Doc_3, Doc_8, Doc_2]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Evaluation (WAS du misst) â† Du bist HIER                â”‚
â”‚    Ranking Metrics (dieses Kapitel):                        â”‚
â”‚    - Precision@K: Wie viele der Top-K sind relevant?       â”‚
â”‚    - MRR: Wo ist das erste relevante Doc?                  â”‚
â”‚    - NDCG: Wie gut ist die Reihenfolge?                    â”‚
â”‚                                                              â”‚
â”‚    â†’ UnabhÃ¤ngig von Schritt 1!                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Beispiel: Gleiche Metriken, verschiedene Methoden**

```python
# Beispiel: Du vergleichst zwei Retrieval-Methoden

# Methode A: Dense Retrieval (Sentence-Transformers)
results_dense = [1, 5, 8, 12, 3]  # Doc IDs
ground_truth = {1, 3, 8, 15, 20}

precision_dense = precision_at_k(results_dense, ground_truth, 5)
# â†’ 60% (3 von 5 relevant)

# Methode B: BM25 (Sparse Retrieval)
results_bm25 = [1, 3, 8, 15, 20]  # Doc IDs
precision_bm25 = precision_at_k(results_bm25, ground_truth, 5)
# â†’ 100% (5 von 5 relevant)

# Gleiche Metrik, verschiedene Methoden!
# Jetzt weiÃŸt du: BM25 funktioniert besser fÃ¼r diesen Use-Case
```

**Was beeinflusst was?**

| Was? | Beeinflusst durch... | Gemessen mit... |
|------|---------------------|-----------------|
| **Embedding-QualitÃ¤t** | Embedding-Model (Sentence-Transformers, OpenAI) | Embedding-Evaluation (03-core/embeddings/) |
| **Similarity-Metrik** | Embedding-Model (normalisiert? â†’ Cosine vs. Dot Product) | Siehe 02-similarity-measures.md |
| **Retrieval-QualitÃ¤t** | Retrieval-Methode (Dense, Sparse, Hybrid) | **Ranking-Metriken (dieses Kapitel)** |

**Merksatz:**
> "Ranking-Metriken sind wie ein Thermometer - sie messen die Temperatur, egal ob du mit Gas, Strom oder Holz heizt."

## ğŸš€ Was du jetzt kannst

**Mathematisches VerstÃ¤ndnis:**
- âœ“ Du verstehst Precision@K, Recall@K, MRR, NDCG von Intuition bis Formalisierung
- âœ“ Du erkennst Trade-offs zwischen Metriken (Precision vs. Recall)
- âœ“ Du siehst warum NDCG komplexer aber aussagekrÃ¤ftiger ist als Precision@K

**Praktische FÃ¤higkeiten:**
- âœ“ Du implementierst alle Standard-Metriken from scratch
- âœ“ Du wÃ¤hlst die richtige Metrik fÃ¼r deinen Use-Case (FAQ vs. E-Commerce vs. Research)
- âœ“ Du berechnest Metriken fÃ¼r dein Retrieval-System und interpretierst Ergebnisse

**Kritisches Denken:**
- âœ“ Du vermeidest Fehlinterpretationen (hoher Recall â‰  gutes System)
- âœ“ Du erkennst wann eine Metrik tÃ¤uscht (Precision@K ohne Context)
- âœ“ Du definierst realistische Targets basierend auf Use-Case und Risiko

## ğŸ”— WeiterfÃ¼hrende Themen

**NÃ¤chster logischer Schritt:**
â†’ [../02-ai-evaluation/04-quality-metrics.md](../02-ai-evaluation/04-quality-metrics.md) - LLM-AntwortqualitÃ¤t bewerten (Faithfulness, Answer Relevance)

**Vertiefung:**
â†’ [../03-production/08-advanced-techniques.md](../03-production/08-advanced-techniques.md) - RAGAS Framework, LLM-as-Judge, Hard Negatives Mining

**Praktische Anwendung:**
â†’ [../../../06-applications/](../../../06-applications/) - RAG-System Evaluation in Production

**Core-Konzepte:**
- [../../02-embeddings/03-model-selection.md](../../02-embeddings/03-model-selection.md) - Embedding-Models beeinflussen Retrieval-Metriken
- [02-similarity-measures.md](02-similarity-measures.md) - Mathematische Grundlagen der Ã„hnlichkeitsmessung

**Advanced Research:**
â†’ [../../../04-advanced/02-retrieval-optimization.md](../../../04-advanced/02-retrieval-optimization.md) - Two-Stage Retrieval, Hybrid Search fÃ¼r bessere Metriken

**Key Papers:**
1. **BEIR Benchmark** (Thakur et al., 2021) - "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models"
2. **NDCG Original** (JÃ¤rvelin & KekÃ¤lÃ¤inen, 2002) - "Cumulated Gain-based Evaluation of IR Techniques"
3. **MS MARCO** (Nguyen et al., 2016) - Large-scale IR dataset mit MRR als primary metric
