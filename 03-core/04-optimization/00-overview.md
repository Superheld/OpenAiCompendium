# Model Optimization

## ðŸŽ¯ Lernziele
- Verstehe Modell-Komprimierung und Optimierung
- Lerne Quantization, Pruning und effiziente Architekturen
- Verstehe Inference-Optimierung

## ðŸ“– Geschichte & Kontext
Modelle schneller und kleiner machen - von Quantization bis zu effizienten Architekturen.

## ðŸ“‚ Kapitel in diesem Abschnitt
- `01-quantization.md` - PTQ, QAT, GGUF, GPTQ, AWQ
- `02-pruning.md` - Structured, Unstructured Pruning
- `03-model-compression.md` - Kombinationen und Trade-offs
- `04-efficient-architectures.md` - MobileNet, EfficientNet
- `05-inference-optimization.md` - Flash Attention, KV-Cache

## ðŸ”— WeiterfÃ¼hrende Themen
Siehe auch: [core/infrastructure/](../infrastructure/)