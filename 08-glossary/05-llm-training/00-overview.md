# LLM Training & Generation: Glossar

## ğŸ¯ Ãœbersicht

Diese Kategorie definiert **LLM Training, Alignment und Text-Generierung** - wie LLMs lernen und Text produzieren.

**Kernfrage:** Wie werden LLMs trainiert und wie generieren sie Text?

---

## ğŸ“‹ Begriffe in dieser Kategorie

### **Fundamental**
1. **[01-token.md](01-token.md)** - Token / Tokenization
   - **KRITISCH**: Die atomare Einheit fÃ¼r LLMs (18 Dateien)
   - "LaborkÃ¼hlschrank" â†’ [`Lab`, `or`, `kÃ¼hl`, `schrank`]
   - Token-Counting fÃ¼r Cost-Estimation

2. **[03-fine-tuning.md](03-fine-tuning.md)** - Fine-Tuning / RLHF
   - Von Pre-Training zu Instruction-Following
   - RLHF, DPO, Constitutional AI
   - Alignment mit menschlichen Werten

3. **[04-hallucination.md](04-hallucination.md)** - Hallucination / Factual Grounding
   - **QualitÃ¤tsproblem**: LLMs "erfinden" Fakten
   - Detection & Mitigation
   - Faithfulness in RAG

---

## ğŸ”— Lernpfad

```
1. Token (01) â†’ verstehe die atomare Einheit
   â†“
2. Fine-Tuning (03) â†’ wie LLMs trainiert werden
   â†“
3. Hallucination (04) â†’ Hauptproblem und LÃ¶sungen
```

---

**Navigation:**
- ğŸ  [ZurÃ¼ck zum Glossar](../00-overview.md)
