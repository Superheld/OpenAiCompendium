# LLM Training & Generation: Glossar

## 🎯 Übersicht

Diese Kategorie definiert **LLM Training, Alignment und Text-Generierung** - wie LLMs lernen und Text produzieren.

**Kernfrage:** Wie werden LLMs trainiert und wie generieren sie Text?

---

## 📋 Begriffe in dieser Kategorie

### **Fundamental**
1. **[01-token.md](01-token.md)** - Token / Tokenization
   - **KRITISCH**: Die atomare Einheit für LLMs (18 Dateien)
   - "Laborkühlschrank" → [`Lab`, `or`, `kühl`, `schrank`]
   - Token-Counting für Cost-Estimation

2. **[03-fine-tuning.md](03-fine-tuning.md)** - Fine-Tuning / RLHF
   - Von Pre-Training zu Instruction-Following
   - RLHF, DPO, Constitutional AI
   - Alignment mit menschlichen Werten

3. **[04-hallucination.md](04-hallucination.md)** - Hallucination / Factual Grounding
   - **Qualitätsproblem**: LLMs "erfinden" Fakten
   - Detection & Mitigation
   - Faithfulness in RAG

---

## 🔗 Lernpfad

```
1. Token (01) → verstehe die atomare Einheit
   ↓
2. Fine-Tuning (03) → wie LLMs trainiert werden
   ↓
3. Hallucination (04) → Hauptproblem und Lösungen
```

---

**Navigation:**
- 🏠 [Zurück zum Glossar](../00-overview.md)
