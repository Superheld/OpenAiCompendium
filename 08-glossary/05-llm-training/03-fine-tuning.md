# Fine-Tuning / Instruction Tuning / RLHF

## Quick Definition

Anpassung eines vortrainierten LLMs an spezifische Aufgaben oder Domains durch weiteres Training auf kuratiertem Daten - von Base-Model zu Instruction-Following Assistant.

**Kategorie:** LLM Training
**Schwierigkeit:** Intermediate
**Aliases:** Fine-Tuning, Instruction Tuning, RLHF (Reinforcement Learning from Human Feedback), Alignment

---

## üß† Die 3 Trainings-Phasen von LLMs

```
1. PRE-TRAINING (Base Model)
   Internet-Scale Data (Trillionen Tokens)
   ‚Üí Sprachverst√§ndnis, Weltwissen
   ‚Üí Output: "GPT-4-base" (completion model)

2. SUPERVISED FINE-TUNING (SFT)
   Instruction-Response Pairs (10k-100k Beispiele)
   ‚Üí Lernt Instruktionen zu folgen
   ‚Üí Output: "GPT-4-SFT"

3. REINFORCEMENT LEARNING (RLHF)
   Human Feedback (Pr√§ferenz-Ranking)
   ‚Üí Alignment: hilfsbereit, harmlos, ehrlich
   ‚Üí Output: "GPT-4" (final model)
```

---

## üí° Fine-Tuning Use Cases

### **1. Instruction Following**
**Base Model:**
```
Input: "√úbersetze ins Deutsche: Hello World"
Output: "Hello World is a common phrase..." ‚Üê Completion, nicht Translation!
```

**After Fine-Tuning:**
```
Input: "√úbersetze ins Deutsche: Hello World"
Output: "Hallo Welt" ‚Üê Folgt Instruktion! ‚úÖ
```

### **2. Domain-Adaptation**
```
General Model ‚Üí Medical Domain
Fine-Tune auf Medical Papers, Diagnose-Daten
‚Üí Besseres Medical-Reasoning
```

### **3. Style/Tone Adaptation**
```
General Assistant ‚Üí Customer Service
Fine-Tune auf Customer Service Conversations
‚Üí Professioneller, h√∂flicher Ton
```

---

## üíª Code-Beispiel (QLoRA Fine-Tuning)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer

# ========================================
# QLORALORA FINE-TUNING (Memory-Efficient)
# ========================================

model_name = "meta-llama/Llama-3.1-8B"

# 1. Load Model (Quantized f√ºr Memory-Effizienz)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,  # INT4 Quantization
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. LoRA Config (trainiere nur kleine Adapter, nicht alle Gewichte)
lora_config = LoraConfig(
    r=16,  # Rank (h√∂her = mehr Parameter, aber teurer)
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Welche Layer
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# ‚Üí "Trainable params: 13M / 8B = 0.16%"  ‚Üê Nur 0.16% trainiert!

# 3. Training Data (Instruction-Response Pairs)
training_data = [
    {"text": "<|user|>√úbersetze: Hello<|assistant|>Hallo"},
    {"text": "<|user|>Erkl√§re KI<|assistant|>KI ist..."},
    # ... 1000+ Beispiele
]

# 4. Training
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=training_data,
    max_seq_length=512,
    args=TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        learning_rate=2e-4,
    )
)

trainer.train()

# 5. Save Adapter (nur ~50 MB statt 16 GB!)
model.save_pretrained("./llama-3.1-8b-custom")
```

**Memory Usage:**
```
Full Fine-Tuning: 8B √ó 4 bytes √ó 2 (gradients) = 64 GB ‚ùå
QLoRA: 8B √ó 0.5 bytes (INT4) + 13M √ó 4 (LoRA) = 4.05 GB ‚úÖ
```

---

## üîó Related Terms

### **Methoden**
- **LoRA / QLoRA**: Memory-effizientes Fine-Tuning
- **RLHF**: Alignment mit Human Feedback
- **DPO**: Direct Preference Optimization (alternative zu RLHF)

### **Verwandt**
- **[Quantization](../03-quantization-optimization/01-quantization.md)**: QLoRA nutzt INT4
- **Pre-Training**: Phase vor Fine-Tuning
- **Alignment**: RLHF, Constitutional AI

---

## üìç Where This Appears

### **Primary Chapter**
- `02-modern-ai/01-llms/01-model-families.md` (Sektion 8) - Fine-Tuning Methods
- `03-core/01-training/` - Training Pipelines

---

## ‚ö†Ô∏è Common Misconceptions

### ‚ùå "Fine-Tuning braucht riesige Datenmengen"
**Falsch!** Instruction Fine-Tuning funktioniert mit 1k-10k Beispielen.

Pre-Training: Trillionen Tokens
Fine-Tuning: 10k-100k Beispiele ‚Üê 1000√ó weniger!

### ‚ùå "Fine-Tuning = Training von Scratch"
**Falsch!** Fine-Tuning startet mit vortrainiertem Modell.

Von Scratch: Wochen auf 1000+ GPUs
Fine-Tuning: Stunden auf 1-8 GPUs

### ‚ùå "Kann nicht auf Consumer-GPU Fine-Tunen"
**Mit QLoRA: Doch!**

Llama 70B Full Fine-Tuning: 280 GB ‚ùå
Llama 70B QLoRA: 35 GB ‚úÖ (2√ó RTX 4090)

---

## üéØ Zusammenfassung

**3 Phasen:**
1. **Pre-Training** ‚Üí Sprachverst√§ndnis (Trillionen Tokens)
2. **Supervised Fine-Tuning** ‚Üí Instruction-Following (10k Beispiele)
3. **RLHF** ‚Üí Alignment (Human Feedback)

**Fine-Tuning Use Cases:**
- Domain-Adaptation (Medical, Legal, etc.)
- Instruction-Following
- Style/Tone Adaptation

**Memory-Efficient: QLoRA**
- Train nur 0.1-1% der Parameter (LoRA Adapter)
- INT4 Quantization
- Llama 70B auf 2√ó Consumer-GPUs trainierbar!

---

**Navigation:**
- üè† [Zur√ºck zur Kategorie](00-overview.md)
- ‚¨ÖÔ∏è [Vorheriger Begriff: Token](01-token.md)
- ‚û°Ô∏è [N√§chster Begriff: Hallucination](04-hallucination.md)
