# ğŸ“š Glossar: Zentrale Begriffsdefinitionen

## ğŸ¯ Ziel

Dieses Glossar ist der **zentrale Referenzpunkt** fÃ¼r alle technischen Begriffe im ML Kompendium. Es lÃ¶st drei zentrale Probleme:

1. **Konsistenz**: Begriffe wie "Embedding" oder "Quantization" werden einheitlich definiert
2. **Fokus**: Hauptkapitel kÃ¶nnen sich auf Konzepte konzentrieren, Details stehen hier
3. **Navigation**: Ein Begriff â†’ eine Definition â†’ alle Vorkommen im Kompendium verlinkt

### Was du hier findest

- **28 Kernbegriffe** von "Embedding" bis "Hallucination"
- **Mathematische Notation** konsistent erklÃ¤rt
- **Code-Beispiele** fÃ¼r praktische Begriffe
- **Cross-References** zu verwandten Konzepten
- **Alias-Tracking** (z.B. "Dense Vector" = "Embedding")

### Was du hier NICHT findest

- **VollstÃ¤ndige Tutorials** â†’ siehe Hauptkapitel
- **Historischen Kontext** â†’ siehe `01-historical/`
- **Produktions-Implementierungen** â†’ siehe `06-applications/`

---

## ğŸ“‚ Kategorien

Das Glossar ist in **6 thematische Kategorien** organisiert:

### **01. Vectors & Embeddings** [`01-vectors-embeddings/`](01-vectors-embeddings/)
*Grundlagen der VektorreprÃ¤sentation*

**Begriffe (6):**
- **Embedding** / Dense Vector / Embedding Vector
- **Cosine Similarity** / KosinusÃ¤hnlichkeit
- **Vector Normalization** / L2-Normalisierung
- **Dot Product** / Inner Product / Skalarprodukt
- **Dense Retrieval** / Neural Search
- **Sparse Retrieval** / BM25 / Lexical Search

**Kernfrage:** Wie werden Texte zu Vektoren und wie vergleicht man sie?

---

### **02. Transformers & Attention** [`02-transformers-attention/`](02-transformers-attention/)
*Transformer-Architektur und Attention-Mechanismen*

**Begriffe (5):**
- **Self-Attention** / Attention Mechanism
- **Multi-Head Attention**
- **Positional Encoding** / RoPE / ALiBi
- **Transformer Block** / Transformer Layer
- **Context Window** / Sequence Length / KV-Cache

**Kernfrage:** Wie funktionieren Transformer und warum sind sie so mÃ¤chtig?

---

### **03. Quantization & Optimization** [`03-quantization-optimization/`](03-quantization-optimization/)
*Modelloptimierung und Ressourcen-Effizienz*

**Begriffe (7):**
- **Quantization** / Model Quantization
- **Precision Types** (FP32, FP16, BF16, INT8, INT4)
- **GGUF** / llama.cpp Format
- **GPTQ** / GPU-optimized Quantization
- **AWQ** / Activation-aware Weight Quantization
- **LoRA** / Low-Rank Adaptation
- **QLoRA** / Quantized LoRA
- **Mixture of Experts (MoE)**

**Kernfrage:** Wie macht man groÃŸe Modelle effizienter ohne QualitÃ¤tsverlust?

---

### **04. RAG Concepts** [`04-rag-concepts/`](04-rag-concepts/)
*Retrieval-Augmented Generation Patterns*

**Begriffe (5):**
- **RAG** / Retrieval-Augmented Generation
- **Chunking** / Chunk Size / Overlap
- **Hybrid Search** / Hybrid Retrieval
- **Re-Ranking** / Cross-Encoder
- **Relevance vs. Similarity**

**Kernfrage:** Wie baut man RAG-Systeme, die korrekte und relevante Antworten liefern?

---

### **05. LLM Training & Generation** [`05-llm-training/`](05-llm-training/)
*Training, Alignment und Text-Generierung*

**Begriffe (5):**
- **Token** / Tokenization / Vocabulary
- **Next-Token Prediction** / Autoregressive Generation
- **Fine-Tuning** / Instruction Tuning / RLHF
- **Hallucination** / Factual Grounding
- **Prompt Engineering**

**Kernfrage:** Wie werden LLMs trainiert und wie generieren sie Text?

---

### **06. Evaluation Metrics** [`06-evaluation-metrics/`](06-evaluation-metrics/)
*Metriken zur Bewertung von ML/AI-Systemen*

**Begriffe (5):**
- **Precision@K** / Recall@K
- **MRR** / Mean Reciprocal Rank
- **NDCG** / Normalized Discounted Cumulative Gain
- **Perplexity**
- **Faithfulness** / Answer Correctness

**Kernfrage:** Wie misst man die QualitÃ¤t von Retrieval- und Generierungs-Systemen?

---

## ğŸ”¥ Meistgenutzte Begriffe

Diese Begriffe erscheinen in 10+ Kapiteln und sind fundamental fÃ¼r das VerstÃ¤ndnis:

| Begriff | Vorkommen | Kategorie | Schwierigkeit |
|---------|-----------|-----------|---------------|
| **Embedding** | 19 Dateien | Vectors | Beginner |
| **Token** | 18 Dateien | LLM Training | Beginner |
| **Quantization** | 11 Dateien | Optimization | Intermediate |
| **Chunking** | 12 Dateien | RAG | Beginner |
| **Self-Attention** | 15 Dateien | Transformers | Intermediate |
| **Cosine Similarity** | 14 Dateien | Vectors | Beginner |
| **Context Window** | 13 Dateien | Transformers | Beginner |

---

## ğŸ—‚ï¸ Alias-Index

Viele Begriffe haben mehrere Namen. Hier findest du alle Varianten:

**A**
- **Attention** â†’ siehe Self-Attention
- **Autoregressive** â†’ siehe Next-Token Prediction
- **AWQ** â†’ siehe Quantization Methods

**B**
- **BF16** â†’ siehe Precision Types
- **BM25** â†’ siehe Sparse Retrieval

**C**
- **Cosine Distance** â†’ siehe Cosine Similarity
- **Cross-Encoder** â†’ siehe Re-Ranking

**D**
- **Dense Embedding** â†’ siehe Embedding
- **Dense Vector** â†’ siehe Embedding
- **Dot Product** â†’ siehe Vector Operations

**E**
- **Embedding Space** â†’ siehe Embedding
- **Embedding Vector** â†’ siehe Embedding

**F**
- **Factual Grounding** â†’ siehe Hallucination
- **FP16** / **FP32** â†’ siehe Precision Types
- **Fine-Tuning** â†’ siehe LLM Training

**G**
- **GGUF Format** â†’ siehe Quantization Methods
- **GPTQ** â†’ siehe Quantization Methods

**H**
- **Hybrid Retrieval** â†’ siehe Hybrid Search

**I**
- **Inner Product** â†’ siehe Dot Product
- **INT4** / **INT8** â†’ siehe Precision Types
- **Instruction Tuning** â†’ siehe Fine-Tuning

**K**
- **KV-Cache** â†’ siehe Context Window

**L**
- **L2 Norm** â†’ siehe Vector Normalization
- **Lexical Search** â†’ siehe Sparse Retrieval
- **LoRA** â†’ siehe Model Optimization
- **Low-Rank Adaptation** â†’ siehe LoRA

**M**
- **MoE** â†’ siehe Mixture of Experts
- **Multi-Head Attention** â†’ siehe Transformer Attention

**N**
- **Neural Search** â†’ siehe Dense Retrieval
- **Next-Token Prediction** â†’ siehe LLM Generation
- **Normalization** â†’ siehe Vector Normalization

**P**
- **Positional Encoding** â†’ siehe Transformer Position
- **Precision** â†’ siehe Quantization
- **Prompt** â†’ siehe Prompt Engineering

**Q**
- **Quantization** â†’ siehe Model Quantization
- **QLoRA** â†’ siehe LoRA

**R**
- **RAG** â†’ siehe Retrieval-Augmented Generation
- **Re-Ranking** â†’ siehe Retrieval Optimization
- **Relevance** â†’ siehe Relevance vs. Similarity
- **Retrieval** â†’ siehe Dense/Sparse/Hybrid Retrieval
- **RLHF** â†’ siehe Fine-Tuning
- **RoPE** â†’ siehe Positional Encoding

**S**
- **Scalar Product** â†’ siehe Dot Product
- **Self-Attention** â†’ siehe Attention Mechanism
- **Semantic Search** â†’ siehe Dense Retrieval
- **Sequence Length** â†’ siehe Context Window
- **Similarity** â†’ siehe Cosine Similarity oder Relevance

**T**
- **Token** â†’ siehe Tokenization
- **Transformer** â†’ siehe Transformer Block

**V**
- **Vector** â†’ siehe Embedding
- **Vector Database** â†’ siehe Dense Retrieval (erwÃ¤hnt in Embeddings)
- **Vocabulary** â†’ siehe Token

---

## ğŸ“ Wie man das Glossar nutzt

### **Als AnfÃ¤nger**
1. **Start:** Lies die "Meistgenutzte Begriffe" oben
2. **Reihenfolge:**
   - Embedding â†’ Token â†’ Chunking
   - Dann: Self-Attention â†’ Context Window
3. **Lernpfad:** Folge den "Related Terms" Links innerhalb der Definitionen

### **Als fortgeschrittener Lernender**
- Nutze das Glossar als **Quick Reference** wÃ¤hrend du Hauptkapitel liest
- Ãœberspringe Basic-Definitionen, fokussiere auf "Why It Matters" und Trade-offs
- Nutze Code-Beispiele zum Experimentieren

### **Als Entwickler**
- **Notation nachschlagen**: Mathematische Formeln konsistent verwenden
- **API-Referenz**: Code-Snippets fÃ¼r schnelle Integration
- **Performance-Hinweise**: Memory/Speed Trade-offs in "Why It Matters"

### **FÃ¼r Research**
- **Paper-Terminology**: Mapping zwischen Paper-Namen und Kompendium-Begriffen
- **Benchmark-Kontext**: Welche Metriken fÃ¼r welche Evaluation?
- **Cross-References**: Von Begriff zu verwandten Papers/Kapiteln

---

## ğŸ“Š Struktur der Glossar-EintrÃ¤ge

Jeder Begriff folgt diesem Template:

### **[Begriff-Name]**

**Quick Definition** (1 Satz, <15 WÃ¶rter)
- Kategorie: [Vector/Transformer/RAG/etc.]
- Schwierigkeit: [Beginner/Intermediate/Advanced]
- Aliases: [Alternative Namen]

**Detaillierte ErklÃ¤rung**
- Intuitive ErklÃ¤rung (Analogie, ohne Formeln)
- Mathematische Formalisierung (wenn relevant)
- Why It Matters (praktische Bedeutung)
- Common Variations (Varianten des Konzepts)

**Code-Beispiel** (wenn praktisch)
```python
# Minimal, lauffÃ¤hig, 3-5 Zeilen
```

**Related Terms** (Cross-References)
- Verwandte Begriffe mit Links
- Voraussetzungen (was man vorher wissen sollte)
- WeiterfÃ¼hrend (was darauf aufbaut)

**Where This Appears**
- Hauptkapitel (detailed explanation)
- Weitere Vorkommen (references)

---

## ğŸ”— Integration mit Hauptkapiteln

### **Von Kapitel â†’ Glossar**
Hauptkapitel verlinken auf das Glossar fÃ¼r:
- **Schnelle Definition**: Wenn ein Begriff kurz erwÃ¤hnt wird
- **Notation-Check**: Mathematische Symbole nachschlagen
- **Prerequisite**: "FÃ¼r dieses Kapitel solltest du [Begriff] kennen"

### **Von Glossar â†’ Kapitel**
Jeder Glossar-Eintrag verlinkt auf:
- **Primary Chapter**: Wo der Begriff detailliert erklÃ¤rt wird
- **Usage Examples**: Wo der Begriff in der Praxis vorkommt
- **Advanced Topics**: WeiterfÃ¼hrende Kapitel

### **Beispiel-Flow**
```
User liest: 06-applications/01-rag-systems.md
â†’ Sieht: "Chunking" (mit Glossar-Link)
â†’ Klickt auf: 08-glossary/04-rag-concepts/02-chunking.md
â†’ Bekommt: Quick Definition + Code + Link zu 04-advanced/02-retrieval-optimization.md
â†’ Entscheidet: Quick Definition reicht ODER deep dive ins Advanced-Kapitel
```

---

## ğŸš€ Was du nach dem Glossar kannst

Nach Durcharbeiten der Glossar-Kategorien kannst du:

### **Vectors & Embeddings**
- âœ… ErklÃ¤ren warum Embeddings semantische Suche ermÃ¶glichen
- âœ… Cosine Similarity von Hand berechnen
- âœ… Verstehen wann Dense vs. Sparse Retrieval besser ist

### **Transformers & Attention**
- âœ… Attention-Mechanismus mathematisch nachvollziehen
- âœ… Context Window Limits bei der Architektur-Wahl berÃ¼cksichtigen
- âœ… Positional Encoding Varianten (RoPE, ALiBi) unterscheiden

### **Quantization & Optimization**
- âœ… GGUF vs. GPTQ vs. AWQ fÃ¼r dein Use-Case wÃ¤hlen
- âœ… Memory-Footprint eines Modells abschÃ¤tzen
- âœ… LoRA/QLoRA fÃ¼r Finetuning einsetzen

### **RAG Concepts**
- âœ… Chunking-Strategien evaluieren
- âœ… Relevance vs. Similarity Unterschied erklÃ¤ren
- âœ… Hybrid Search mit Re-Ranking implementieren

### **LLM Training**
- âœ… Token-Counting fÃ¼r Cost-Estimation nutzen
- âœ… Hallucination-Risiken einschÃ¤tzen
- âœ… Prompt Engineering Techniken anwenden

### **Evaluation**
- âœ… Richtige Metriken fÃ¼r Retrieval (NDCG) vs. Generation (Perplexity) wÃ¤hlen
- âœ… Precision@K vs. Recall@K Trade-off verstehen
- âœ… RAG-System mit Faithfulness/Correctness evaluieren

---

## ğŸ”¬ Contribution & Updates

Das Glossar ist ein **lebendes Dokument**:

### **Wenn neue Begriffe auftauchen:**
1. PrÃ¼fe ob Begriff in 3+ Kapiteln vorkommt
2. Erstelle Glossar-Eintrag nach Template
3. Verlinke von allen Vorkommen

### **Wenn Definitionen sich Ã¤ndern:**
1. Update nur im Glossar (Single Source of Truth)
2. Alle Kapitel Ã¼bernehmen automatisch neue Definition

### **Wenn Notation inkonsistent:**
1. Entscheide im Glossar fÃ¼r Standard-Notation
2. Markiere Varianten in "Common Variations"
3. Update Hauptkapitel zur Konsistenz

---

## ğŸ“œ Lizenz & Nutzung

Wie das gesamte ML Kompendium:
- âœ… Frei nutzbar fÃ¼r Lernen und Lehre
- âœ… Weitergabe unter gleichen Bedingungen
- âœ… Kommerzielle Nutzung erlaubt
- âš ï¸ Mit Quellenangabe

---

**Navigation:**
- ğŸ  [ZurÃ¼ck zur HauptÃ¼bersicht](../00-overview.md)
- ğŸ“– [Kategorie 1: Vectors & Embeddings](01-vectors-embeddings/)
- ğŸ§  [Kategorie 2: Transformers & Attention](02-transformers-attention/)
- âš™ï¸ [Kategorie 3: Quantization & Optimization](03-quantization-optimization/)
- ğŸ” [Kategorie 4: RAG Concepts](04-rag-concepts/)
- ğŸ“ [Kategorie 5: LLM Training](05-llm-training/)
- ğŸ“Š [Kategorie 6: Evaluation Metrics](06-evaluation-metrics/)

---

*Zentrale Definitionen fÃ¼r 28 Kernbegriffe â€“ konsistent, verlinkt, fokussiert.*
