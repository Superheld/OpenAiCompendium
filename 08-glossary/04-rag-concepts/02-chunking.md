# Chunking / Chunk Size / Text Segmentation

## Quick Definition

Aufteilung langer Dokumente in kleinere Segmente (Chunks) fÃ¼r effizientes Retrieval und LLM-Verarbeitung - kritisch fÃ¼r RAG-QualitÃ¤t.

**Kategorie:** RAG Concepts
**Schwierigkeit:** Beginner (Konzept), Intermediate (Optimization)
**Aliases:** Chunking, Chunk Size, Text Segmentation, Document Splitting

---

## ğŸ§  Warum Chunking?

**Problem:** Dokumente sind zu lang fÃ¼r Embeddings und Context Windows!

```
Dokument: 50 Seiten Handbuch (50k tokens)
Embedding-Modell: Max 512 tokens
Context Window: 128k tokens (aber optimal: kurze, fokussierte Chunks)

â†’ Muss gesplittet werden!
```

**LÃ¶sung: Chunking**
```
50-Seiten Dokument
â†’ Split in 100 Chunks Ã  500 tokens
â†’ Jeder Chunk = 1 Embedding
â†’ Bei Query: Retrieve Top-K relevante Chunks
```

---

## ğŸ’¡ Key Decisions

### **1. Chunk Size** (tokens)

| Size | Pros | Cons | Use Case |
|------|------|------|----------|
| **128-256** | PrÃ¤zise Retrieval | Fragmentiert, Kontext verloren | FAQ, Definitionen |
| **512-1024** | â­ **Sweet Spot** | Gute Balance | Allgemein |
| **2048+** | Viel Kontext | Weniger prÃ¤zise Retrieval | Lange Narratives |

**Default-Empfehlung:** 512 tokens

### **2. Overlap** (tokens)

```
Chunk 1: [Token 0-512]
Chunk 2: [Token 462-974]  â† 50 tokens overlap
         ^^^^^^^^^^^
         Overlap verhindert Information-Loss an Chunk-Grenzen
```

**Typisch:** 10-20% Overlap (50-100 tokens bei 512-token Chunks)

### **3. Chunking-Strategie**

**A) Fixed-Size Chunking** (einfach)
```python
chunks = [text[i:i+512] for i in range(0, len(text), 512)]
```

**B) Semantic Chunking** (besser)
```
Split an natÃ¼rlichen Grenzen:
- AbsÃ¤tze
- Ãœberschriften
- Satzende
```

**C) Sentence-Window** (advanced)
```
Jeder Chunk = 1 Satz
Aber: Bei Retrieval, erweitere um Â±2 SÃ¤tze fÃ¼r Kontext
```

---

## ğŸ’» Code-Beispiel (Kompakt)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tiktoken

enc = tiktoken.encoding_for_model("gpt-4")

def chunk_document(text, chunk_size=512, overlap=50):
    """Chunk mit Token-Awareness"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        length_function=lambda t: len(enc.encode(t)),  # Token-based!
        separators=["\n\n", "\n", ". ", " ", ""]  # Semantic boundaries
    )
    chunks = splitter.split_text(text)
    return chunks

# Test
doc = "Long document... " * 1000
chunks = chunk_document(doc, chunk_size=512, overlap=50)
print(f"Created {len(chunks)} chunks")
print(f"Chunk 0: {len(enc.encode(chunks[0]))} tokens")
```

---

## âš ï¸ Common Mistakes

âŒ **Character-based Chunking** â†’ Use Token-based!
âŒ **No Overlap** â†’ Information-Loss an Grenzen
âŒ **Too Small** (<256 tokens) â†’ Fragmentiert, kein Kontext
âŒ **Too Large** (>2048 tokens) â†’ UnprÃ¤zises Retrieval
âŒ **Ignoriert Semantik** â†’ Split mitten im Satz

---

## ğŸ¯ Zusammenfassung

**Best Practice:**
- **Chunk Size:** 512 tokens
- **Overlap:** 50-100 tokens (10-20%)
- **Strategy:** Semantic (AbsÃ¤tze, SÃ¤tze)
- **Length Function:** Token-based (nicht characters!)

**Trade-off:**
```
Kleinere Chunks â†’ PrÃ¤ziseres Retrieval, weniger Kontext
GrÃ¶ÃŸere Chunks â†’ Mehr Kontext, unprÃ¤ziseres Retrieval
```

---

**Navigation:**
- ğŸ  [ZurÃ¼ck zur Kategorie](00-overview.md)
- â¬…ï¸ [Vorheriger Begriff: RAG](01-rag.md)
