# RAG / Retrieval-Augmented Generation

## Quick Definition

Kombination von **Retrieval** (Suche relevanter Dokumente) + **Generation** (LLM erstellt Antwort basierend auf Retrieved Context) - l√∂st Hallucination-Problem durch externe Knowledge Base.

**Kategorie:** RAG Concepts
**Schwierigkeit:** Intermediate
**Aliases:** RAG, Retrieval-Augmented Generation, Grounded Generation

---

## üß† Detaillierte Erkl√§rung

### Das Problem das RAG l√∂st

**LLM-Only (ohne RAG):**
```
User: "Ist der Labork√ºhlschrank defekt?"
LLM: "Ich habe keine Informationen √ºber spezifische K√ºhlschr√§nke."
```
‚Üí LLM kennt nur Training-Data, keine aktuellen/spezifischen Daten ‚ùå

**RAG (mit External Knowledge):**
```
1. RETRIEVE: Suche "Labork√ºhlschrank Status" in Company-DB
   ‚Üí Findet: "Labork√ºhlschrank LK-42 defekt seit 2025-01-15"

2. AUGMENT: F√ºge Retrieved Info zum Prompt hinzu
   Context: "Labork√ºhlschrank LK-42 defekt seit 2025-01-15"
   Query: "Ist der Labork√ºhlschrank defekt?"

3. GENERATE: LLM antwortet basierend auf Context
   LLM: "Ja, der Labork√ºhlschrank LK-42 ist seit 15.01.2025 defekt."
```
‚Üí Faktisch korrekt, grounded in Company-Data ‚úÖ

### RAG Pipeline (3 Schritte)

$$\text{RAG}(\text{query}) = \text{Generate}(\text{query}, \text{Retrieve}(\text{query}, \text{Knowledge Base}))$$

**1. Indexing** (Offline, einmalig)
```python
docs = load_documents()  # Handb√ºcher, Wikis, DBs
chunks = chunk_documents(docs, chunk_size=512)
embeddings = embed(chunks)  # Text ‚Üí Vectors
vector_db.store(embeddings, chunks)  # In Vector DB speichern
```

**2. Retrieval** (Query-Time)
```python
query_embedding = embed(query)
top_k_chunks = vector_db.search(query_embedding, k=5)  # Top-5 √§hnlichste
```

**3. Generation** (Query-Time)
```python
context = "\n".join(top_k_chunks)
prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer:"
response = llm.generate(prompt)
```

---

## üíª Code-Beispiel (Minimal RAG)

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ========================================
# MINIMAL RAG IMPLEMENTATION
# ========================================

# 1. INDEXING (Offline)
documents = [
    "Labork√ºhlschrank LK-42 defekt seit 15.01.2025",
    "Serverrack √ºberhitzt, Wartung geplant",
    "Kaffeemaschine wurde repariert",
]

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
doc_embeddings = model.encode(documents, normalize_embeddings=True)

# 2. RETRIEVAL (Query-Time)
query = "Ist der K√ºhlschrank kaputt?"
query_embedding = model.encode([query], normalize_embeddings=True)

# Cosine Similarity
scores = cosine_similarity(query_embedding, doc_embeddings)[0]

# Top-K
k = 2
top_k_indices = np.argsort(scores)[::-1][:k]
retrieved_docs = [documents[i] for i in top_k_indices]

print(f"Query: {query}\n")
print("Retrieved Context:")
for i, doc in enumerate(retrieved_docs, 1):
    print(f"  {i}. {doc} (score: {scores[top_k_indices[i-1]]:.3f})")

# 3. GENERATION (w√ºrde echtes LLM nutzen)
context = "\n".join(retrieved_docs)
prompt = f"""Context:
{context}

Question: {query}
Answer based on the context above:"""

print(f"\nü§ñ LLM Prompt:\n{prompt}")
# response = llm.generate(prompt)  # Mit echtem LLM
```

**Output:**
```
Query: Ist der K√ºhlschrank kaputt?

Retrieved Context:
  1. Labork√ºhlschrank LK-42 defekt seit 15.01.2025 (score: 0.672)
  2. Kaffeemaschine wurde repariert (score: 0.234)

ü§ñ LLM Prompt:
Context:
Labork√ºhlschrank LK-42 defekt seit 15.01.2025
Kaffeemaschine wurde repariert

Question: Ist der K√ºhlschrank kaputt?
Answer based on the context above:
```

---

## üîó Related Terms

### **Komponenten**
- **[Dense Retrieval](../01-vectors-embeddings/05-dense-retrieval.md)**: Embedding-basierte Suche
- **[Chunking](02-chunking.md)**: Dokumente in Chunks splitten
- **[Context Window](../02-transformers-attention/05-context-window.md)**: Wie viele Chunks passen

### **Qualit√§t**
- **[Hallucination](../05-llm-training/04-hallucination.md)**: RAG reduziert Hallucinations
- **Faithfulness**: Antwort grounded in Retrieved Context

---

## üìç Where This Appears

### **Primary Chapter**
- `06-applications/01-rag-systems.md` - Vollst√§ndige RAG-Architekturen
- `04-advanced/01-retrieval-methods.md` - Advanced Retrieval

---

## ‚ö†Ô∏è Common Mistakes

‚ùå **Retrieval-Only** ‚Üí Kein LLM, nur Suche (nicht RAG!)
‚ùå **Generation-Only** ‚Üí Kein Retrieval (halluziniert!)
‚ùå **Poor Chunking** ‚Üí Irrelevante Chunks retrieved
‚ùå **No Re-Ranking** ‚Üí Top-K nicht optimal
‚ùå **Ignoring Faithfulness** ‚Üí LLM ignoriert Context

---

## üéØ Zusammenfassung

**RAG in 3 Schritten:**
1. **Retrieve** relevante Dokumente (Dense Search)
2. **Augment** LLM-Prompt mit Retrieved Context
3. **Generate** Antwort grounded in Context

**Vorteile:**
- ‚úÖ Reduziert Hallucinations
- ‚úÖ Aktuelle/spezifische Daten (nicht nur Training-Data)
- ‚úÖ Transparent (zeige Retrieved Sources)

**Trade-off:**
- Retrieval-Qualit√§t bestimmt Output-Qualit√§t
- Latenz h√∂her (Retrieval + Generation)

---

**Navigation:**
- üè† [Zur√ºck zur Kategorie](00-overview.md)
- ‚û°Ô∏è [N√§chster Begriff: Chunking](02-chunking.md)
