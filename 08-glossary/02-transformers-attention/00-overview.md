# Transformers & Attention: Glossar

## üéØ √úbersicht

Diese Kategorie definiert **Transformer-Architektur und Attention-Mechanismen** - die Revolution, die GPT, BERT und alle modernen LLMs erm√∂glicht hat.

**Kernfrage:** Wie funktionieren Transformer und warum sind sie so m√§chtig?

---

## üìã Begriffe in dieser Kategorie

### **Kern-Mechanismen (2 Begriffe)**
1. **[01-self-attention.md](01-self-attention.md)** - Self-Attention / Attention Mechanism
   - **KRITISCH**: Das fundamentale Konzept hinter Transformers
   - Query, Key, Value (Q, K, V)
   - Warum "Attention is All You Need"

2. **[02-multi-head-attention.md](02-multi-head-attention.md)** - Multi-Head Attention
   - Warum mehrere "K√∂pfe"?
   - Parallel lernen verschiedener Repr√§sentationen
   - Wie viele Heads? (z.B. GPT-3: 96 Heads)

### **Position & Kontext (2 Begriffe)**
3. **[03-positional-encoding.md](03-positional-encoding.md)** - Positional Encoding / RoPE / ALiBi
   - Problem: Transformer hat keine "Reihenfolge"
   - Sinusoidal, Learned, RoPE, ALiBi
   - Warum wichtig f√ºr Sprachverst√§ndnis

4. **[05-context-window.md](05-context-window.md)** - Context Window / Sequence Length / KV-Cache
   - **KRITISCH**: Max Tokens ein LLM verarbeiten kann
   - Trade-offs: GPT-4 (128k) vs. Llama 3.2 (128k) vs. Claude 3.5 (200k)
   - KV-Cache Memory Kosten

### **Architektur (1 Begriff)**
5. **[04-transformer-block.md](04-transformer-block.md)** - Transformer Block / Transformer Layer
   - Aufbau: Attention + Feed-Forward + Normalization
   - Encoder vs. Decoder Blocks
   - Wie viele Layers? (GPT-3: 96 Layers)

---

## üîó Lernpfad: Empfohlene Reihenfolge

```
1. Self-Attention (01) ‚Üí verstehe das Kern-Konzept
   ‚Üì
2. Multi-Head Attention (02) ‚Üí warum parallel mehrere Attention-Mechanismen?
   ‚Üì
3. Positional Encoding (03) ‚Üí wie Transformer Reihenfolge versteht
   ‚Üì
4. Transformer Block (04) ‚Üí wie alles zusammenkommt
   ‚Üì
5. Context Window (05) ‚Üí Grenzen und Trade-offs
```

---

## üéì Was du danach kannst

Nach Durcharbeiten dieser 5 Begriffe kannst du:

- ‚úÖ **Erkl√§ren** wie Self-Attention mathematisch funktioniert (Q¬∑K^T)
- ‚úÖ **Verstehen** warum Transformer paralleler sind als RNNs
- ‚úÖ **Berechnen** Memory-Kosten von KV-Cache
- ‚úÖ **Entscheiden** zwischen RoPE vs. ALiBi Positional Encoding
- ‚úÖ **Absch√§tzen** wie viele Layers/Heads f√ºr ein Problem n√∂tig sind

---

## üîó Verwandte Themen im Kompendium

### **Historischer Kontext:**
- `01-historical/04-attention-transformers/` - "Attention is All You Need" (2017)
- `01-historical/03-deep-learning/` - Warum Transformer RNNs abl√∂sten

### **Moderne Anwendung:**
- `02-modern-ai/01-llms/01-model-families.md` - Transformer in GPT, Llama, Mistral
- `02-modern-ai/02-vision/` - Vision Transformers (ViT)

### **Praktische Implementierung:**
- `06-applications/04-model-selection.md` - Context Window bei Modell-Wahl
- `03-core/01-training/` - Training von Transformers

### **Advanced Topics:**
- `04-advanced/` - Flash Attention, Sparse Attention, Long-Context Techniques

---

## üìä Cross-Reference Matrix

| Begriff | Verwendet in | Voraussetzung f√ºr |
|---------|--------------|-------------------|
| **Self-Attention** | 15 Dateien | Alle Transformer-Modelle |
| **Multi-Head Attention** | 12 Dateien | GPT, BERT, alle modernen LLMs |
| **Positional Encoding** | 8 Dateien | Sprachverst√§ndnis, Reihenfolge |
| **Context Window** | 13 Dateien | RAG Chunking, Cost Estimation |
| **Transformer Block** | 10 Dateien | Architektur-Verst√§ndnis |

---

**Navigation:**
- üè† [Zur√ºck zum Glossar](../00-overview.md)
- ‚¨ÖÔ∏è [Vorige Kategorie: Vectors & Embeddings](../01-vectors-embeddings/)
- ‚û°Ô∏è [N√§chste Kategorie: Quantization & Optimization](../03-quantization-optimization/)
