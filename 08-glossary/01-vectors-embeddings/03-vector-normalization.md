# Vector Normalization / L2-Normalisierung

## Quick Definition

Skalierung eines Vektors auf L√§nge 1, sodass nur die **Richtung** erhalten bleibt, nicht die **Magnitude** (L√§nge).

**Kategorie:** Vectors & Embeddings
**Schwierigkeit:** Beginner
**Aliases:** Vector Normalization, L2-Norm, Unit Vector, Normalisierung

---

## üß† Detaillierte Erkl√§rung

### Intuitive Erkl√§rung

Stell dir einen Pfeil vor:
- **Vor Normalisierung**: Pfeil kann beliebige L√§nge haben (1m, 5m, 100m)
- **Nach Normalisierung**: Pfeil hat immer L√§nge = 1 (aber gleiche Richtung!)

**Warum?** Wenn du nur die **Richtung** vergleichen willst (nicht die L√§nge), normalisiere zuerst.

**Beispiel:**
```
Vektor A: [3, 4]          L√§nge: 5
Normalisiert: [0.6, 0.8]  L√§nge: 1 (Richtung identisch!)
```

### Mathematische Formalisierung

F√ºr einen Vektor $\mathbf{v} = [v_1, v_2, \ldots, v_n]$:

**L2-Norm (Vektorl√§nge):**
$$||\mathbf{v}||_2 = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}$$

**Normalisierter Vektor:**
$$\hat{\mathbf{v}} = \frac{\mathbf{v}}{||\mathbf{v}||_2} = \left[\frac{v_1}{||\mathbf{v}||_2}, \frac{v_2}{||\mathbf{v}||_2}, \ldots, \frac{v_n}{||\mathbf{v}||_2}\right]$$

**Eigenschaft:**
$$||\hat{\mathbf{v}}||_2 = 1 \quad \text{(L√§nge ist immer 1)}$$

**Beispiel-Berechnung:**
```
v = [3, 4]
||v|| = ‚àö(3¬≤ + 4¬≤) = ‚àö(9 + 16) = ‚àö25 = 5

vÃÇ = [3/5, 4/5] = [0.6, 0.8]
||vÃÇ|| = ‚àö(0.6¬≤ + 0.8¬≤) = ‚àö(0.36 + 0.64) = ‚àö1 = 1 ‚úì
```

### Why It Matters

**1. Cosine Similarity = Dot Product** (wenn normalisiert)

**Ohne Normalisierung:**
$$\text{cosine}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||} \quad \text{(aufwendig!)}$$

**Mit Normalisierung** ($||\mathbf{A}|| = ||\mathbf{B}|| = 1$):
$$\text{cosine}(\mathbf{A}, \mathbf{B}) = \mathbf{A} \cdot \mathbf{B} \quad \text{(nur Dot Product!)}$$

**Performance-Gewinn:** 10√ó schneller in Vector Databases!

**2. Fair Comparison**

Ohne Normalisierung bevorzugen Distanz-Metriken k√ºrzere Vektoren:

```python
# Nicht-normalisiert
doc1_emb = [0.1, 0.2, 0.1]  # ||v|| = 0.24
doc2_emb = [1.0, 2.0, 1.0]  # ||v|| = 2.45 (10√ó l√§nger!)

euclidean_distance(doc1_emb, doc2_emb)  # Gro√ü, wegen L√§ngen-Unterschied
```

**Mit Normalisierung:**
```python
doc1_norm = normalize(doc1_emb)  # ||v|| = 1
doc2_norm = normalize(doc2_emb)  # ||v|| = 1

cosine_similarity(doc1_norm, doc2_norm)  # Fair, nur Richtung z√§hlt!
```

**3. Standard in Embedding-Modellen**

Viele Embedding-Modelle normalisieren bereits automatisch:
- **Sentence-BERT**: `normalize_embeddings=True` (default)
- **OpenAI ada-002**: Pre-normalized
- **E5, BGE**: Normalisiert w√§hrend Training

**Check ob normalisiert:**
```python
embedding = model.encode("Text")
print(np.linalg.norm(embedding))  # Sollte ‚âà 1.0 sein
```

### Common Variations

**1. L2-Normalisierung** (Standard)
$$\hat{\mathbf{v}} = \frac{\mathbf{v}}{||\mathbf{v}||_2} = \frac{\mathbf{v}}{\sqrt{\sum v_i^2}}$$

**2. L1-Normalisierung** (selten f√ºr Embeddings)
$$\hat{\mathbf{v}} = \frac{\mathbf{v}}{||\mathbf{v}||_1} = \frac{\mathbf{v}}{\sum |v_i|}$$

**3. Max-Normalisierung** (f√ºr Bilder)
$$\hat{\mathbf{v}} = \frac{\mathbf{v}}{\max(|v_i|)}$$

**F√ºr Embeddings:** Immer L2-Norm!

---

## üíª Code-Beispiel

```python
import numpy as np

# Original Embedding (nicht normalisiert)
embedding = np.array([3.0, 4.0, 0.0])
print(f"Original: {embedding}")
print(f"L√§nge: {np.linalg.norm(embedding):.3f}")  # 5.0

# Methode 1: NumPy (manuell)
norm = np.linalg.norm(embedding)
normalized = embedding / norm
print(f"\nNormalisiert: {normalized}")
print(f"L√§nge: {np.linalg.norm(normalized):.3f}")  # 1.0

# Methode 2: sklearn (batch-f√§hig)
from sklearn.preprocessing import normalize

embeddings = np.array([
    [3.0, 4.0, 0.0],
    [1.0, 0.0, 0.0],
    [0.0, 1.0, 1.0]
])
normalized_batch = normalize(embeddings, norm='l2', axis=1)
print(f"\nBatch normalisiert:\n{normalized_batch}")
print(f"L√§ngen: {np.linalg.norm(normalized_batch, axis=1)}")  # [1. 1. 1.]

# Anwendung: Cosine Similarity via Dot Product
emb1_norm = normalize(embeddings[[0]])[0]
emb2_norm = normalize(embeddings[[1]])[0]
similarity = np.dot(emb1_norm, emb2_norm)
print(f"\nCosine Similarity (via Dot Product): {similarity:.3f}")
```

**Output:**
```
Original: [3. 4. 0.]
L√§nge: 5.000

Normalisiert: [0.6 0.8 0. ]
L√§nge: 1.000

Batch normalisiert:
[[0.6        0.8        0.        ]
 [1.         0.         0.        ]
 [0.         0.70710678 0.70710678]]
L√§ngen: [1. 1. 1.]

Cosine Similarity (via Dot Product): 0.600
```

---

## üîó Related Terms

### **Voraussetzungen**
- **[Embedding](01-embedding.md)**: Was normalisiert wird
- **Vektor-L√§nge / Magnitude**: $||\mathbf{v}||$

### **Nutzt Normalisierung**
- **[Cosine Similarity](02-cosine-similarity.md)**: Wird trivial mit normalisierten Vektoren
- **[Dense Retrieval](05-dense-retrieval.md)**: Vector DBs nutzen normalisierte Embeddings

### **Verwandt**
- **[Dot Product](04-dot-product.md)**: Wird zu Cosine Similarity bei normalisierten Vektoren

---

## üìç Where This Appears

### **Primary Chapter**
- `03-core/02-embeddings/01-vector-fundamentals.md` (Sektion 2.3) - Mathematische Grundlagen
- `03-core/03-evaluation/01-data-metrics/02-similarity-measures.md` - Normalisierung f√ºr Metriken

### **Implementation**
- `03-core/05-infrastructure/02-model-serving.md` (Sektion 3) - Vector DB Optimierung
- `03-core/02-embeddings/04-vector-databases.md` - Normalisierung in Pinecone, Weaviate

### **Usage Examples**
- `06-applications/01-rag-systems.md` (Sektion 2.2) - RAG Embeddings normalisieren
- `04-advanced/02-retrieval-optimization.md` - Normalisierung f√ºr Re-Ranking

---

## ‚ö†Ô∏è Common Misconceptions

### ‚ùå "Normalisierung ver√§ndert semantische Bedeutung"
**Falsch!** Nur die **L√§nge** √§ndert sich, nicht die **Richtung** (Semantik).

**Beweis:**
```python
original = [3, 4]
normalized = [0.6, 0.8]

# Gleiche Richtung!
angle_original = np.arctan2(4, 3)      # 0.927 rad
angle_normalized = np.arctan2(0.8, 0.6)  # 0.927 rad (identisch!)
```

**Richtig:** Semantische √Ñhnlichkeit bleibt erhalten, nur L√§ngen-Information geht verloren.

### ‚ùå "Alle Embedding-Modelle geben normalisierte Vektoren zur√ºck"
**Falsch!** Manche Modelle normalisieren, manche nicht.

**Check:**
```python
embedding = model.encode("Text")
length = np.linalg.norm(embedding)
print(f"L√§nge: {length}")

# Normalisiert: length ‚âà 1.0
# Nicht normalisiert: length > 1.0
```

**Best Practice:** Immer selbst normalisieren, au√üer du bist sicher:
```python
from sklearn.preprocessing import normalize
embeddings = normalize(embeddings, norm='l2')
```

### ‚ùå "Normalisierung ist optional"
**Kommt drauf an!**

**MUSS normalisieren:**
- Vector DBs die Dot Product nutzen (z.B. FAISS mit InnerProduct)
- Wenn Cosine Similarity via Dot Product berechnet werden soll
- Bei unterschiedlich langen Dokumenten

**KANN normalisieren:**
- Wenn Vector DB Cosine Similarity direkt unterst√ºtzt
- sklearn `cosine_similarity()` macht es automatisch

**Best Practice:** Normalisiere beim **Einf√ºgen in Vector DB** (einmalig), nicht bei jeder Query!

---

## üéØ Zusammenfassung

**Ein Satz:** Normalisierung skaliert Vektoren auf L√§nge 1, sodass nur Richtung (Semantik) z√§hlt, nicht Magnitude.

**Formel (Merksatz):**
$$\hat{\mathbf{v}} = \frac{\mathbf{v}}{||\mathbf{v}||}$$

**Key Takeaways:**
1. **Performance:** Cosine = Dot Product (10√ó schneller!)
2. **Fairness:** L√§ngen-unabh√§ngiger Vergleich
3. **Einmalig:** Normalisiere beim Einf√ºgen, nicht bei jeder Query
4. **√úberpr√ºfen:** Nicht alle Modelle normalisieren automatisch

**Wann nutzen?**
- ‚úÖ Vector DBs mit Dot Product Index
- ‚úÖ Cosine Similarity Optimierung
- ‚úÖ Fair Comparison unterschiedlich langer Texte
- ‚ùå Wenn absolute Vektorl√§nge Information enth√§lt

---

**Navigation:**
- üè† [Zur√ºck zur Kategorie](00-overview.md)
- ‚¨ÖÔ∏è [Vorheriger Begriff: Cosine Similarity](02-cosine-similarity.md)
- ‚û°Ô∏è [N√§chster Begriff: Dot Product](04-dot-product.md)
