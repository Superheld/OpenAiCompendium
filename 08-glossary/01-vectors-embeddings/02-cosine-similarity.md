# Cosine Similarity / Kosinus√§hnlichkeit

## Quick Definition

Ein √Ñhnlichkeitsma√ü zwischen zwei Vektoren basierend auf dem **Winkel** zwischen ihnen (nicht der Distanz), Wertebereich: [-1, +1].

**Kategorie:** Vectors & Embeddings
**Schwierigkeit:** Beginner
**Aliases:** Cosine Similarity, Kosinus√§hnlichkeit, Cosine Distance (1 - cosine similarity)

---

## üß† Detaillierte Erkl√§rung

### Intuitive Erkl√§rung

Stell dir zwei Pfeile vor, die vom Ursprung aus in verschiedene Richtungen zeigen:
- **Gleiche Richtung** (Winkel 0¬∞) ‚Üí Cosine Similarity = **1.0** (identisch)
- **Orthogonal** (Winkel 90¬∞) ‚Üí Cosine Similarity = **0.0** (keine √Ñhnlichkeit)
- **Entgegengesetzt** (Winkel 180¬∞) ‚Üí Cosine Similarity = **-1.0** (Gegenteil)

**Wichtig:** Cosine Similarity ignoriert die **L√§nge** der Vektoren, nur die **Richtung** z√§hlt!

**Beispiel:**
```
Vektor A: [3, 4]     (L√§nge: 5)
Vektor B: [6, 8]     (L√§nge: 10, doppelt so lang)
Cosine Similarity: 1.0 (gleiche Richtung!)
```

### Mathematische Formalisierung

F√ºr zwei Vektoren $\mathbf{A}$ und $\mathbf{B}$:

$$\text{cosine\_similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}$$

**Komponenten:**
- **Z√§hler**: $\mathbf{A} \cdot \mathbf{B}$ = Dot Product (Skalarprodukt)
- **Nenner**: $||\mathbf{A}|| \cdot ||\mathbf{B}||$ = Produkt der Vektorl√§ngen (L2-Normen)

**Geometrische Interpretation:**

$$\text{cosine\_similarity}(\mathbf{A}, \mathbf{B}) = \cos(\theta)$$

wo $\theta$ der Winkel zwischen den Vektoren ist.

**Wertebereich:**
- $+1$: Vektoren zeigen in exakt gleiche Richtung
- $0$: Vektoren sind orthogonal (keine √Ñhnlichkeit)
- $-1$: Vektoren zeigen in entgegengesetzte Richtung

**Beispiel-Berechnung:**

```
A = [1, 2, 3]
B = [2, 4, 6]

Dot Product: 1√ó2 + 2√ó4 + 3√ó6 = 2 + 8 + 18 = 28
||A|| = ‚àö(1¬≤ + 2¬≤ + 3¬≤) = ‚àö14 ‚âà 3.74
||B|| = ‚àö(2¬≤ + 4¬≤ + 6¬≤) = ‚àö56 ‚âà 7.48

Cosine Similarity = 28 / (3.74 √ó 7.48) ‚âà 28 / 28 = 1.0
```

### Why It Matters

**1. Skalierungs-Invarianz**

Cosine Similarity ist unabh√§ngig von der Vektorl√§nge - perfekt f√ºr Text-Embeddings!

**Problem mit Euclidean Distance:**
```python
# Lange Dokumente haben gr√∂√üere Embeddings ‚Üí unfairer Vergleich
doc1 = "KI"           ‚Üí embedding_norm = 0.5
doc2 = "KI ist ..."   ‚Üí embedding_norm = 2.0  (4√ó l√§nger)
# Euclidean Distance bevorzugt k√ºrzere Embeddings!
```

**L√∂sung mit Cosine:**
```python
# Nur Richtung z√§hlt, nicht L√§nge ‚Üí fairer Vergleich
cosine_similarity(doc1, doc2)  # Vergleicht semantische √Ñhnlichkeit
```

**2. Effiziente Berechnung mit normalisierten Vektoren**

Wenn Embeddings **normalisiert** sind ($||\mathbf{v}|| = 1$):

$$\text{cosine\_similarity}(\mathbf{A}, \mathbf{B}) = \mathbf{A} \cdot \mathbf{B}$$

**Nur ein Dot Product!** ‚Üí 10√ó schneller in Vector Databases

**3. Standard in Embedding-Modellen**

Fast alle Embedding-Modelle sind auf Cosine Similarity optimiert:
- **Sentence-BERT**: Trainiert mit Cosine Loss
- **E5, BGE**: Contrastive Learning mit Cosine
- **OpenAI ada-002**: Dokumentiert als Cosine-optimiert

### Common Variations

**1. Cosine Distance** (f√ºr Clustering/KNN)

$$\text{cosine\_distance}(\mathbf{A}, \mathbf{B}) = 1 - \text{cosine\_similarity}(\mathbf{A}, \mathbf{B})$$

Wertebereich: [0, 2]
- 0 = identisch
- 2 = entgegengesetzt

**2. Normalisierte Embeddings + Dot Product**

```python
# Viele Vector DBs normalisieren automatisch
embeddings = normalize(embeddings)  # ||v|| = 1
similarity = dot_product(emb1, emb2)  # = cosine similarity!
```

**3. Angular Distance**

$$\text{angular\_distance} = \frac{\arccos(\text{cosine\_similarity})}{\pi}$$

Wertebereich: [0, 1], entspricht "normalized angle"

---

## üíª Code-Beispiel

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Beispiel: Zwei S√§tze als Embeddings
# (normalerweise von Sentence-BERT, hier vereinfacht)
embedding1 = np.array([[0.5, 0.8, 0.3, 0.1]])  # "Labork√ºhlschrank defekt"
embedding2 = np.array([[0.52, 0.79, 0.31, 0.09]])  # "K√ºhlschrank im Labor kaputt"
embedding3 = np.array([[-0.3, 0.2, -0.9, 0.4]])  # "Serverausfall"

# Methode 1: sklearn (einfach, aber langsamer)
sim_12 = cosine_similarity(embedding1, embedding2)[0][0]
sim_13 = cosine_similarity(embedding1, embedding3)[0][0]
print(f"Similarity (1 vs 2): {sim_12:.3f}")  # ~0.998 (sehr √§hnlich!)
print(f"Similarity (1 vs 3): {sim_13:.3f}")  # ~-0.234 (un√§hnlich)

# Methode 2: NumPy (manuell, f√ºr Verst√§ndnis)
def cosine_sim(a, b):
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot_product / (norm_a * norm_b)

print(f"Manual (1 vs 2): {cosine_sim(embedding1[0], embedding2[0]):.3f}")

# Methode 3: Optimiert mit normalisierten Vektoren
def normalize(v):
    return v / np.linalg.norm(v)

emb1_norm = normalize(embedding1[0])
emb2_norm = normalize(embedding2[0])
sim_optimized = np.dot(emb1_norm, emb2_norm)  # Nur Dot Product!
print(f"Optimized (1 vs 2): {sim_optimized:.3f}")
```

**Output:**
```
Similarity (1 vs 2): 0.998
Similarity (1 vs 3): -0.234
Manual (1 vs 2): 0.998
Optimized (1 vs 2): 0.998
```

---

## üîó Related Terms

### **Voraussetzungen**
- **[Dot Product](04-dot-product.md)**: Z√§hler der Cosine-Formel
- **[Vector Normalization](03-vector-normalization.md)**: Nenner der Cosine-Formel
- **[Embedding](01-embedding.md)**: Was verglichen wird

### **Baut darauf auf**
- **[Dense Retrieval](05-dense-retrieval.md)**: Nutzt Cosine Similarity f√ºr Ranking
- **Re-Ranking** (siehe `04-rag-concepts/04-reranking.md`): Verfeinert Cosine-basierte Ergebnisse

### **Alternativen**
- **Euclidean Distance**: $||\mathbf{A} - \mathbf{B}||$ (l√§ngenabh√§ngig, nicht ideal f√ºr Embeddings)
- **Manhattan Distance**: $\sum |A_i - B_i|$ (selten f√ºr Embeddings)
- **Dot Product**: Wenn Vektoren bereits normalisiert sind

---

## üìç Where This Appears

### **Primary Chapter**
- `03-core/02-embeddings/01-vector-fundamentals.md` (Sektion 3) - Vollst√§ndige Ableitung
- `03-core/03-evaluation/01-data-metrics/02-similarity-measures.md` - Vergleich verschiedener Metriken

### **Usage Examples**
- `06-applications/01-rag-systems.md` (Sektion 2.3) - Cosine in RAG Retrieval
- `06-applications/02-search-systems.md` (Sektion 2.1) - Dense Search Ranking
- `04-advanced/01-retrieval-methods.md` - Dense Retrieval Scoring

### **Implementation Details**
- `03-core/05-infrastructure/02-model-serving.md` - Optimierte Cosine-Berechnung in Vector DBs
- `03-core/02-embeddings/04-vector-databases.md` - Index-Strukturen f√ºr Cosine Search (HNSW, IVF)

---

## ‚ö†Ô∏è Common Misconceptions

### ‚ùå "Cosine Similarity ist eine Distanz-Metrik"
**Falsch!** Cosine Similarity ist ein **√Ñhnlichkeitsma√ü** (h√∂her = √§hnlicher).

**Distanz-Metriken:**
- Cosine **Distance** = $1 - \text{cosine\_similarity}$ (niedriger = √§hnlicher)
- Euclidean Distance = $||\mathbf{A} - \mathbf{B}||$

**Merksatz:**
- **Similarity**: H√∂her = √§hnlicher (0 bis 1)
- **Distance**: Niedriger = √§hnlicher (0 bis ‚àû)

### ‚ùå "Cosine Similarity funktioniert nur f√ºr positive Werte"
**Falsch!** Cosine funktioniert f√ºr **beliebige** reelle Zahlen.

Embeddings k√∂nnen negative Werte haben:
```python
embedding = [0.5, -0.3, 0.8, -0.1, ...]  # V√∂llig normal!
```

**Wertebereich:** $[-1, +1]$ (nicht $[0, 1]$!)
- $+1$: Gleiche Richtung
- $0$: Orthogonal
- $-1$: Entgegengesetzt

**Aber:** In der Praxis sind Embedding-Similarities meist **positiv** (0 bis 1), weil Sentence-BERT Modelle so trainiert sind.

### ‚ùå "Cosine Similarity = Dot Product"
**Nur wenn normalisiert!**

**Allgemein:**
$$\text{cosine} = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||}$$

**Wenn** $||\mathbf{A}|| = ||\mathbf{B}|| = 1$ **dann:**
$$\text{cosine} = \mathbf{A} \cdot \mathbf{B}$$

**Trade-off:**
- **Mit Normalisierung**: Cosine = Dot Product (schnell!)
- **Ohne Normalisierung**: Vollst√§ndige Formel n√∂tig (langsamer)

**Best Practice:** Normalisiere Embeddings **einmal** beim Einf√ºgen in Vector DB ‚Üí alle Queries nutzen schnellen Dot Product.

---

## üìä Performance-Vergleich

| Methode | Berechnung | Geschwindigkeit | Use-Case |
|---------|------------|-----------------|----------|
| **Cosine (full formula)** | $\mathbf{A} \cdot \mathbf{B} / (\\|\mathbf{A}\\| \\|\mathbf{B}\\|)$ | Baseline (1√ó) | Nicht-normalisierte Vektoren |
| **Normalized + Dot Product** | $\mathbf{A} \cdot \mathbf{B}$ | **10√ó schneller** | Vector DBs (Pinecone, Weaviate) |
| **Euclidean Distance** | $\\|\mathbf{A} - \mathbf{B}\\|$ | ~5√ó langsamer | Nicht f√ºr Embeddings! |

**Benchmark (1M Vergleiche):**
```
Cosine (sklearn):      1.2s
Normalized Dot (NumPy): 0.12s  ‚Üê 10√ó Speedup!
Euclidean:             2.1s
```

---

## üéØ Zusammenfassung

**Ein Satz:** Cosine Similarity misst den **Winkel** zwischen Vektoren (nicht Distanz) und ist der Standard f√ºr Embedding-Vergleiche.

**Formel (Merksatz):**
$$\text{cosine} = \frac{\text{Dot Product}}{\text{L√§nge}_A \times \text{L√§nge}_B}$$

**Key Takeaways:**
1. **Skalierungs-invariant**: L√§nge egal, nur Richtung z√§hlt
2. **Optimiert f√ºr Embeddings**: Fast alle Modelle trainiert mit Cosine Loss
3. **Performance-Hack**: Normalisierung ‚Üí Cosine = Dot Product (10√ó schneller)
4. **Wertebereich**: [-1, +1], aber bei Embeddings meist [0, 1]

**Wann nutzen?**
- ‚úÖ Text-Embeddings vergleichen (Standard!)
- ‚úÖ Dense Retrieval Ranking
- ‚úÖ Skalierungs-unabh√§ngige √Ñhnlichkeit
- ‚ùå Wenn absolute Vektorl√§nge wichtig ist

---

**Navigation:**
- üè† [Zur√ºck zur Kategorie](00-overview.md)
- ‚¨ÖÔ∏è [Vorheriger Begriff: Embedding](01-embedding.md)
- ‚û°Ô∏è [N√§chster Begriff: Vector Normalization](03-vector-normalization.md)
