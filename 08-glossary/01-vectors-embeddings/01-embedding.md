# Embedding / Dense Vector / Embedding Vector

## Quick Definition

Eine mathematische Repr√§sentation von Text (oder anderen Daten) als Liste von Zahlen in einem hochdimensionalen Raum, die semantische Bedeutung erh√§lt.

**Kategorie:** Vectors & Embeddings
**Schwierigkeit:** Beginner
**Aliases:** Embedding, Dense Vector, Embedding Vector, Representation, Vector Representation

---

## üß† Detaillierte Erkl√§rung

### Intuitive Erkl√§rung

Stell dir vor, du m√∂chtest W√∂rter auf einem Blatt Papier so anordnen, dass √§hnliche W√∂rter nahe beieinander liegen:
- "Hund" und "Katze" liegen nahe (beide Haustiere)
- "Auto" und "Fahrrad" liegen nahe (beide Fahrzeuge)
- "Hund" und "Auto" liegen weit auseinander (keine √Ñhnlichkeit)

Ein **Embedding** ist genau das - aber in **384-1024 Dimensionen** statt auf einem 2D-Papier. Jedes Wort (oder Satz) wird zu einem Punkt in diesem hochdimensionalen Raum.

**Beispiel:** Der Satz "Labork√ºhlschrank" wird zu:
```
[0.23, -0.45, 0.12, ..., 0.67]  # 384 Zahlen
```

### Mathematische Formalisierung

Ein Embedding ist ein Vektor $\mathbf{v} \in \mathbb{R}^n$:

$$\mathbf{v} = [v_1, v_2, v_3, \ldots, v_n]$$

wo:
- $n$ = Dimensionalit√§t (typisch 384, 512, 768, 1024, 1536)
- $v_i \in \mathbb{R}$ = i-te Komponente (reelle Zahl)
- $\mathbf{v}$ repr√§sentiert semantische Bedeutung

**Embedding-Funktion:**
$$f: \text{Text} \rightarrow \mathbb{R}^n$$

Beispiel:
$$f(\text{"Labork√ºhlschrank"}) = [0.23, -0.45, 0.12, \ldots, 0.67]$$

**Wichtige Eigenschaft:**

Semantisch √§hnliche Texte haben **√§hnliche Embeddings** (kleiner Abstand/Winkel im Vektorraum).

$$\text{similarity}(\text{"Hund"}, \text{"Katze"}) > \text{similarity}(\text{"Hund"}, \text{"Auto"})$$

### Why It Matters

**Problem ohne Embeddings:**
Computer k√∂nnen nur exakte String-Matches finden:
- Query: "K√ºhlschrank f√ºr Labor"
- Dokument: "Labork√ºhlschrank"
- **Match: NEIN** ‚ùå (unterschiedliche W√∂rter)

**Mit Embeddings:**
- Query-Embedding: `[0.22, -0.44, 0.11, ...]`
- Dokument-Embedding: `[0.23, -0.45, 0.12, ...]`
- Cosine Similarity: **0.95** ‚úÖ (semantisch identisch!)

**Anwendungen:**
- **Semantic Search**: Finde Dokumente nach Bedeutung, nicht Keywords
- **RAG Systems**: Hole relevante Kontexte f√ºr LLM
- **Recommendation**: Finde √§hnliche Produkte/Artikel
- **Clustering**: Gruppiere semantisch verwandte Texte

### Common Variations

**1. Text Embeddings** (S√§tze/Paragraphen)
```python
model.encode("Der Labork√ºhlschrank ist defekt")
# ‚Üí [0.23, -0.45, ..., 0.67] (384 dims)
```

**2. Token Embeddings** (einzelne Tokens in LLMs)
```python
# In GPT/BERT: jedes Token ‚Üí Embedding
"Labork√ºhlschrank" ‚Üí [Token1_emb, Token2_emb, ...]
```

**3. Multimodal Embeddings** (Bild + Text in gleichem Raum)
```python
# CLIP: Bild und Text in gemeinsamen Embedding-Space
image_emb = [0.1, 0.2, ...]
text_emb  = [0.11, 0.19, ...]  # √§hnlich!
```

**Dimensionalit√§ten nach Modell:**

| Modell | Dimensionen | Use-Case |
|--------|-------------|----------|
| `all-MiniLM-L6-v2` | 384 | Schnell, gute Balance |
| `paraphrase-multilingual-MiniLM-L12-v2` | 384 | Multilingual (DE/EN) |
| `all-mpnet-base-v2` | 768 | H√∂here Qualit√§t |
| `text-embedding-ada-002` (OpenAI) | 1536 | State-of-the-Art (API) |
| `e5-large-v2` | 1024 | Open-Source SOTA |

---

## üíª Code-Beispiel

```python
from sentence_transformers import SentenceTransformer

# 1. Modell laden
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 2. Text ‚Üí Embedding
text = "Der Labork√ºhlschrank ist defekt"
embedding = model.encode(text)

print(f"Dimensionen: {len(embedding)}")  # 384
print(f"Erste 5 Werte: {embedding[:5]}")
# [0.23456, -0.45123, 0.12789, 0.88234, -0.34567]

# 3. Mehrere Texte gleichzeitig
texts = [
    "Labork√ºhlschrank defekt",
    "K√ºhlschrank im Labor kaputt",
    "Serverausfall im Rechenzentrum"
]
embeddings = model.encode(texts)  # Shape: (3, 384)

# 4. √Ñhnlichkeit berechnen (Cosine Similarity)
from sklearn.metrics.pairwise import cosine_similarity
sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
print(f"√Ñhnlichkeit (1 vs 2): {sim:.3f}")  # ~0.95 (sehr √§hnlich!)

sim_diff = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]
print(f"√Ñhnlichkeit (1 vs 3): {sim_diff:.3f}")  # ~0.30 (un√§hnlich)
```

**Output:**
```
Dimensionen: 384
Erste 5 Werte: [0.23456 -0.45123  0.12789  0.88234 -0.34567]
√Ñhnlichkeit (1 vs 2): 0.953
√Ñhnlichkeit (1 vs 3): 0.298
```

---

## üîó Related Terms

### **Voraussetzungen** (was du vorher wissen solltest)
- **Vektor (Mathematik)**: $n$-Tupel reeller Zahlen
- **Dimensionalit√§t**: Anzahl der Komponenten im Vektor

### **Baut darauf auf** (was danach kommt)
- **[Cosine Similarity](02-cosine-similarity.md)**: Wie man Embeddings vergleicht
- **[Vector Normalization](03-vector-normalization.md)**: Embeddings f√ºr Vergleich vorbereiten
- **[Dense Retrieval](05-dense-retrieval.md)**: Embeddings f√ºr Suche nutzen

### **Verwandt**
- **Token** (siehe `05-llm-training/01-token.md`): In LLMs wird jedes Token eingebettet
- **Vector Database**: Speichert Millionen Embeddings effizient (Pinecone, Weaviate, Qdrant)

---

## üìç Where This Appears

### **Primary Chapter** (detaillierte Erkl√§rung)
- `03-core/02-embeddings/01-vector-fundamentals.md` - Vollst√§ndige mathematische Grundlagen
- `03-core/02-embeddings/02-embedding-architectures.md` - Wie Embeddings trainiert werden (BERT, Sentence-BERT)
- `03-core/02-embeddings/03-model-selection.md` - Welches Embedding-Modell w√§hlen?

### **Usage Examples** (praktische Anwendung)
- `06-applications/01-rag-systems.md` (Sektion 2) - Embeddings in RAG
- `06-applications/02-search-systems.md` (Sektion 2) - Dense Retrieval
- `04-advanced/01-retrieval-methods.md` - Dense vs. Sparse vs. Hybrid

### **Mentioned In** (weitere Vorkommen)
- `02-modern-ai/01-llms/01-model-families.md` (Sektion 1.1) - Token Embeddings in LLMs
- `02-modern-ai/03-multimodal/` - Multimodale Embeddings (CLIP)
- `03-core/03-evaluation/02-ai-evaluation/03-retrieval-metrics.md` - Embedding-basierte Retrieval-Evaluation

---

## ‚ö†Ô∏è Common Misconceptions

### ‚ùå "Embeddings sind einfach Word2Vec"
**Falsch!** Word2Vec (2013) war ein fr√ºher Ansatz. Moderne Embeddings:
- **Contextualisiert**: "Bank" (Finanz) vs. "Bank" (Sitz) haben unterschiedliche Embeddings
- **Satz-Level**: Nicht nur W√∂rter, sondern ganze S√§tze/Paragraphen
- **Task-optimiert**: Trainiert f√ºr spezifische Aufgaben (Retrieval, QA, etc.)

**Richtig:** Word2Vec war der Anfang. Moderne Sentence-BERT/E5 sind weit √ºberlegen.

### ‚ùå "Mehr Dimensionen = immer besser"
**Falsch!** Dimensionen haben Trade-offs:

| Dimensionen | Vorteile | Nachteile |
|-------------|----------|-----------|
| 384 | Schnell, wenig Speicher | Etwas weniger Pr√§zision |
| 768 | Gute Balance | Standard-Choice |
| 1536 | H√∂chste Qualit√§t | 4√ó mehr Speicher, langsamer |

**Richtig:** W√§hle Dimensionen basierend auf **Latenz-Budget** und **Qualit√§ts-Anforderungen**.

### ‚ùå "Embedding = Feature Vector"
**Technisch √§hnlich, semantisch unterschiedlich:**
- **Feature Vector** (klassisches ML): Hand-crafted Features (TF-IDF, Bag-of-Words)
- **Embedding** (Deep Learning): **Gelernte** Repr√§sentation, erfasst semantische Bedeutung

**Richtig:** Embeddings sind semantisch reichhaltiger als klassische Features.

---

## üéØ Zusammenfassung

**Ein Satz:** Embeddings sind Vektoren, die semantische Bedeutung von Text in Zahlen codieren und Maschinen erm√∂glichen, "√Ñhnlichkeit" zu verstehen.

**Merksatz:** "√Ñhnliche Bedeutung ‚Üí √§hnliche Vektoren ‚Üí kleine Distanz im Embedding-Space"

**Key Takeaway:** Ohne Embeddings keine moderne Semantic Search, kein RAG, keine multimodale AI. Embeddings sind das **Fundament aller retrieval-basierten AI-Systeme**.

---

**Navigation:**
- üè† [Zur√ºck zur Kategorie](00-overview.md)
- ‚û°Ô∏è [N√§chster Begriff: Cosine Similarity](02-cosine-similarity.md)
