# Quantization & Optimization: Glossar

## ğŸ¯ Ãœbersicht

Diese Kategorie definiert **Modelloptimierung und Ressourcen-Effizienz** - wie man groÃŸe LLMs auf Consumer-Hardware lauffÃ¤hig macht.

**Kernfrage:** Wie macht man 70B Parameter Modelle auf 24GB VRAM lauffÃ¤hig?

---

## ğŸ“‹ Begriffe in dieser Kategorie

### **Kern-Konzept**
1. **[01-quantization.md](01-quantization.md)** - Quantization / Model Quantization
   - **KRITISCH**: Von 16GB â†’ 4GB ohne groÃŸen QualitÃ¤tsverlust
   - FP16 â†’ INT8 â†’ INT4 Kompression
   - Trade-off: Memory vs. Accuracy

---

## ğŸ”— Lernpfad

```
1. Quantization (01) â†’ verstehe das Kern-Konzept
```

---

## ğŸ“ Was du danach kannst

- âœ… **Berechnen** Memory-Footprint eines Modells
- âœ… **Entscheiden** welche Quantization-Methode (GGUF vs. GPTQ vs. AWQ)
- âœ… **AbschÃ¤tzen** QualitÃ¤tsverlust bei verschiedenen Bit-Tiefen

---

**Navigation:**
- ğŸ  [ZurÃ¼ck zum Glossar](../00-overview.md)
- â¬…ï¸ [Vorige Kategorie: Transformers & Attention](../02-transformers-attention/)
- â¡ï¸ [NÃ¤chste Kategorie: RAG Concepts](../04-rag-concepts/)
